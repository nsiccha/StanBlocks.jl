[
  {
    "objectID": "implementations.html",
    "href": "implementations.html",
    "title": "Julia posteriordb implementations",
    "section": "",
    "text": "See the PosteriorDB.jl extension at https://github.com/nsiccha/StanBlocks.jl/blob/main/ext/PosteriorDBExt.jl, also included below:\nmodule PosteriorDBExt\n\nimport PosteriorDB\nimport StanBlocks\nimport StanBlocks: julia_implementation, @stan, @parameters, @transformed_parameters, @model, @broadcasted\nimport StanBlocks: bernoulli_lpmf, binomial_lpmf, log_sum_exp, logit, binomial_logit_lpmf, bernoulli_logit_lpmf, inv_logit, log_inv_logit, rep_vector, square, normal_lpdf, sd, multi_normal_lpdf, student_t_lpdf, gp_exp_quad_cov, log_mix, append_row, append_col, pow, diag_matrix, normal_id_glm_lpdf, rep_matrix, rep_row_vector, cholesky_decompose, dot_self, cumulative_sum, softmax, log1m_inv_logit, matrix_constrain, integrate_ode_rk45, integrate_ode_bdf, poisson_lpdf, nothrow_log, categorical_lpmf, dirichlet_lpdf, exponential_lpdf, sub_col, stan_tail, dot_product, segment, inv_gamma_lpdf, diag_pre_multiply, multi_normal_cholesky_lpdf, logistic_lpdf\nusing Statistics, LinearAlgebra\n\n@inline PosteriorDB.implementation(model::PosteriorDB.Model, ::Val{:stan_blocks}) = julia_implementation(Val(Symbol(PosteriorDB.name(model))))\n# @inline StanBlocks.stan_implementation(posterior::PosteriorDB.Posterior) = StanProblem(\n#     PosteriorDB.path(PosteriorDB.implementation(PosteriorDB.model(posterior), \"stan\")), \n#     PosteriorDB.load(PosteriorDB.dataset(posterior), String);\n#     nan_on_error=true\n# )\n\njulia_implementation(posterior::PosteriorDB.Posterior) = julia_implementation(\n    Val(Symbol(PosteriorDB.name(PosteriorDB.model(posterior))));\n    Dict([Symbol(k)=&gt;v for (k, v) in pairs(PosteriorDB.load(PosteriorDB.dataset(posterior)))])...\n)\n\njulia_implementation(::Val{:earn_height}; N, earn, height, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0.)\n        end\n        @model begin\n            earn ~ normal(@broadcasted(beta[1] + beta[2] * height), sigma);\n        end\n    end\nend\n\njulia_implementation(::Val{:wells_dist}; N, switched, dist, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n        end\n        @model begin\n            switched ~ bernoulli_logit(@broadcasted(beta[1] + beta[2] * dist));\n        end\n    end\nend\njulia_implementation(::Val{:sesame_one_pred_a}; N, encouraged, watched, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0.)\n        end\n        @model begin\n            watched ~ normal(@broadcasted(beta[1] + beta[2] * encouraged), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:Rate_1_model}; n, k, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0.,upper=1.)\n        end\n        @model begin\n            theta ~ beta(1, 1)\n            k ~ binomial(n, theta)\n        end\n    end\nend\njulia_implementation(::Val{:nes_logit_model}; N, income, vote, kwargs...) = begin \n    X = reshape(income, (N, 1))\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[1]\n        end\n        @model begin\n            vote ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_momiq}; N, kid_score, mom_iq, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0)\n        end\n        @model begin\n            sigma ~ cauchy(0, 2.5);\n            kid_score ~ normal(@broadcasted(beta[1] + beta[2] * mom_iq), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_momhs}; N, kid_score, mom_hs, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0)\n        end\n        @model begin\n            sigma ~ cauchy(0, 2.5);\n            kid_score ~ normal(@broadcasted(beta[1] + beta[2] * mom_hs), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:logearn_height}; N, earn, height, kwargs...) = begin \n    log_earn = @. log(earn)\n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0)\n        end\n        @model begin\n            log_earn ~ normal(@broadcasted(beta[1] + beta[2] * height), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:blr}; N, D, X, y, kwargs...) = begin \n    @assert size(X) == (N,D)\n    @stan begin \n        @parameters begin\n            beta::vector[D]\n            sigma::real(lower=0)\n        end\n        @model begin\n            target += normal_lpdf(beta, 0, 10);\n            target += normal_lpdf(sigma, 0, 10);\n            target += normal_lpdf(y, X * beta, sigma);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dist100_model}; N, switched, dist, kwargs...) = begin \n    dist100 = @. dist / 100.\n    X = reshape(dist100, (N, 1))\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[1]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:Rate_3_model}; n1, n2, k1, k2, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0,upper=1)\n        end\n        @model begin\n            theta ~ beta(1, 1)\n            k1 ~ binomial(n1, theta)\n            k2 ~ binomial(n2, theta)\n        end\n    end\nend\njulia_implementation(::Val{:logearn_height_male}; N, earn, height, male, kwargs...) = begin \n    log_earn = @. log(earn)\n    @stan begin \n        @parameters begin\n            beta::vector[3]\n            sigma::real(lower=0)\n        end\n        @model begin\n            log_earn ~ normal(@broadcasted(beta[1] + beta[2] * height + beta[3] * male), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_momhsiq}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[3]\n            sigma::real(lower=0)\n        end\n        @model begin\n            sigma ~ cauchy(0, 2.5);\n            kid_score ~ normal(@broadcasted(beta[1] + beta[2] * mom_hs + beta[3] * mom_iq), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:log10earn_height}; N, earn, height, kwargs...) = begin \n    log10_earn = @. log10(earn)\n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0)\n        end\n        @model begin\n            log10_earn ~ normal(@broadcasted(beta[1] + beta[2] * height), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dist100ars_model}; N, switched, dist, arsenic, kwargs...) = begin \n    dist100 = @. dist / 100.\n    X = hcat(dist100, arsenic)\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[2]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:low_dim_gauss_mix_collapse}; N, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::vector[2]\n            sigma::vector(lower=0)[2]\n            theta::real(lower=0,upper=1)\n        end\n        @model begin\n            sigma ~ normal(0, 2);\n            mu ~ normal(0, 2);\n            theta ~ beta(5, 5);\n            for n in 1:N\n              target += log_mix(theta, normal_lpdf(y[n], mu[1], sigma[1]),\n                                normal_lpdf(y[n], mu[2], sigma[2]));\n            end\n        end\n    end\nend\njulia_implementation(::Val{:normal_mixture_k}; K, N, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::simplex[K]\n            mu::vector[K]\n            sigma::vector(lower=0.,upper=10.)[K]\n        end\n        @model begin\n            mu ~ normal(0., 10.);\n            for n in 1:N\n                ps = @broadcasted(log(theta) + normal_lpdf(y[n], mu, sigma))\n                target += log_sum_exp(ps);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:low_dim_gauss_mix}; N, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::ordered[2]\n            sigma::vector(lower=0)[2]\n            theta::real(lower=0,upper=1)\n        end\n        @model begin\n            sigma ~ normal(0, 2);\n            mu ~ normal(0, 2);\n            theta ~ beta(5, 5);\n            for n in 1:N\n                target += log_mix(theta, normal_lpdf(y[n], mu[1], sigma[1]),\n                                normal_lpdf(y[n], mu[2], sigma[2]));\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_county}; N, J, county, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            a::vector[J]\n            mu_a::real\n            sigma_a::real(lower=0., upper=100.)\n            sigma_y::real(lower=0., upper=100.)\n        end\n        @model @views begin\n            # y_hat = a[county]\n            y_hat = StanBlocks.constview(a, county)\n            \n            mu_a ~ normal(0., 1.);\n            a ~ normal(mu_a, sigma_a);\n            y ~ normal(y_hat, sigma_y);\n        end\n    end\nend\njulia_implementation(::Val{:logearn_logheight_male}; N, earn, height, male, kwargs...) = begin \n    log_earn = @. log(earn)\n    log_height = @. log(height)\n    @stan begin \n        @parameters begin\n            beta::vector[3]\n            sigma::real(lower=0)\n        end\n        @model begin\n            log_earn ~ normal(@broadcasted(beta[1] + beta[2] * log_height + beta[3] * male), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dae_model}; N, switched, dist, arsenic, educ, kwargs...) = begin \n    dist100 = @. dist / 100.\n    educ4 = @. educ / 4.\n    X = hcat(dist100, arsenic, educ4)\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[3]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:arK}; K, T, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[K]\n            sigma::real(lower=0)\n        end\n        @model begin\n            alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            sigma ~ cauchy(0, 2.5);\n            for t in K+1:T\n                mu = alpha\n                for k in 1:K\n                    mu += beta[k] * y[t-k]\n                end\n                y[t] ~ normal(mu, sigma)\n            end\n        end\n    end\nend\njulia_implementation(::Val{:wells_interaction_model}; N, switched, dist, arsenic, kwargs...) = begin \n    dist100 = @. dist / 100.\n    inter = @. dist100 * arsenic\n    X = hcat(dist100, arsenic, inter)\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[3]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:radon_pooled}; N, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::real\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_y ~ normal(0, 1);\n            alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n\n            log_radon ~ normal(@broadcasted(alpha + beta * floor_measure), sigma_y)\n        end\n    end\nend\njulia_implementation(::Val{:logearn_interaction}; N, earn, height, male, kwargs...) = begin \n        log_earn = @. log(earn)\n        inter = @. height * male\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_earn ~ normal(@broadcasted(beta[1] + beta[2] * height + beta[3] * male + beta[4] * inter), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:logmesquite_logvolume}; N, weight, diam1, diam2, canopy_height, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_canopy_volume = @. log(diam1 * diam2 * canopy_height);\n@stan begin \n            @parameters begin\n                beta::vector[2]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_canopy_volume), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:garch11}; T, y, sigma1, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::real\n            alpha0::real(lower=0.)\n            alpha1::real(lower=0., upper=1.)\n            beta1::real(lower=0., upper=1. - alpha1)\n        end\n        @model begin\n            sigma = sigma1\n            y[1] ~ normal(mu, sigma)\n            for t in 2:T\n                sigma = sqrt(alpha0 + alpha1 * square(y[t - 1] - mu) + beta1 * square(sigma))\n                y[t] ~ normal(mu, sigma)\n            end\n        end\n    end\nend\njulia_implementation(::Val{:eight_schools_centered}; J, y, sigma, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::vector[J]\n            mu::real\n            tau::real(lower=0)\n        end\n        @model begin\n            tau ~ cauchy(0, 5);\n            theta ~ normal(mu, tau);\n            y ~ normal(theta, sigma);\n            mu ~ normal(0, 5);\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_interaction}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n        inter = @. mom_hs * mom_iq;\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                sigma ~ cauchy(0, 2.5);\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * mom_hs + beta[3] * mom_iq + beta[4] * inter), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:mesquite}; N, weight, diam1, diam2, canopy_height, total_height, density, group, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[7]\n            sigma::real(lower=0)\n        end\n        @model begin\n            weight ~ normal(@broadcasted(beta[1] + beta[2] * diam1 + beta[3] * diam2\n            + beta[4] * canopy_height + beta[5] * total_height\n            + beta[6] * density + beta[7] * group), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:gp_regr}; N, x, y, kwargs...) = begin \n@stan begin \n            @parameters begin\n                rho::real(lower=0)\n                alpha::real(lower=0)\n                sigma::real(lower=0)\n            end\n            @model begin\n                cov = gp_exp_quad_cov(x, alpha, rho) + diag_matrix(rep_vector(sigma, N));\n                # L_cov = cholesky_decompose(cov);\n  \n                rho ~ gamma(25, 4);\n                alpha ~ normal(0, 2);\n                sigma ~ normal(0, 1);\n                \n                y ~ multi_normal(rep_vector(0., N), cov);\n                # Think about how to do this\n                # y ~ multi_normal_cholesky(rep_vector(0, N), L_cov);\n            end\n    end\nend\njulia_implementation(::Val{:kidscore_mom_work}; N, kid_score, mom_work, kwargs...) = begin \n        work2 = @. Float64(mom_work == 2)\n        work3 = @. Float64(mom_work == 3)\n        work4 = @. Float64(mom_work == 4)\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * work2 + beta[3] * work3\n                + beta[4] * work4), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:Rate_2_model}; n1, n2, k1, k2, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta1::real(lower=0,upper=1)\n            theta2::real(lower=0,upper=1)\n        end\n        delta = theta1 - theta2\n        @model begin\n            theta1 ~ beta(1, 1)\n            theta2 ~ beta(1, 1)\n            k1 ~ binomial(n1, theta1)\n            k2 ~ binomial(n2, theta2)\n        end\n    end\nend\njulia_implementation(::Val{:wells_interaction_c_model}; N, switched, dist, arsenic, kwargs...) = begin \n        c_dist100 = @. (dist - $mean(dist)) / 100.0;\n        c_arsenic = @. arsenic - $mean(arsenic);\n        inter = @. c_dist100 * c_arsenic\n        X = hcat(c_dist100, c_arsenic, inter)\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::vector[3]\n            end\n            @model begin\n                switched ~ bernoulli_logit_glm(X, alpha, beta);\n            end\n        end\nend\njulia_implementation(::Val{:radon_county_intercept}; N, J, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[J]\n            beta::real\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_y ~ normal(0, 1);\n            alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            for n in 1:N\n                mu = alpha[county_idx[n]] + beta * floor_measure[n];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_interaction_c}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n        c_mom_hs = @. mom_hs - $mean(mom_hs);\n        c_mom_iq = @. mom_iq - $mean(mom_iq);\n        inter = @. c_mom_hs * c_mom_iq;\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * c_mom_hs + beta[3] * c_mom_iq + beta[4] * inter), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:kidscore_interaction_c2}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n        c_mom_hs = @. mom_hs - .5;\n        c_mom_iq = @. mom_iq - 100;\n        inter = @. c_mom_hs * c_mom_iq;\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * c_mom_hs + beta[3] * c_mom_iq + beta[4] * inter), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:gp_pois_regr}; N, x, k, kwargs...) = begin \n        nugget = diag_matrix(rep_vector(1e-10, N))\n@stan begin \n            @parameters begin\n                rho::real(lower=0)\n                alpha::real(lower=0)\n                f_tilde::vector[N]\n            end\n            @transformed_parameters begin \n                cov = gp_exp_quad_cov(x, alpha, rho)\n                cov .+= nugget;\n                L_cov = cholesky_decompose(cov);\n                f = L_cov * f_tilde;\n            end\n            @model begin\n                rho ~ gamma(25, 4);\n                alpha ~ normal(0, 2);\n                f_tilde ~ normal(0, 1);\n                \n                k ~ poisson_log(f);\n            end\n    end\nend\njulia_implementation(::Val{:surgical_model}; N, r, n, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::real\n            sigmasq::real(lower=0)\n            b::vector[N]\n        end\n        @transformed_parameters begin \n            sigma = sqrt(sigmasq)\n            p = @broadcasted inv_logit(b)\n        end\n        @model begin\n            mu ~ normal(0.0, 1000.0);\n            sigmasq ~ inv_gamma(0.001, 0.001);\n            b ~ normal(mu, sigma);\n            r ~ binomial_logit(n, b);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dae_c_model}; N, switched, dist, arsenic, educ, kwargs...) = begin \n    c_dist100 = @. (dist - $mean(dist)) / 100.0;\n    c_arsenic = @. arsenic - $mean(arsenic);\n    da_inter = @. c_dist100 * c_arsenic;\n    educ4 = @. educ / 4.\n    X = hcat(c_dist100, c_arsenic, da_inter, educ4)\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[4]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:Rate_4_model}; n, k, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0,upper=1)\n            thetaprior::real(lower=0,upper=1)\n        end\n        @model begin\n            theta ~ beta(1, 1);\n            thetaprior ~ beta(1, 1);\n            k ~ binomial(n, theta);\n        end\n    end\nend\njulia_implementation(::Val{:logearn_interaction_z}; N, earn, height, male, kwargs...) = begin \n        log_earn = @. log(earn)\n        z_height = @. (height - $mean(height)) / $sd(height);\n        inter = @. z_height * male\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_earn ~ normal(@broadcasted(beta[1] + beta[2] * z_height + beta[3] * male + beta[4] * inter), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:normal_mixture}; N, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0,upper=1)\n            mu::vector[2]\n        end\n        @model begin\n            theta ~ uniform(0, 1); \n            for k in 1:2\n                mu[k] ~ normal(0, 10);\n            end\n            for n in 1:N\n                target += log_mix(theta, normal_lpdf(y[n], mu[1], 1.0),\n                                normal_lpdf(y[n], mu[2], 1.0));\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_partially_pooled_centered}; N, J, county_idx, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[J]\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n\n            alpha ~ normal(mu_alpha, sigma_alpha);\n            for n in 1:N\n                mu = alpha[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_interaction_z}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n        c_mom_hs = @. (mom_hs - $mean(mom_hs)) / (2 * $sd(mom_hs));\n        c_mom_iq = @. (mom_iq - $mean(mom_iq)) / (2 * $sd(mom_iq));\n        inter = @. c_mom_hs * c_mom_iq;\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * c_mom_hs + beta[3] * c_mom_iq + beta[4] * inter), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:kilpisjarvi}; N, x, y, xpred, pmualpha, psalpha, pmubeta, psbeta, kwargs...) = begin \n        x_ = x\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::real\n                sigma::real(lower=0)\n            end\n            @model begin\n                alpha ~ normal(pmualpha, psalpha);\n                beta ~ normal(pmubeta, psbeta);\n                y ~ normal(@broadcasted(alpha + beta * x_), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:wells_daae_c_model}; N, switched, dist, arsenic, assoc, educ, kwargs...) = begin \n        c_dist100 = @. (dist - $mean(dist)) / 100.0;\n        c_arsenic = @. arsenic - $mean(arsenic);\n        da_inter = @. c_dist100 * c_arsenic;\n        educ4 = @. educ / 4.\n        X = hcat(c_dist100, c_arsenic, da_inter, assoc, educ4)\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::vector[5]\n            end\n            @model begin\n                switched ~ bernoulli_logit_glm(X, alpha, beta);\n            end\n    end\nend\njulia_implementation(::Val{:eight_schools_noncentered}; J, y, sigma, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta_trans::vector[J]\n            mu::real\n            tau::real(lower=0)\n        end\n        theta = @broadcasted(theta_trans * tau + mu);\n        @model begin\n            theta_trans ~ normal(0, 1);\n            y ~ normal(theta, sigma);\n            mu ~ normal(0, 5);\n            tau ~ cauchy(0, 5);\n        end\n    end\nend\njulia_implementation(::Val{:Rate_5_model}; n1, n2, k1, k2, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0,upper=1)\n        end\n        @model begin\n            theta ~ beta(1, 1);\n            k1 ~ binomial(n1, theta);\n            k2 ~ binomial(n2, theta);\n        end\n    end\nend\njulia_implementation(::Val{:dugongs_model}; N, x, Y, kwargs...) = begin \n        x_ = x\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::real\n                lambda::real(lower=.5,upper=1.)\n                tau::real(lower=0.)\n            end\n            @transformed_parameters begin\n                sigma = 1. / sqrt(tau);\n                U3 = logit(lambda);\n            end\n            @model begin\n                for i in 1:N\n                    m = alpha - beta * pow(lambda, x_[i]);\n                    Y[i] ~ normal(m, sigma);\n                end\n                \n                alpha ~ normal(0.0, 1000.);\n                beta ~ normal(0.0, 1000.);\n                lambda ~ uniform(.5, 1.);\n                tau ~ gamma(.0001, .0001);\n            end\n    end\nend\njulia_implementation(::Val{:irt_2pl}; I, J, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            sigma_theta::real(lower=0);\n            theta::vector[J];\n\n            sigma_a::real(lower=0);\n            a::vector(lower=0)[I];\n\n            mu_b::real;\n            sigma_b::real(lower=0);\n            b::vector[I];\n        end\n        @model begin\n            sigma_theta ~ cauchy(0, 2);\n            theta ~ normal(0, sigma_theta);\n            \n            sigma_a ~ cauchy(0, 2);\n            a ~ lognormal(0, sigma_a);\n            \n            mu_b ~ normal(0, 5);\n            sigma_b ~ cauchy(0, 2);\n            b ~ normal(mu_b, sigma_b);\n            \n            for i in 1:I\n                y[i,:] ~ bernoulli_logit(@broadcasted(a[i] * (theta - b[i])));\n            end\n        end\n    end\nend\njulia_implementation(::Val{:logmesquite_logva}; N, weight, diam1, diam2, canopy_height, group, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_canopy_volume = @. log(diam1 * diam2 * canopy_height);\n        log_canopy_area = @. log(diam1 * diam2)\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_canopy_volume\n                + beta[3] * log_canopy_area + beta[4] * group), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:radon_variable_slope_centered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[J]\n            mu_beta::real\n            sigma_beta::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            alpha ~ normal(0, 10);\n            sigma_y ~ normal(0, 1);\n            sigma_beta ~ normal(0, 1);\n            mu_beta ~ normal(0, 10);\n            \n            beta ~ normal(mu_beta, sigma_beta);\n            for n in 1:N\n                mu = alpha + floor_measure[n] * beta[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_variable_intercept_centered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[J]\n            beta::real\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            \n            alpha ~ normal(mu_alpha, sigma_alpha);\n            for n in 1:N\n                mu = alpha[county_idx[n]] + floor_measure[n] * beta;\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:seeds_stanified_model}; I, n, N, x1, x2, kwargs...) = begin \n        x1x2 = @. x1 * x2;\n@stan begin \n            @parameters begin\n                alpha0::real;\n                alpha1::real;\n                alpha12::real;\n                alpha2::real;\n                b::vector[I];\n                sigma::real(lower=0);\n            end\n            @model begin\n                alpha0 ~ normal(0.0, 1.0);\n                alpha1 ~ normal(0.0, 1.0);\n                alpha2 ~ normal(0.0, 1.0);\n                alpha12 ~ normal(0.0, 1.0);\n                sigma ~ cauchy(0, 1);\n                \n                b ~ normal(0.0, sigma);\n                n ~ binomial_logit(N,\n                                   @broadcasted(alpha0 + alpha1 * x1 + alpha2 * x2 + alpha12 * x1x2 + b));\n            end\n    end\nend\njulia_implementation(::Val{:state_space_stochastic_level_stochastic_seasonal}; n, y, x, w, kwargs...) = begin \n        x_ = x\n        mu_lower = mean(y) - 3 * sd(y)\n        mu_upper = mean(y) + 3 * sd(y)\n@stan begin \n            @parameters begin\n                mu::vector(lower=mu_lower, upper=mu_upper)[n]\n                seasonal::vector[n]\n                beta::real\n                lambda::real\n                sigma::positive_ordered[3]\n            end\n            @model @views begin\n                for t in 12:n\n                    seasonal[t] ~ normal(-sum(seasonal[t-11:t-1]), sigma[1]);\n                end\n                \n                for t in 2:n\n                    mu[t] ~ normal(mu[t - 1], sigma[2]);\n                end\n                \n                y ~ normal(@broadcasted(mu + beta * x_ + lambda * w + seasonal), sigma[3]);\n                \n                sigma ~ student_t(4, 0, 1);\n            end\n        end\nend\njulia_implementation(::Val{:radon_partially_pooled_noncentered}; N, J, county_idx, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha_raw::vector[J]\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        alpha = @.(mu_alpha + sigma_alpha * alpha_raw);\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            alpha_raw ~ normal(0, 1);\n\n            for n in 1:N\n                mu = alpha[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:seeds_model}; I, n, N, x1, x2, kwargs...) = begin \n        x1x2 = @. x1 * x2;\n@stan begin \n            @parameters begin\n                alpha0::real;\n                alpha1::real;\n                alpha12::real;\n                alpha2::real;\n                tau::real(lower=0);\n                b::vector[I];\n            end\n            sigma = 1.0 / sqrt(tau);\n            @model begin\n                alpha0 ~ normal(0.0, 1.0E3);\n                alpha1 ~ normal(0.0, 1.0E3);\n                alpha2 ~ normal(0.0, 1.0E3);\n                alpha12 ~ normal(0.0, 1.0E3);\n                tau ~ gamma(1.0E-3, 1.0E-3);\n                \n                b ~ normal(0.0, sigma);\n                n ~ binomial_logit(N,\n                                   @broadcasted(alpha0 + alpha1 * x1 + alpha2 * x2 + alpha12 * x1x2 + b));\n            end\n        end\nend\njulia_implementation(::Val{:arma11}; T, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::real\n            phi::real\n            theta::real\n            sigma::real(lower=0)\n        end\n        @model begin\n            mu ~ normal(0, 10);\n            phi ~ normal(0, 2);\n            theta ~ normal(0, 2);\n            sigma ~ cauchy(0, 2.5);\n            nu = mu + phi * mu\n            err = y[1] - nu\n            err ~ normal(0, sigma);\n            for t in 2:T\n                nu = mu + phi * y[t-1] + theta * err\n                err = y[t] - nu\n                err ~ normal(0, sigma);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_hierarchical_intercept_centered}; J, N, county_idx, log_uppm, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[J]\n            beta::vector[2]\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_alpha ~ normal(0, 1);\n            sigma_y ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            \n            alpha ~ normal(mu_alpha, sigma_alpha);\n            for n in 1:N\n                muj = alpha[county_idx[n]] + log_uppm[n] * beta[1]\n                mu = muj + floor_measure[n] * beta[2];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:seeds_centered_model}; I, n, N, x1, x2, kwargs...) = begin \n        x1x2 = @. x1 * x2;\n@stan begin \n            @parameters begin\n                alpha0::real;\n                alpha1::real;\n                alpha12::real;\n                alpha2::real;\n                c::vector[I];\n                sigma::real(lower=0);\n            end\n            b = @. c - $mean(c);\n            @model begin\n                alpha0 ~ normal(0.0, 1.0);\n                alpha1 ~ normal(0.0, 1.0);\n                alpha2 ~ normal(0.0, 1.0);\n                alpha12 ~ normal(0.0, 1.0);\n                sigma ~ cauchy(0, 1);\n                \n                c ~ normal(0.0, sigma);\n                n ~ binomial_logit(N,\n                                   @broadcasted(alpha0 + alpha1 * x1 + alpha2 * x2 + alpha12 * x1x2 + b));\n            end\n    end\nend\njulia_implementation(::Val{:pilots}; N, n_groups, n_scenarios, group_id, scenario_id, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            a::vector[n_groups];\n            b::vector[n_scenarios];\n            mu_a::real;\n            mu_b::real;\n            sigma_a::real(lower=0, upper=100);\n            sigma_b::real(lower=0, upper=100);\n            sigma_y::real(lower=0, upper=100);\n        end\n        y_hat = @broadcasted(a[group_id] + b[scenario_id]);\n        @model begin\n            mu_a ~ normal(0, 1);\n            a ~ normal(10 * mu_a, sigma_a);\n            \n            mu_b ~ normal(0, 1);\n            b ~ normal(10 * mu_b, sigma_b);\n            \n            y ~ normal(y_hat, sigma_y);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dae_inter_model}; N, switched, dist, arsenic, educ, kwargs...) = begin \n        c_dist100 = @. (dist - $mean(dist)) / 100.0;\n        c_arsenic = @. arsenic - $mean(arsenic);\n        c_educ4 = @. (educ - $mean(educ)) / 4.\n        da_inter = @. c_dist100 * c_arsenic;\n        de_inter = @. c_dist100 * c_educ4;\n        ae_inter = @. c_arsenic * c_educ4;\n        X = hcat(c_dist100, c_arsenic, c_educ4, da_inter, de_inter, ae_inter, )\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::vector[6]\n            end\n            @model begin\n                switched ~ bernoulli_logit_glm(X, alpha, beta);\n            end\n    end\nend\njulia_implementation(::Val{:radon_variable_slope_noncentered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::real\n            beta_raw::vector[J]\n            mu_beta::real\n            sigma_beta::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        beta = @. mu_beta + sigma_beta * beta_raw;\n        @model begin\n            alpha ~ normal(0, 10);\n            sigma_y ~ normal(0, 1);\n            sigma_beta ~ normal(0, 1);\n            mu_beta ~ normal(0, 10);\n            beta_raw ~ normal(0, 1);\n\n            for n in 1:N\n                mu = alpha + floor_measure[n] * beta[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_variable_intercept_slope_centered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n@stan begin \n            @parameters begin\n                sigma_y::real(lower=0)\n                sigma_alpha::real(lower=0)\n                sigma_beta::real(lower=0)\n                alpha::vector[J]\n                beta::vector[J]\n                mu_alpha::real\n                mu_beta::real\n            end\n            @model begin\n                sigma_y ~ normal(0, 1);\n                sigma_beta ~ normal(0, 1);\n                sigma_alpha ~ normal(0, 1);\n                mu_alpha ~ normal(0, 10);\n                mu_beta ~ normal(0, 10);\n                \n                alpha ~ normal(mu_alpha, sigma_alpha);\n                beta ~ normal(mu_beta, sigma_beta);\n                for n in 1:N\n                    mu = alpha[county_idx[n]] + floor_measure[n] * beta[county_idx[n]];\n                    target += normal_lpdf(log_radon[n], mu, sigma_y);\n                end\n            end\n        end\nend\njulia_implementation(::Val{:radon_variable_intercept_noncentered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha_raw::vector[J]\n            beta::real\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        alpha = @. mu_alpha + sigma_alpha * alpha_raw;\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            alpha_raw ~ normal(0, 1);\n\n            for n in 1:N\n                mu = alpha[county_idx[n]] + floor_measure[n] * beta;\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:GLM_Poisson_model}; n, C, year, kwargs...) = begin \n        year_squared = year .^ 2\n        year_cubed = year .^ 3\n@stan begin \n            @parameters begin\n                alpha::real(lower=-20, upper=+20)\n                beta1::real(lower=-10, upper=+10)\n                beta2::real(lower=-10, upper=+10)\n                beta3::real(lower=-10, upper=+10)\n            end\n            log_lambda = @broadcasted(alpha + beta1 * year + beta2 * year_squared + beta3 * year_cubed)\n            @model begin\n                C ~ poisson_log(log_lambda);\n            end\n    end\nend\njulia_implementation(::Val{:ldaK5}; V, M, N, w, doc, alpha, beta, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::simplex[M,5];\n            phi::simplex[5,V];\n        end\n        @model @views begin\n            for m in 1:M\n                theta[m,:] ~ dirichlet(alpha);\n            end\n            for k in 1:5\n                phi[k,:] ~ dirichlet(beta);\n            end\n            for n in 1:N\n                gamma = @broadcasted(log(theta[doc[n], :]) + log(phi[:, w[n]]))\n                target += log_sum_exp(gamma);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:dogs}; n_dogs, n_trials, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[3];\n        end\n        @model begin\n            beta ~ normal(0, 100);\n            for i in 1:n_dogs\n                n_avoid = 0\n                n_shock = 0\n                for j in 1:n_trials\n                    p = beta[1] + beta[2] * n_avoid + beta[3] * n_shock\n                    y[i, j] ~ bernoulli_logit(p);\n                    n_avoid += 1 - y[i,j]\n                    n_shock += y[i,j]\n                end\n            end\n        end\n    end\nend\njulia_implementation(::Val{:nes}; N, partyid7, real_ideo, race_adj, educ1, gender, income, age_discrete, kwargs...) = begin \n        age30_44 = @. Float64(age_discrete == 2);\n        age45_64 = @. Float64(age_discrete == 3);\n        age65up = @. Float64(age_discrete == 4);\n@stan begin \n            @parameters begin\n                beta::vector[9]\n                sigma::real(lower=0)\n            end\n            @model begin\n                partyid7 ~ normal(@broadcasted(beta[1] + beta[2] * real_ideo + beta[3] * race_adj\n                + beta[4] * age30_44 + beta[5] * age45_64\n                + beta[6] * age65up + beta[7] * educ1 + beta[8] * gender\n                + beta[9] * income), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:dogs_log}; n_dogs, n_trials, y, kwargs...) = begin \n    @assert size(y) == (n_dogs, n_trials)\n@stan begin \n        @parameters begin\n            beta::vector[2];\n        end\n        @model begin\n            beta[1] ~ uniform(-100, 0);\n            beta[2] ~ uniform(0, 100);\n            for i in 1:n_dogs\n                n_avoid = 0\n                n_shock = 0\n                for j in 1:n_trials\n                    p = inv_logit(beta[1] * n_avoid + beta[2] * n_shock)\n                    y[i, j] ~ bernoulli(p);\n                    n_avoid += 1 - y[i,j]\n                    n_shock += y[i,j]\n                end\n            end\n        end\n    end\nend\njulia_implementation(::Val{:GLM_Binomial_model};nyears, C, N, year, kwargs...) = begin \n        year_squared = year .^ 2\n@stan begin \n            @parameters begin\n                alpha::real\n                beta1::real\n                beta2::real\n            end\n            logit_p = @broadcasted alpha + beta1 * year + beta2 * year_squared;\n            @model begin\n                alpha ~ normal(0, 100)\n                beta1 ~ normal(0, 100)\n                beta2 ~ normal(0, 100)\n                C ~ binomial_logit(N, logit_p)\n            end\n    end\nend\njulia_implementation(::Val{:radon_hierarchical_intercept_noncentered}; J, N, county_idx, log_uppm, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha_raw::vector[J]\n            beta::vector[2]\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        alpha = @. mu_alpha + sigma_alpha * alpha_raw;\n        @model begin\n            sigma_alpha ~ normal(0, 1);\n            sigma_y ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            alpha_raw ~ normal(0, 1);\n\n            for n in 1:N\n                muj = alpha[county_idx[n]] + log_uppm[n] * beta[1]\n                mu = muj + floor_measure[n] * beta[2];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:logmesquite_logvash}; N, weight, diam1, diam2, canopy_height, total_height, group, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_canopy_volume = @. log(diam1 * diam2 * canopy_height);\n        log_canopy_area = @. log(diam1 * diam2)\n        log_canopy_shape = @. log(diam1 / diam2);\n        log_total_height = @. log(total_height);\n@stan begin \n            @parameters begin\n                beta::vector[6]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_canopy_volume\n                + beta[3] * log_canopy_area\n                + beta[4] * log_canopy_shape\n                + beta[5] * log_total_height + beta[6] * group), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:ldaK2}; V, M, N, w, doc, kwargs...) = begin \n        K = 2\n        alpha = fill(1, K)\n        beta = fill(1, V)\n@stan begin \n            @parameters begin\n                theta::simplex[M,K];\n                phi::simplex[K,V];\n            end\n            @model @views begin\n                for m in 1:M\n                  theta[m,:] ~ dirichlet(alpha);\n                end\n                for k in 1:K\n                  phi[k,:] ~ dirichlet(beta);\n                end\n                for n in 1:N\n                    gamma = @broadcasted log(theta[doc[n], :]) + log(phi[:, w[n]])\n                  target += log_sum_exp(gamma);\n                end\n            end\n    end\nend\njulia_implementation(::Val{:logmesquite}; N, weight, diam1, diam2, canopy_height, total_height, density, group, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_diam1 = @. log(diam1);\n        log_diam2 = @. log(diam2);\n        log_canopy_height = @. log(canopy_height);\n        log_total_height = @. log(total_height);\n        log_density = @. log(density);\n@stan begin \n            @parameters begin\n                beta::vector[7]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_diam1 + beta[3] * log_diam2\n                + beta[4] * log_canopy_height\n                + beta[5] * log_total_height + beta[6] * log_density\n                + beta[7] * group), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:rats_model}; N, Npts, rat, x, y, xbar, kwargs...) = begin \n        x_ = x\n@stan begin \n            @parameters begin\n                alpha::vector[N];\n                beta::vector[N];\n                \n                mu_alpha::real;\n                mu_beta::real;\n                sigma_y::real(lower=0);\n                sigma_alpha::real(lower=0);\n                sigma_beta::real(lower=0);\n            end\n            @model begin\n                mu_alpha ~ normal(0, 100);\n                mu_beta ~ normal(0, 100);\n                alpha ~ normal(mu_alpha, sigma_alpha);\n                beta ~ normal(mu_beta, sigma_beta);\n                for n in 1:Npts\n                  irat = rat[n];\n                  y[n] ~ normal(alpha[irat] + beta[irat] * (x_[n] - xbar), sigma_y);\n                end\n            end\n    end\nend\njulia_implementation(::Val{:logmesquite_logvas}; N, weight, diam1, diam2, canopy_height, total_height, density, group, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_canopy_volume = @. log(diam1 * diam2 * canopy_height);\n        log_canopy_area = @. log(diam1 * diam2)\n        log_canopy_shape = @. log(diam1 / diam2);\n        log_total_height = @. log(total_height);\n        log_density = @. log(density);\n@stan begin \n            @parameters begin\n                beta::vector[7]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_canopy_volume\n                + beta[3] * log_canopy_area\n                + beta[4] * log_canopy_shape\n                + beta[5] * log_total_height + beta[6] * log_density\n                + beta[7] * group), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:lsat_model}; N, R, T, culm, response, kwargs...) = begin \n    r = zeros(Int, (T, N))\n    for j in 1:culm[1], k in 1:T\n        r[k, j] = response[1, k];\n    end\n    for i in 2:R\n        for j in (culm[i-1]+1):culm[i], k in 1:T\n            r[k, j] = response[i, k];\n        end\n    end\n    ones = fill(1., N)\n    @stan begin \n        @parameters begin\n            alpha::vector[T];\n            theta::vector[N];\n            beta::real(lower=0);\n        end\n        @model @views begin\n            alpha ~ normal(0, 100.);\n            theta ~ normal(0, 1);\n            beta ~ normal(0.0, 100.);\n            for k in 1:T\n                r[k,:] ~ bernoulli_logit(beta * theta - alpha[k] * ones);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_variable_intercept_slope_noncentered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            sigma_y::real(lower=0)\n            sigma_alpha::real(lower=0)\n            sigma_beta::real(lower=0)\n            alpha_raw::vector[J]\n            beta_raw::vector[J]\n            mu_alpha::real\n            mu_beta::real\n        end\n        alpha = @. mu_alpha + sigma_alpha * alpha_raw;\n        beta = @. mu_beta + sigma_beta * beta_raw;\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_beta ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            mu_beta ~ normal(0, 10);\n            alpha_raw ~ normal(0, 1);\n            beta_raw ~ normal(0, 1);\n\n            for n in 1:N\n                mu = alpha[county_idx[n]] + floor_measure[n] * beta[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:GLMM_Poisson_model};n, C, year) = begin \n    year_squared = year .^ 2\n    year_cubed = year .^ 3\n    @stan begin\n        @parameters begin\n            alpha::real(lower=-20, upper=+20)\n            beta1::real(lower=-10, upper=+10)\n            beta2::real(lower=-10, upper=+20)\n            beta3::real(lower=-10, upper=+10)\n            eps::vector[n]\n            sigma::real(lower=0, upper=5)\n        end\n        log_lambda = @broadcasted alpha + beta1 * year + beta2 * year_squared + beta3 * year_cubed + eps\n        @model begin\n            C ~ poisson_log(log_lambda)\n            eps ~ normal(0, sigma)\n        end\n    end\nend\njulia_implementation(::Val{:GLMM1_model};nsite, nobs, obs, obsyear, obssite, misyear, missite, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[nsite]\n            mu_alpha::real\n            sd_alpha::real(lower=0,upper=5)\n        end\n    #   log_lambda = rep_matrix(alpha', nyear);\n        @model begin\n            alpha ~ normal(mu_alpha, sd_alpha)\n            mu_alpha ~ normal(0, 10)\n            for i in 1:nobs\n                # obs[i] ~ poisson_log(log_lambda[obsyear[i], obssite[i]])\n                obs[i] ~ poisson_log(alpha[obssite[i]])\n            end\n        end\n    end\nend\njulia_implementation(::Val{:hier_2pl}; I, J, N, ii, jj, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::vector[J];\n            xi1::vector[I];\n            xi2::vector[I];\n            mu::vector[2];\n            tau::vector(lower=0)[2];\n            L_Omega::cholesky_factor_corr[2]\n        end\n        xi = hcat(xi1, xi2)\n        alpha = @. exp(xi1);\n        beta = xi2;\n        @model begin\n            L_Sigma = diag_pre_multiply(tau, L_Omega);\n            for i in 1:I\n                target += multi_normal_cholesky_lpdf(xi[i, :], mu, L_Sigma);\n            end\n            theta ~ normal(0, 1);\n            L_Omega ~ lkj_corr_cholesky(4);\n            mu[1] ~ normal(0, 1);\n            tau[1] ~ exponential(.1);\n            mu[2] ~ normal(0, 5);\n            tau[2] ~ exponential(.1);\n            y ~ bernoulli_logit(alpha[ii] .* (theta[jj] - beta[ii]));\n        end\n    end\nend\njulia_implementation(::Val{:dogs_hierarchical}; n_dogs, n_trials, y, kwargs...) = begin \n        J = n_dogs;\n        T = n_trials;\n        prev_shock = zeros((J,T));\n        prev_avoid = zeros((J,T));\n        \n        for j in 1:J\n            prev_shock[j, 1] = 0;\n            prev_avoid[j, 1] = 0;\n            for t in 2:T\n                prev_shock[j, t] = prev_shock[j, t - 1] + y[j, t - 1];\n                prev_avoid[j, t] = prev_avoid[j, t - 1] + 1 - y[j, t - 1];\n            end\n        end\n@stan begin \n            @parameters begin\n                a::real(lower=0, upper=1);\n                b::real(lower=0, upper=1);\n            end\n            @model begin\n                y ~ bernoulli(@broadcasted(a ^ prev_shock * b ^ prev_avoid));\n            end\n    end\nend\n\n\njulia_implementation(::Val{:dogs_nonhierarchical}; n_dogs, n_trials, y, kwargs...) = begin \n    J = n_dogs;\n    T = n_trials;\n    prev_shock = zeros((J,T));\n    prev_avoid = zeros((J,T));\n    \n    for j in 1:J\n        prev_shock[j, 1] = 0;\n        prev_avoid[j, 1] = 0;\n        for t in 2:T\n            prev_shock[j, t] = prev_shock[j, t - 1] + y[j, t - 1];\n            prev_avoid[j, t] = prev_avoid[j, t - 1] + 1 - y[j, t - 1];\n        end\n    end\n    @stan begin \n        @parameters begin\n            mu_logit_ab::vector[2]\n            sigma_logit_ab::vector(lower=0)[2]\n            L_logit_ab::cholesky_factor_corr[2]\n            z::matrix[J, 2]\n        end\n        @model @views begin\n            logit_ab = rep_vector(1, J) * mu_logit_ab' + z * diag_pre_multiply(sigma_logit_ab, L_logit_ab);\n            a = inv_logit.(logit_ab[ : , 1]);\n            b = inv_logit.(logit_ab[ : , 2]);\n            y ~ bernoulli(@broadcasted(a ^ prev_shock * b ^ prev_avoid));\n            mu_logit_ab ~ logistic(0, 1);\n            sigma_logit_ab ~ normal(0, 1);\n            L_logit_ab ~ lkj_corr_cholesky(2);\n            (z) ~ normal(0, 1);\n        end\n    end\nend\njulia_implementation(::Val{:M0_model}; M, T, y, kwargs...) = begin\n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0,upper=1)\n                p::real(lower=0,upper=1)\n            end\n            @model begin\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + binomial_lpmf(s[i], T, p)\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + binomial_lpmf(0, T, p),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n    end\nend\njulia_implementation(::Val{:diamonds}; N, Y, K, X, prior_only, kwargs...) = begin\n        Kc = K - 1;\n        Xc = zeros((N, Kc))\n        means_X = zeros(Kc)\n        for i in 2:K\n            means_X[i - 1] = mean(X[ : , i]);\n            @. Xc[ : , i - 1] = X[ : , i] - means_X[i - 1];\n        end\n@stan begin \n            @parameters begin\n                b::vector[Kc];\n                Intercept::real;\n                sigma::real(lower=0.)\n            end\n            @model begin\n                target += normal_lpdf(b, 0., 1.);\n                target += student_t_lpdf(Intercept, 3., 8., 10.);\n                target += student_t_lpdf(sigma, 3., 0., 10.)# - 1 * student_t_lccdf(0, 3, 0, 10);\n                if !(prior_only == 1)\n                    target += normal_id_glm_lpdf(Y, Xc, Intercept, b, sigma);\n                end\n            end\n    end\nend\njulia_implementation(::Val{:Mt_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0,upper=1)\n                p::vector(lower=0,upper=1)[T]\n            end\n            @model @views begin\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + bernoulli_lpmf(y[i,:], p)\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + bernoulli_lpmf(y[i,:], p),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n    end\nend\njulia_implementation(::Val{:election88_full}; \n    N,\n    n_age,\n    n_age_edu,\n    n_edu,\n    n_region_full,\n    n_state,\n    age,\n    age_edu,\n    black,\n    edu,\n    female,\n    region_full,\n    state,\n    v_prev_full,\n    y,\n    kwargs...) = begin \n@stan begin \n            @parameters begin\n                a::vector[n_age];\n                b::vector[n_edu];\n                c::vector[n_age_edu];\n                d::vector[n_state];\n                e::vector[n_region_full];\n                beta::vector[5];\n                sigma_a::real(lower=0, upper=100);\n                sigma_b::real(lower=0, upper=100);\n                sigma_c::real(lower=0, upper=100);\n                sigma_d::real(lower=0, upper=100);\n                sigma_e::real(lower=0, upper=100);\n            end\n            @model @views begin\n                y_hat = @broadcasted (beta[1] + beta[2] * black + beta[3] * female\n                    + beta[5] * female * black + beta[4] * v_prev_full\n                    + a[age] + b[edu] + c[age_edu] + d[state]\n                    + e[region_full])\n                a ~ normal(0, sigma_a);\n                b ~ normal(0, sigma_b);\n                c ~ normal(0, sigma_c);\n                d ~ normal(0, sigma_d);\n                e ~ normal(0, sigma_e);\n                beta ~ normal(0, 100);\n                y ~ bernoulli_logit(y_hat);\n            end\n        end\nend\njulia_implementation(::Val{:nn_rbm1bJ10}; N, M, x, K, y, kwargs...) = begin \n            J = 10\n            nu_alpha = 0.5;\n            s2_0_alpha = (0.05 / M ^ (1 / nu_alpha)) ^ 2;\n            nu_beta = 0.5;\n            s2_0_beta = (0.05 / J ^ (1 / nu_beta)) ^ 2;\n            \n            ones = rep_vector(1., N);\n            x1 = append_col(ones, x);\n\n    return @stan begin end\n    @stan begin \n        @parameters begin\n            sigma2_alpha::real(lower=0);\n            sigma2_beta::real(lower=0);\n            alpha::matrix[M, J];\n            beta::matrix[J, K - 1];\n            alpha1::row_vector[J];\n            beta1::row_vector[K - 1];\n        end\n        @model @views begin\n            v = append_col(\n                ones,\n                append_col(\n                    ones,\n                    tanh.(x1 * append_row(alpha1, alpha))\n                ) * append_row(beta1, beta)\n            );\n            alpha1 ~ normal(0, 1);\n            beta1 ~ normal(0, 1);\n            sigma2_alpha ~ inv_gamma(nu_alpha / 2, nu_alpha * s2_0_alpha / 2);\n            sigma2_beta ~ inv_gamma(nu_beta / 2, nu_beta * s2_0_beta / 2);\n            \n            (alpha) ~ normal(0, sqrt(sigma2_alpha));\n            (beta) ~ normal(0, sqrt(sigma2_beta));\n            for n in 1:N\n                y[n] ~ categorical_logit(v[n, :]);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:nn_rbm1bJ100}; N, M, x, K, y, kwargs...) = begin \n    J = 100\n    nu_alpha = 0.5;\n    s2_0_alpha = (0.05 / M ^ (1 / nu_alpha)) ^ 2;\n    nu_beta = 0.5;\n    s2_0_beta = (0.05 / J ^ (1 / nu_beta)) ^ 2;\n    \n    ones = rep_vector(1., N);\n    x1 = append_col(ones, x);\n    return @stan begin end\n    @stan begin \n        @parameters begin\n            sigma2_alpha::real(lower=0);\n            sigma2_beta::real(lower=0);\n            alpha::matrix[M, J];\n            beta::matrix[J, K - 1];\n            alpha1::row_vector[J];\n            beta1::row_vector[K - 1];\n        end\n        @model @views begin\n            v = append_col(\n                ones,\n                append_col(\n                    ones,\n                    tanh.(x1 * append_row(alpha1, alpha))\n                ) * append_row(beta1, beta)\n            );\n            alpha1 ~ normal(0, 1);\n            beta1 ~ normal(0, 1);\n            sigma2_alpha ~ inv_gamma(nu_alpha / 2, nu_alpha * s2_0_alpha / 2);\n            sigma2_beta ~ inv_gamma(nu_beta / 2, nu_beta * s2_0_beta / 2);\n            \n            (alpha) ~ normal(0, sqrt(sigma2_alpha));\n            (beta) ~ normal(0, sqrt(sigma2_beta));\n            for n in 1:N\n                y[n] ~ categorical_logit(v[n, :]);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:Survey_model}; nmax, m, k) = begin \n    nmin = maximum(k)\n    @stan begin \n        @parameters begin \n            theta::real(lower=0, upper=1)\n        end\n        @model begin \n            target += log_sum_exp([\n                n &lt; nmin ? log(1.0 / nmax) - Inf : log(1.0 / nmax) + binomial_lpmf(k, n, theta)\n                for n in 1:nmax\n            ])\n        end\n    end\nend\n\njulia_implementation(::Val{:sir}; N_t, t, y0, stoi_hat, B_hat) = begin \n    y0 = collect(Float64, y0)\n    simple_SIR(t, y, theta, x_r, x_i)  = begin\n        dydt = zero(y)\n        \n        dydt[1] = -theta[1] * y[4] / (y[4] + theta[2]) * y[1];\n        dydt[2] = theta[1] * y[4] / (y[4] + theta[2]) * y[1] - theta[3] * y[2];\n        dydt[3] = theta[3] * y[2];\n        dydt[4] = theta[4] * y[2] - theta[5] * y[4];\n        \n        return dydt;\n    end\n    t0 = 0.\n    kappa = 1000000.\n    @stan begin \n        @parameters begin \n            beta::real(lower=0)\n            gamma::real(lower=0)\n            xi::real(lower=0)\n            delta::real(lower=0)\n        end\n        @model @views begin \n            y = integrate_ode_rk45(simple_SIR, y0, t0, t, [beta, kappa, gamma, xi, delta]);\n\n            beta ~ cauchy(0, 2.5);\n            gamma ~ cauchy(0, 1);\n            xi ~ cauchy(0, 25);\n            delta ~ cauchy(0, 1);\n\n            stoi_hat[1] ~ poisson(y0[1] - y[1, 1]);\n            for n in 2:N_t\n                stoi_hat[n] ~ poisson(max(1e-16, y[n - 1, 1] - y[n, 1]));\n            end\n            \n            B_hat ~ lognormal(@broadcasted(nothrow_log(y[:, 4])), 0.15);\n        \n        end\n    end\nend\n\njulia_implementation(::Val{:lotka_volterra}; N, ts, y_init, y) = begin \n    dz_dt(t, z, theta, x_r, x_i)  = begin\n        u, v = z\n        \n        alpha, beta, gamma, delta = theta\n\n        du_dt = (alpha - beta * v) * u\n        dv_dt = (-gamma +delta * u) * v\n        [du_dt, dv_dt]\n    end\n    @stan begin \n        @parameters begin \n            theta::real(lower=0)[4]\n            z_init::real(lower=0)[2]\n            sigma::real(lower=0)[2]\n        end\n        @model @views begin \n            z = integrate_ode_rk45(dz_dt, z_init, 0, ts, theta,\n                missing, missing, 1e-5, 1e-3, 5e2);\n\n            theta[[1,3]] ~ normal(1, 0.5);\n            theta[[2,4]] ~ normal(0.05, 0.05);\n            sigma ~ lognormal(-1, 1);\n            z_init ~ lognormal(log(10), 1);\n            y_init ~ lognormal(@broadcasted(log(z_init)), sigma);\n            y ~ lognormal(@broadcasted(nothrow_log(z)), sigma');\n        end\n    end\nend\n\njulia_implementation(::Val{:soil_incubation}; totalC_t0, t0, N_t, ts, eCO2mean, kwargs...) = begin \n    two_pool_feedback(t, C, theta, x_r, x_i)  = begin\n        k1, k2, alpha21, alpha12 = theta\n        [\n            -k1 * C[1] + alpha12 * k2 * C[2]\n            -k2 * C[2] + alpha21 * k1 * C[1]\n        ]\n    end\n    evolved_CO2(N_t, t0, ts, gamma, totalC_t0, k1, k2, alpha21, alpha12) = begin \n        C_t0 = [gamma * totalC_t0, (1 - gamma) * totalC_t0]\n        theta = k1, k2, alpha21, alpha12\n        C_hat = integrate_ode_rk45(two_pool_feedback, C_t0, t0, ts, theta)\n        totalC_t0 .- sum.(eachrow(C_hat))\n    end\n    @stan begin \n        @parameters begin \n            k1::real(lower=0)\n            k2::real(lower=0)\n            alpha21::real(lower=0)\n            alpha12::real(lower=0)\n            gamma::real(lower=0, upper=1)\n            sigma::real(lower=0)\n        end\n        @model @views begin \n            eCO2_hat = evolved_CO2(N_t, t0, ts, gamma, totalC_t0, k1, k2, alpha21, alpha12)\n            gamma ~ beta(10, 1); \n            k1 ~ normal(0, 1);\n            k2 ~ normal(0, 1);\n            alpha21 ~ normal(0, 1);\n            alpha12 ~ normal(0, 1);\n            sigma ~ cauchy(0, 1);\n            eCO2mean ~ normal(eCO2_hat, sigma);\n        end\n    end\nend\n\njulia_implementation(::Val{:one_comp_mm_elim_abs}; t0, D, V, N_t, times, C_hat) = begin \n    one_comp_mm_elim_abs(t, y, theta, x_r, x_i)  = begin\n        k_a, K_m, V_m = theta\n        D, V = x_r\n        dose = 0.\n        elim = (V_m / V) * y[1] / (K_m + y[1]);\n        if t &gt; 0\n            dose = exp(-k_a * t) * D * k_a / V;\n        end\n        [dose - elim]\n    end\n    C0 = [0.]\n    x_r = [D,V]\n    x_i = missing\n    @stan begin \n        @parameters begin \n            k_a::real(lower=0)\n            K_m::real(lower=0)\n            V_m::real(lower=0)\n            sigma::real(lower=0)\n        end\n        @model @views begin \n            theta = [k_a, K_m, V_m]\n            C = integrate_ode_bdf(one_comp_mm_elim_abs, C0, t0, times, theta, x_r, x_i)\n            k_a ~ cauchy(0, 1);\n            K_m ~ cauchy(0, 1);\n            V_m ~ cauchy(0, 1);\n            sigma ~ cauchy(0, 1);\n            \n            C_hat ~ lognormal(@broadcasted(nothrow_log(C[:, 1])), sigma);\n        end\n    end\nend\n\n\njulia_implementation(::Val{:bym2_offset_only}; N, N_edges, node1, node2, y, E, scaling_factor, kwargs...) = begin \n        log_E = @. log(E)\n@stan begin \n            @parameters begin\n                beta0::real;\n                sigma::real(lower=0);\n                rho::real(lower=0, upper=1);\n                theta::vector[N];\n                phi::vector[N];\n            end\n            convolved_re = @broadcasted(sqrt(1 - rho) * theta + sqrt(rho / scaling_factor) * phi)\n            @model @views begin\n                y ~ poisson_log(@broadcasted(log_E + beta0 + convolved_re * sigma));\n                \n                target += -0.5 * dot_self(phi[node1] - phi[node2]);\n                \n                beta0 ~ normal(0, 1);\n                theta ~ normal(0, 1);\n                sigma ~ normal(0, 1);\n                rho ~ beta(0.5, 0.5);\n                sum(phi) ~ normal(0, 0.001 * N);\n            end\n        end\nend\njulia_implementation(::Val{:bones_model}; nChild, nInd, gamma, delta, ncat, grade, kwargs...) = begin \n        # error(ncat)\n    @stan begin \n        @parameters begin\n            theta::real[nChild]\n        end\n        @model @views begin\n            theta ~ normal(0.0, 36.);\n            p = zeros((nChild, nInd, 5))\n            Q = zeros((nChild, nInd, 4))\n            for i in 1:nChild\n                for j in 1:nInd\n                    for k in 1:(ncat[j]-1)\n                        Q[i,j,k] = inv_logit(delta[j] * (theta[i] - gamma[j, k]))\n                    end\n                    p[i,j,1] = 1 - Q[i,j,1]\n                    for k in 2:(ncat[j]-1)\n                        p[i, j, k] = Q[i, j, k - 1] - Q[i, j, k];\n                    end\n                    p[i, j, ncat[j]] = Q[i, j, ncat[j] - 1];\n                    if grade[i, j] != -1\n                        target += log(p[i, j, grade[i, j]]);\n                    end\n                end\n            end\n        end\n    end\nend\njulia_implementation(::Val{:logistic_regression_rhs}; n, d, y, x, scale_icept,\n    scale_global,\n    nu_global,\n    nu_local, \n    slab_scale,\n    slab_df, kwargs...) = begin \n        x = Matrix{Float64}(x)\n@stan begin \n            @parameters begin\n                beta0::real;\n                z::vector[d];\n                tau::real(lower=0.);\n                lambda::vector(lower=0.)[d];\n                caux::real(lower=0.);\n            end\n            c = slab_scale * sqrt(caux);\n            lambda_tilde = @broadcasted sqrt(c ^ 2 * square(lambda) / (c ^ 2 + tau ^ 2 * square(lambda)));\n            beta = @. z * lambda_tilde * tau;\n            @model begin\n                z ~ std_normal();\n                lambda ~ student_t(nu_local, 0., 1.);\n                tau ~ student_t(nu_global, 0, 2. * scale_global);\n                caux ~ inv_gamma(0.5 * slab_df, 0.5 * slab_df);\n                beta0 ~ normal(0., scale_icept);\n                \n                y ~ bernoulli_logit_glm(x, beta0, beta);\n            end\n        end\nend\n\njulia_implementation(::Val{:hmm_example}; N, K, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta1::simplex[K]\n            theta2::simplex[K]\n            mu::positive_ordered[K]\n        end\n        theta = hcat(theta1, theta2)'\n        @model @views begin\n            target += normal_lpdf(mu[1], 3, 1);\n            target += normal_lpdf(mu[2], 10, 1);\n            acc = zeros(K)\n            gamma = zeros((K,N))'\n            gamma[1, :] .= @. normal_lpdf(y[1], mu, 1) \n            for t in 2:N\n                for k in 1:K\n                    for j in 1:K\n                        acc[j] =  gamma[t - 1, j] + log(theta[j, k]) + normal_lpdf(y[t], mu[k], 1);\n                    end\n                    gamma[t, k] = log_sum_exp(acc);\n                end\n            end\n            target += log_sum_exp(gamma[N, :])\n        end\n    end\nend\njulia_implementation(::Val{:Mb_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0.,upper=1.)\n                p::real(lower=0.,upper=1.)\n                c::real(lower=0.,upper=1.)\n            end\n            @model @views begin\n                p_eff = hcat(\n                    fill(p, M), \n                    @. (1 - y[:, 1:end-1]) * p + y[:, 1:end-1] * c\n                )\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + bernoulli_lpmf(y[i, :], p_eff[i, :])\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + bernoulli_lpmf(y[i, :], p_eff[i, :]),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n        end\nend\njulia_implementation(::Val{:Mh_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, ) \n        C = 0\n        for i in 1:M\n            if y[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0,upper=1)\n                mean_p::real(lower=0,upper=1)\n                sigma::real(lower=0,upper=5)\n                eps_raw::vector[M]\n            end\n            eps = @. logit(mean_p) + sigma * eps_raw\n            @model begin\n                eps_raw ~ normal(0, 1)\n                for i in 1:M\n                    if y[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + binomial_logit_lpmf(y[i], T, eps[i])\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + binomial_logit_lpmf(0, T, eps[i]),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n        end\nend\njulia_implementation(::Val{:Mth_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n        @stan begin \n            @parameters begin\n                omega::real(lower=0.,upper=1.)\n                mean_p::vector(lower=0.,upper=1.)[T]\n                sigma::real(lower=0., upper=5.)\n                eps_raw::vector[M]\n            end\n            @model @views begin\n                logit_p = @. logit(mean_p)' .+ sigma * eps_raw\n                eps_raw ~ normal(0., 1.)\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + bernoulli_logit_lpmf(y[i, :], logit_p[i, :])\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + bernoulli_logit_lpmf(y[i, :], logit_p[i, :]),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n    end\nend\njulia_implementation(::Val{Symbol(\"2pl_latent_reg_irt\")}; I, J, N, ii, jj, y, K, W, kwargs...) = begin \n    obtain_adjustments(W) = begin \n        local M, K = size(W)\n        adj = zeros((2, K))\n        adj[1,1] = 0\n        adj[2, 1] = 1\n        for k in 2:K\n            min_w = minimum(W[:,k])\n            max_w = maximum(W[:,k])\n            minmax_count = sum(w-&gt;w in (min_w, max_w), W[:, k])\n            if minmax_count == M\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = max_w - min_w;\n            else\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = sd(W[:, k]) * 2;\n            end\n        end\n        return adj\n    end\n    adj = obtain_adjustments(W);\n    W_adj = @. ((W - adj[1:1,:])/adj[2:2,:])\n    @stan begin \n        @parameters begin\n            alpha::vector(lower=0)[I];\n            beta_free::vector[I - 1] ;\n            theta::vector[J];\n            lambda_adj::vector[K];\n        end\n        beta = vcat(beta_free, -sum(beta_free))\n        @model @views begin\n            alpha ~ lognormal(1, 1);\n            target += normal_lpdf(beta, 0, 3);\n            lambda_adj ~ student_t(3, 0, 1);\n            theta ~ normal(W_adj * lambda_adj, 1);\n            y ~ bernoulli_logit(@broadcasted(alpha[ii] * theta[jj] - beta[ii]));\n        end\n    end\nend\njulia_implementation(::Val{:Mtbh_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0,upper=1)\n                mean_p::vector(lower=0,upper=1)[T]\n                gamma::real\n                sigma::real(lower=0, upper=3)\n                eps_raw::vector[M]\n            end\n            @model @views begin\n                eps = @. sigma * eps_raw\n                alpha = @. logit(mean_p)\n                logit_p = hcat(\n                    @.(alpha[1] + eps),\n                    @.(alpha[2:end]' + eps + gamma * y[:, 1:end-1])\n                )\n                gamma ~ normal(0, 10)\n                eps_raw ~ normal(0, 1)\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + bernoulli_logit_lpmf(y[i, :], logit_p[i, :])\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + bernoulli_logit_lpmf(y[i, :], logit_p[i, :]),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n    end\nend\njulia_implementation(::Val{:multi_occupancy}; J, K, n, X, S, kwargs...) = begin \n    cov_matrix_2d(sigma, rho) = begin\n        rv12 = sigma[1] * sigma[2] * rho\n        [\n            square(sigma[1]) rv12\n            rv12 square(sigma[2])\n        ]\n    end\n    lp_observed(X, K, logit_psi, logit_theta) = log_inv_logit(logit_psi) + binomial_logit_lpmf(X, K, logit_theta);\n    lp_unobserved(K, logit_psi, logit_theta) = log_sum_exp(\n        lp_observed(0, K, logit_psi, logit_theta),\n        log1m_inv_logit(logit_psi)\n    );\n    lp_never_observed(J, K, logit_psi, logit_theta, Omega) = begin\n        lp_unavailable = bernoulli_lpmf(0, Omega);\n        lp_available = bernoulli_lpmf(1, Omega) + J * lp_unobserved(K, logit_psi, logit_theta);\n        return log_sum_exp(lp_unavailable, lp_available);\n    end\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::real\n                Omega::real(lower=0, upper=+1)\n                rho_uv::real(lower=-1, upper=+1)\n                sigma_uv::vector(lower=0,upper=+Inf)[2]\n                uv1::vector[S]\n                uv2::vector[S]\n            end\n            @transformed_parameters begin \n                uv = hcat(uv1, uv2)\n                logit_psi = @. uv1 + alpha\n                logit_theta = @. uv2 + beta\n            end\n            @model begin\n                alpha ~ cauchy(0, 2.5);\n                beta ~ cauchy(0, 2.5);\n                sigma_uv ~ cauchy(0, 2.5);\n                ((rho_uv + 1) / 2) ~ beta(2, 2);\n                target += multi_normal_lpdf(uv, rep_vector(0., 2), cov_matrix_2d(sigma_uv, rho_uv));\n                Omega ~ beta(2, 2);\n  \n                for i in 1:n \n                    1 ~ bernoulli(Omega); \n                    for j in 1:J\n                        if X[i, j] &gt; 0\n                            target += lp_observed(X[i, j], K, logit_psi[i], logit_theta[i]);\n                        else\n                            target += lp_unobserved(K, logit_psi[i], logit_theta[i]);\n                        end\n                    end\n                end\n                for i in (n + 1):S\n                  target += lp_never_observed(J, K, logit_psi[i], logit_theta[i], Omega);\n                end\n            end\n        end\nend\njulia_implementation(::Val{:losscurve_sislob};\ngrowthmodel_id, \nn_data,\nn_time,\nn_cohort,\ncohort_id,\nt_idx,\ncohort_maxtime,\nt_value,\npremium,\nloss,\nkwargs...) = begin \n    growth_factor_weibull(t, omega, theta) = begin\n        return 1 - exp(-(t / theta) ^ omega);\n    end\n\n    growth_factor_loglogistic(t, omega, theta) = begin\n        pow_t_omega = t ^ omega;\n        return pow_t_omega / (pow_t_omega + theta ^ omega);\n    end\n    @stan begin \n            @parameters begin\n                omega::real(lower=0);\n                theta::real(lower=0);\n                \n                LR::vector(lower=0)[n_cohort];\n                \n                mu_LR::real;\n                sd_LR::real(lower=0);\n                \n                loss_sd::real(lower=0);\n            end\n            gf = if growthmodel_id == 1\n                @. growth_factor_weibull(t_value, omega, theta)\n            else\n                @. growth_factor_loglogistic(t_value, omega, theta)\n            end\n            @model @views begin\n                mu_LR ~ normal(0, 0.5);\n                sd_LR ~ lognormal(0, 0.5);\n                \n                LR ~ lognormal(mu_LR, sd_LR);\n                \n                loss_sd ~ lognormal(0, 0.7);\n                \n                omega ~ lognormal(0, 0.5);\n                theta ~ lognormal(0, 0.5);\n                \n                loss ~ normal(@broadcasted(LR[cohort_id] * premium[cohort_id] * gf[t_idx]), (loss_sd * premium)[cohort_id]);\n            end\n    end\nend\n\njulia_implementation(::Val{:accel_splines}; N,Y,Ks,Xs,knots_1,Zs_1_1, Ks_sigma, Xs_sigma,knots_sigma_1,Zs_sigma_1_1,prior_only, kwargs...) = begin \n@stan begin \n            @parameters begin\n                Intercept::real;\n                bs::vector[Ks];\n                zs_1_1::vector[knots_1];\n                sds_1_1::real(lower=0);\n                Intercept_sigma::real;\n                bs_sigma::vector[Ks_sigma];\n                zs_sigma_1_1::vector[knots_sigma_1];\n                sds_sigma_1_1::real(lower=0);\n            end\n            s_1_1 = @. sds_1_1 * zs_1_1\n            s_sigma_1_1 = @. sds_sigma_1_1 * zs_sigma_1_1;\n            @model begin\n                mu = Intercept .+ Xs * bs + Zs_1_1 * s_1_1;\n                sigma = exp.(Intercept_sigma .+ Xs_sigma * bs_sigma\n                                  + Zs_sigma_1_1 * s_sigma_1_1);\n                target += student_t_lpdf(Intercept, 3, -13, 36);\n                target += normal_lpdf(zs_1_1, 0, 1);\n                target += student_t_lpdf(sds_1_1, 3, 0, 36)\n                        #   - 1 * student_t_lccdf(0, 3, 0, 36);\n                target += student_t_lpdf(Intercept_sigma, 3, 0, 10);\n                target += normal_lpdf(zs_sigma_1_1, 0, 1);\n                target += student_t_lpdf(sds_sigma_1_1, 3, 0, 36)\n                        #   - 1 * student_t_lccdf(0, 3, 0, 36);\n                if !(prior_only == 1)\n                    target += normal_lpdf(Y, mu, sigma);\n                end\n            end\n        end\nend\njulia_implementation(::Val{Symbol(\"grsm_latent_reg_irt\")}; I, J, N, ii, jj, y, K, W, kwargs...) = begin \n    rsm(y, theta, beta, kappa) = begin\n      unsummed = vcat(0, theta .- beta .- kappa);\n      probs = softmax(cumulative_sum(unsummed));\n      return categorical_lpmf(y + 1, probs);\n    end\n    obtain_adjustments(W) = begin \n        local M, K = size(W)\n        adj = zeros((2, K))\n        adj[1,1] = 0\n        adj[2, 1] = 1\n        for k in 2:K\n            min_w = minimum(W[:,k])\n            max_w = maximum(W[:,k])\n            minmax_count = sum(w-&gt;w in (min_w, max_w), W[:, k])\n            if minmax_count == M\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = max_w - min_w;\n            else\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = sd(W[:, k]) * 2;\n            end\n        end\n        return adj\n    end\n        m = maximum(y)\n        adj = obtain_adjustments(W);\n        W_adj = @. ((W - adj[1:1,:])/adj[2:2,:])\n@stan begin \n            @parameters begin\n                alpha::vector(lower=0)[I];\n                beta_free::vector[I - 1] ;\n                kappa_free::vector[m - 1] ;\n                theta::vector[J];\n                lambda_adj::vector[K];\n            end\n            beta = vcat(beta_free, -sum(beta_free))\n            kappa = vcat(kappa_free, -sum(kappa_free))\n            @model @views begin\n                alpha ~ lognormal(1, 1);\n                target += normal_lpdf(beta, 0, 3);\n                target += normal_lpdf(kappa, 0, 3);\n                theta ~ normal(W_adj * lambda_adj, 1);\n                lambda_adj ~ student_t(3, 0, 1);\n                for n in 1:N\n                    target += rsm(y[n], theta[jj[n]] .* alpha[ii[n]], beta[ii[n]], kappa)\n                end\n            end\n    end\nend\njulia_implementation(::Val{:prophet};\n    T,\n    K,\n    t,\n    cap,\n    y,\n    S,\n    t_change,\n    X,\n    sigmas,\n    tau,\n    trend_indicator,\n    s_a,\n    s_m, \n    kwargs...) = begin \n    get_changepoint_matrix(t, t_change, T, S) = begin\n        local A = rep_matrix(0, T, S);\n        a_row = rep_row_vector(0, S);\n        cp_idx = 1;\n        \n        for i in 1:T\n          while ((cp_idx &lt;= S) && (t[i] &gt;= t_change[cp_idx])) \n            a_row[cp_idx] = 1;\n            cp_idx = cp_idx + 1;\n          end\n          A[i,:] = a_row;\n        end\n        return A;\n      end\n      \n      \n      logistic_gamma(k, m, delta, t_change, S) = begin\n        local gamma = zeros(S)\n        k_s = append_row(k, k + cumulative_sum(delta));\n        \n        m_pr = m; \n        for i in 1:S\n          gamma[i] = (t_change[i] - m_pr) * (1 - k_s[i] / k_s[i + 1]);\n          m_pr = m_pr + gamma[i]; \n        end\n        return gamma;\n      end\n      \n      logistic_trend(k, m, delta, t, cap, A, t_change, S) = begin\n        local gamma = logistic_gamma(k, m, delta, t_change, S);\n        return cap .* inv_logit.((k .+ A * delta) .* (t .- m .- A * gamma));\n      end\n      \n      linear_trend(k, m, delta, t, A, t_change) = begin\n        return (k .+ A * delta) .* t .+ (m .+ A * (-t_change .* delta));\n      end\n        A = get_changepoint_matrix(t, t_change, T, S)\n@stan begin \n            @parameters begin\n                k::real\n                m::real\n                delta::vector[S]\n                sigma_obs::real(lower=0)\n                beta::vector[K]\n            end\n            @model begin\n                k ~ normal(0, 5);\n                m ~ normal(0, 5);\n                delta ~ double_exponential(0, tau);\n                sigma_obs ~ normal(0, 0.5);\n                beta ~ normal(0, sigmas);\n                \n                if trend_indicator == 0\n                    y ~ normal(linear_trend(k, m, delta, t, A, t_change)\n                             .* (1 .+ X * (beta .* s_m)) + X * (beta .* s_a), sigma_obs);\n                elseif trend_indicator == 1\n                    y ~ normal(logistic_trend(k, m, delta, t, cap, A, t_change, S)\n                             .* (1 .+ X * (beta .* s_m)) + X * (beta .* s_a), sigma_obs);\n                end\n            end\n        end\nend\n\njulia_implementation(::Val{:hmm_gaussian}; T, K, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            pi1::simplex[K]\n            A::simplex[K,K]\n            mu::ordered[K]\n            sigma::vector(lower=0)[K]\n        end\n        @model @views begin\n            logalpha = zeros((K,T))'\n            accumulator = zeros(K)\n            logalpha[1,:] .= log.(pi1) .+ normal_lpdf(y[1], mu, sigma);\n            for t in 2 : T\n                for j in 1:K\n                    for i in 1:K\n                        accumulator[i] = logalpha[t - 1, i] + log(A[i, j]) + normal_lpdf(y[t], mu[j], sigma[j]);\n                    end\n                logalpha[t, j] = log_sum_exp(accumulator);\n                end\n            end\n            target += log_sum_exp(logalpha[T,:]);\n        end\n    end\nend\n\njulia_implementation(::Val{:hmm_drive_0}; K, N, u, v, alpha, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta1::simplex[K]\n            theta2::simplex[K]\n            phi::positive_ordered[K]\n            lambda::positive_ordered[K]\n        end\n        theta = hcat(theta1, theta2)'\n        @model @views begin\n            for k in 1:K\n              target += dirichlet_lpdf(theta[k, :], alpha[k, :]);\n            end\n            target += normal_lpdf(phi[1], 0, 1);\n            target += normal_lpdf(phi[2], 3, 1);\n            target += normal_lpdf(lambda[1], 0, 1);\n            target += normal_lpdf(lambda[2], 3, 1);\n\n            acc = zeros(K)\n            gamma = zeros((K,N))'\n            gamma[1,:] .= @.(exponential_lpdf(u[1], phi) + exponential_lpdf(v[1], lambda))\n            for t in 2:N\n                for k in 1:K\n                    for j in 1:K\n                        acc[j] = gamma[t - 1, j] + log(theta[j, k]) + exponential_lpdf(u[t], phi[k]) + exponential_lpdf(v[t], lambda[k]);\n                    end\n                    gamma[t, k] = log_sum_exp(acc);\n                end\n            end\n            target += log_sum_exp(gamma[N, :])\n        end\n    end\nend\n\njulia_implementation(::Val{:hmm_drive_1}; K, N, u, v, alpha, tau, rho, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta1::simplex[K]\n            theta2::simplex[K]\n            phi::ordered[K]\n            lambda::ordered[K]\n        end\n        theta = hcat(theta1, theta2)'\n        @model @views begin\n            for k in 1:K\n              target += dirichlet_lpdf(theta[k, :], alpha[k, :]);\n            end\n            target += normal_lpdf(phi[1], 0, 1);\n            target += normal_lpdf(phi[2], 3, 1);\n            target += normal_lpdf(lambda[1], 0, 1);\n            target += normal_lpdf(lambda[2], 3, 1);\n\n            acc = zeros(K)\n            gamma = zeros((K,N))'\n            gamma[1,:] .= @.(normal_lpdf(u[1], phi, tau) + normal_lpdf(v[1], lambda, rho))\n            for t in 2:N\n                for k in 1:K\n                    for j in 1:K\n                        acc[j] = gamma[t - 1, j] + log(theta[j, k]) + normal_lpdf(u[t], phi[k], tau) + normal_lpdf(v[t], lambda[k], rho);\n                    end\n                    gamma[t, k] = log_sum_exp(acc);\n                end\n            end\n            target += log_sum_exp(gamma[N, :])\n        end\n    end\nend\n\njulia_implementation(::Val{:iohmm_reg}; T, K, M, y, u, kwargs...) = begin \n    @inline normalize(x) = x ./ sum(x)\n    @stan begin \n        @parameters begin\n            pi1::simplex[K]\n            w::vector[K,M]\n            b::vector[K,M]\n            sigma::vector(lower=0)[K]\n        end\n        @model @views begin\n            unA = hcat(pi1, w * u'[:, 2:end])'\n            A = copy(unA)\n            for t in 2:T\n                A[t, :] .= softmax(unA[t, :])\n            end\n            logA = log.(A)\n            logoblik = normal_lpdf.(y, u * b', sigma')\n            accumulator = zeros(K)\n            logalpha = zeros((K,T))'\n            logalpha[1,:] .= @.(log(pi1) + logoblik[1,:])\n            for t in 2:T\n                for j in 1:K\n                    for i in 1:K\n                        accumulator[i] = logalpha[t - 1, i] + logA[t,i] + logoblik[t,j];\n                    end\n                    logalpha[t, j] = log_sum_exp(accumulator);\n                end\n            end\n            w ~ normal(0, 5);\n            b ~ normal(0, 5);\n            sigma ~ normal(0, 3);\n            target += log_sum_exp(logalpha[T,:]);\n        end\n    end\nend\n\njulia_implementation(::Val{:accel_gp}; N, Y, Kgp_1, Dgp_1, NBgp_1, Xgp_1, slambda_1, Kgp_sigma_1, Dgp_sigma_1, NBgp_sigma_1, Xgp_sigma_1, slambda_sigma_1, prior_only, kwargs...) = begin \n    @inline sqrt_spd_cov_exp_quad(slambda, sdgp, lscale) = begin \n        NB, D = size(slambda)\n        Dls = length(lscale)\n        if Dls == 1\n            constant = (sdgp) * (sqrt(2 * pi) * lscale[1]) ^ (D/2)\n            neg_half_lscale2 = -0.25 * square(lscale[1])\n            @.(constant * exp(neg_half_lscale2 * dot_self($eachrow(slambda))))\n        else\n            error()\n        end\n    end\n    @inline gpa(X, sdgp, lscale, zgp, slambda) = X * (sqrt_spd_cov_exp_quad(slambda, sdgp, lscale) .* zgp)\n    @stan begin \n        @parameters begin\n            Intercept::real\n            sdgp_1::real(lower=0)\n            lscale_1::real(lower=0)\n            zgp_1::vector[NBgp_1]\n            Intercept_sigma::real\n            sdgp_sigma_1::real(lower=0)\n            lscale_sigma_1::real(lower=0)\n            zgp_sigma_1::vector[NBgp_sigma_1]\n        end\n        vsdgp_1 = fill(sdgp_1, 1)\n        vlscale_1 = fill(lscale_1, (1, 1))\n        vsdgp_sigma_1 = fill(sdgp_sigma_1, 1)\n        vlscale_sigma_1 = fill(lscale_sigma_1, (1, 1))\n        @model @views begin\n            mu = Intercept .+ gpa(Xgp_1, vsdgp_1[1], vlscale_1[1,:], zgp_1, slambda_1);\n            sigma = exp.(Intercept_sigma .+ gpa(Xgp_sigma_1, vsdgp_sigma_1[1], vlscale_sigma_1[1,:], zgp_sigma_1, slambda_sigma_1));\n            target += student_t_lpdf(Intercept, 3, -13, 36);\n            target += student_t_lpdf(vsdgp_1, 3, 0, 36)\n                    #   - 1 * student_t_lccdf(0, 3, 0, 36);\n            target += normal_lpdf(zgp_1, 0, 1);\n            target += inv_gamma_lpdf(vlscale_1[1,:], 1.124909, 0.0177);\n            target += student_t_lpdf(Intercept_sigma, 3, 0, 10);\n            target += student_t_lpdf(vsdgp_sigma_1, 3, 0, 36)\n                    #   - 1 * student_t_lccdf(0, 3, 0, 36);\n            target += normal_lpdf(zgp_sigma_1, 0, 1);\n            target += inv_gamma_lpdf(vlscale_sigma_1[1,:], 1.124909, 0.0177);\n            if prior_only == 0\n              target += normal_lpdf(Y, mu, sigma);\n            end\n        end\n    end\nend\n\n\n\njulia_implementation(::Val{:hierarchical_gp};\n        N,\n        N_states,\n        N_regions,\n        N_years_obs,\n        N_years,\n        state_region_ind,\n        state_ind,\n        region_ind,\n        year_ind,\n        y,\n        kwargs...) = begin \n    @stan begin \n        years = 1:N_years\n        counts = fill(2, 17)\n        @parameters begin\n            GP_region_std::matrix[N_years, N_regions]\n            GP_state_std::matrix[N_years, N_states]\n            year_std::vector[N_years_obs]\n            state_std::vector[N_states]\n            region_std::vector[N_regions]\n            tot_var::real(lower=0)\n            prop_var::simplex[17]\n            mu::real\n            length_GP_region_long::real(lower=0)\n            length_GP_state_long::real(lower=0)\n            length_GP_region_short::real(lower=0)\n            length_GP_state_short::real(lower=0)\n        end\n\n\n        vars = 17 * prop_var * tot_var;\n        sigma_year = sqrt(vars[1]);\n        sigma_region = sqrt(vars[2]);\n        sigma_state = @.(sqrt(vars[3:end]))\n        \n        sigma_GP_region_long = sqrt(vars[13]);\n        sigma_GP_state_long = sqrt(vars[14]);\n        sigma_GP_region_short = sqrt(vars[15]);\n        sigma_GP_state_short = sqrt(vars[16]);\n        sigma_error_state_2 = sqrt(vars[17]);\n        \n        region_re = sigma_region * region_std;\n        year_re = sigma_year * year_std;\n        state_re = sigma_state[state_region_ind] .* state_std;\n        \n        begin\n            cov_region = gp_exp_quad_cov(years, sigma_GP_region_long,\n                                        length_GP_region_long)\n                        + gp_exp_quad_cov(years, sigma_GP_region_short,\n                                        length_GP_region_short);\n            cov_state = gp_exp_quad_cov(years, sigma_GP_state_long,\n                                        length_GP_state_long)\n                        + gp_exp_quad_cov(years, sigma_GP_state_short,\n                                        length_GP_state_short);\n            for year in 1 : N_years\n                cov_region[year, year] = cov_region[year, year] + 1e-6;\n                cov_state[year, year] = cov_state[year, year] + 1e-6;\n            end\n            \n            L_cov_region = cholesky_decompose(cov_region);\n            L_cov_state = cholesky_decompose(cov_state);\n            GP_region = L_cov_region * GP_region_std;\n            GP_state = L_cov_state * GP_state_std;\n        end\n        @model begin\n            obs_mu = zeros(N)\n            for n in 1 : N\n                obs_mu[n] = mu + year_re[year_ind[n]] + state_re[state_ind[n]]\n                            + region_re[region_ind[n]]\n                            + GP_region[year_ind[n], region_ind[n]]\n                            + GP_state[year_ind[n], state_ind[n]];\n                end\n                y ~ normal(obs_mu, sigma_error_state_2); \n                \n                (GP_region_std) ~ normal(0, 1);\n                (GP_state_std) ~ normal(0, 1);\n                year_std ~ normal(0, 1);\n                state_std ~ normal(0, 1);\n                region_std ~ normal(0, 1);\n                mu ~ normal(.5, .5);\n                tot_var ~ gamma(3, 3);\n                prop_var ~ dirichlet(counts);\n                length_GP_region_long ~ weibull(30, 8);\n                length_GP_state_long ~ weibull(30, 8);\n                length_GP_region_short ~ weibull(30, 3);\n                length_GP_state_short ~ weibull(30, 3);\n        end\n    end\nend\njulia_implementation(::Val{:kronecker_gp}; n1, n2, x1, y, kwargs...) = begin \n    kron_mvprod(A, B, V) = adjoint(A * adjoint(B * V))\n    calculate_eigenvalues(A, B, sigma2) = A .* B' .+ sigma2\n    xd  = -(x1 .- x1') .^ 2\n    return @stan begin end\n    @stan begin \n        @parameters begin\n            var1::real(lower=0)\n            bw1::real(lower=0)\n            L::cholesky_factor_corr[n2]\n            sigma1::real(lower=.00001)\n        end\n        Lambda = multiply_lower_tri_self_transpose(L)\n        Sigma1 = Symmetric(var1 .* exp.(xd .* bw1) + .00001 * I);\n        R1, Q1 = eigen(Sigma1);\n        R2, Q2 = eigen(Lambda);\n        eigenvalues = calculate_eigenvalues(R2, R1, sigma1);\n        @model @views begin\n            var1 ~ lognormal(0, 1);\n            bw1 ~ cauchy(0, 2.5);\n            sigma1 ~ lognormal(0, 1);\n            L ~ lkj_corr_cholesky(2);\n            target += -0.5 * sum(y .* kron_mvprod(Q1, Q2, kron_mvprod(transpose(Q1), transpose(Q2), y) ./ eigenvalues)) - 0.5 * sum(log(eigenvalues))\n        end\n    end\nend\n\n\njulia_implementation(::Val{:covid19imperial_v2}; M, P, N0, N, N2, cases, deaths, f, X, EpidemicStart, pop, SI, kwargs...) = begin \n    SI_rev = reverse(SI)\n    f_rev = mapreduce(reverse, hcat, eachcol(f))'\n    @stan begin \n        @parameters begin\n            mu::real(lower=0)[M]\n            alpha_hier::real(lower=0)[P]\n            kappa::real(lower=0)\n            y::real(lower=0)[M]\n            phi::real(lower=0)\n            tau::real(lower=0)\n            ifr_noise::real(lower=0)[M]\n        end\n        @model @views begin\n            prediction = rep_matrix(0., N2, M);\n            E_deaths = rep_matrix(0., N2, M);\n            Rt = rep_matrix(0., N2, M);\n            Rt_adj = copy(Rt);\n            cumm_sum = rep_matrix(0., N2, M);\n            alpha = alpha_hier .- (log(1.05) / 6.)\n            for m in 1:M\n                prediction[1:N0, m] .= y[m]\n                cumm_sum[2:N0, m] .= cumulative_sum(prediction[2:N0, m]);\n                Rt[:, m] .= mu[m] * exp.(-X[m,:,:] * alpha);\n                Rt_adj[1:N0, m] .= Rt[1:N0, m];\n                for i in (N0+1):N2\n                    convolution = dot_product(sub_col(prediction, 1, m, i - 1), stan_tail(SI_rev, i - 1))\n                    cumm_sum[i, m] = cumm_sum[i - 1, m] + prediction[i - 1, m];\n                    Rt_adj[i, m] = ((pop[m] - cumm_sum[i, m]) / pop[m]) * Rt[i, m];\n                    prediction[i, m] = Rt_adj[i, m] * convolution;\n                end\n                E_deaths[1, m] = 1e-15 * prediction[1, m];\n                for i in 2:N2\n                    E_deaths[i, m] = ifr_noise[m] * dot_product(sub_col(prediction, 1, m, i - 1), stan_tail(f_rev[m, :], i - 1));\n                end\n            end\n            tau ~ exponential(0.03);\n            y ~ exponential(1 / tau)\n            phi ~ normal(0, 5);\n            kappa ~ normal(0, 0.5);\n            mu ~ normal(3.28, kappa);\n            alpha_hier ~ gamma(.1667, 1);\n            ifr_noise ~ normal(1, 0.1);\n            for m in 1:M\n                deaths[EpidemicStart[m] : N[m], m] ~ neg_binomial_2(E_deaths[EpidemicStart[m] : N[m], m], phi);\n            end\n        end\n    end\nend\n\n\njulia_implementation(::Val{:covid19imperial_v3}; kwargs...) = julia_implementation(Val{:covid19imperial_v2}(); kwargs...)\n\njulia_implementation(::Val{:gpcm_latent_reg_irt}; I, J, N, ii, jj, y, K, W, kwargs...) = begin \n    pcm(y, theta, beta) = begin \n        unsummed = append_row(rep_vector(0.0, 1), theta .- beta)\n        probs = softmax(cumulative_sum(unsummed))\n        categorical_lpmf(y + 1, probs)\n    end\n    obtain_adjustments(W) = begin \n        local M, K = size(W)\n        adj = zeros((2, K))\n        adj[1,1] = 0\n        adj[2, 1] = 1\n        for k in 2:K\n            min_w = minimum(W[:,k])\n            max_w = maximum(W[:,k])\n            minmax_count = sum(w-&gt;w in (min_w, max_w), W[:, k])\n            if minmax_count == M\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = max_w - min_w;\n            else\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = sd(W[:, k]) * 2;\n            end\n        end\n        return adj\n    end\n    m = fill(0, I)\n    pos = fill(1, I)\n    for n in 1:N\n        if y[n] &gt; m[ii[n]]\n            m[ii[n]] = y[n]\n        end\n    end\n    for i in 2:I\n        pos[i] = m[i - 1] + pos[i - 1]\n    end\n    adj = obtain_adjustments(W);\n    W_adj = @. ((W - adj[1:1,:])/adj[2:2,:])\n    @stan begin \n        @parameters begin\n            alpha::real(lower=0)[I]\n            beta_free::vector[sum(m) - 1]\n            theta::vector[J]\n            lambda_adj::vector[K]\n        end\n        beta = vcat(beta_free, -sum(beta_free))\n        @model @views begin\n            alpha ~ lognormal(1, 1);\n            target += normal_lpdf(beta, 0, 3);\n            theta ~ normal(W_adj * lambda_adj, 1);\n            lambda_adj ~ student_t(3, 0, 1);\n            for n in 1:N\n                target += pcm(y[n], theta[jj[n]] .* alpha[ii[n]], segment(beta, pos[ii[n]], m[ii[n]]));\n            end\n        end\n    end\nend\nend"
  },
  {
    "objectID": "slic/golf.html",
    "href": "slic/golf.html",
    "title": "Golf models reimplementation",
    "section": "",
    "text": "TO DO: elaborate"
  },
  {
    "objectID": "slic/golf.html#stanblocks.jl-implementation",
    "href": "slic/golf.html#stanblocks.jl-implementation",
    "title": "Golf models reimplementation",
    "section": "StanBlocks.jl implementation",
    "text": "StanBlocks.jl implementation\nTO DO: elaborate"
  },
  {
    "objectID": "slic/golf.html#generated-stan-models",
    "href": "slic/golf.html#generated-stan-models",
    "title": "Golf models reimplementation",
    "section": "Generated Stan models",
    "text": "Generated Stan models\n\n\n\ngolf_logisticgolf_anglegolf_angle_distance_2golf_angle_distance_3_with_residsgolf_angle_distance_4\n\n\nfunctions {\nvector binomial_logit_lpmfs(\n    array[] int y,\n    array[] int args1,\n    vector args2\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_binomial_logit_lpmfs(y, args1, args2);\n}\nvector jbroadcasted_binomial_logit_lpmfs(\n    array[] int x1,\n    array[] int x2,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = binomial_logit_lpmfs(\n            broadcasted_getindex(x1, i),\n            broadcasted_getindex(x2, i),\n            broadcasted_getindex(x3, i)\n        );\n    }\n    return rv;\n}\nreal binomial_logit_lpmfs(\n    int args1,\n    int args2,\n    real args3\n) {\n    return binomial_logit_lpmf(args1 | args2, args3);\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n}\ndata {\n    int y_n;\n    array[y_n] int y;\n    int n_n;\n    array[n_n] int n;\n    int x_n;\n    vector[x_n] x;\n}\ntransformed data {\n}\nparameters {\n    real a;\n    real b;\n}\ntransformed parameters {\n}\nmodel {\n    y ~ binomial_logit(n, (a + (b * x)));\n}\ngenerated quantities {\n    vector[y_n] y_likelihood = binomial_logit_lpmfs(y, n, (a + (b * x)));\n    array[x_n] int y_gen = binomial_logit_rng(n, (a + (b * x)));\n}\n\n\nfunctions {\nvector binomial_lpmfs(\n    array[] int y,\n    array[] int args1,\n    vector args2\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_binomial_lpmfs(y, args1, args2);\n}\nvector jbroadcasted_binomial_lpmfs(\n    array[] int x1,\n    array[] int x2,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = binomial_lpmfs(\n            broadcasted_getindex(x1, i),\n            broadcasted_getindex(x2, i),\n            broadcasted_getindex(x3, i)\n        );\n    }\n    return rv;\n}\nreal binomial_lpmfs(\n    int args1,\n    int args2,\n    real args3\n) {\n    return binomial_lpmf(args1 | args2, args3);\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n}\ndata {\n    int x_n;\n    real R;\n    real r;\n    vector[x_n] x;\n    int y_n;\n    array[y_n] int y;\n    int n_n;\n    array[n_n] int n;\n}\ntransformed data {\n    vector[x_n] threshold_angle = asin(((R - r) ./ x));\n}\nparameters {\n    real&lt;lower=0&gt; sigma;\n}\ntransformed parameters {\n    vector[x_n] p = ((2 * Phi((threshold_angle / sigma))) - 1);\n}\nmodel {\n    y ~ binomial(n, p);\n}\ngenerated quantities {\n    vector[y_n] y_likelihood = binomial_lpmfs(y, n, p);\n    int y_gen = binomial_rng(n, p);\n    real sigma_degrees = ((sigma * 180) / 3.141592653589793);\n}\n\n\nfunctions {\nvector binomial_lpmfs(\n    array[] int y,\n    array[] int args1,\n    vector args2\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_binomial_lpmfs(y, args1, args2);\n}\nvector jbroadcasted_binomial_lpmfs(\n    array[] int x1,\n    array[] int x2,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = binomial_lpmfs(\n            broadcasted_getindex(x1, i),\n            broadcasted_getindex(x2, i),\n            broadcasted_getindex(x3, i)\n        );\n    }\n    return rv;\n}\nreal binomial_lpmfs(\n    int args1,\n    int args2,\n    real args3\n) {\n    return binomial_lpmf(args1 | args2, args3);\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n}\ndata {\n    int x_n;\n    real R;\n    real r;\n    vector[x_n] x;\n    real distance_tolerance;\n    real overshot;\n    int y_n;\n    array[y_n] int y;\n    int n_n;\n    array[n_n] int n;\n}\ntransformed data {\n    vector[x_n] threshold_angle = asin(((R - r) ./ x));\n}\nparameters {\n    vector&lt;lower=0.0&gt;[2] sigma;\n}\ntransformed parameters {\n    real sigma_angle = sigma[1];\n    vector[x_n] p_angle = ((2 * Phi((threshold_angle / sigma_angle))) - 1);\n    real sigma_distance = sigma[2];\n    vector[x_n] p_distance = (\n        Phi(((distance_tolerance - overshot) ./ ((x + overshot) * sigma_distance))) -\n        Phi(((-overshot) ./ ((x + overshot) * sigma_distance)))\n    );\n    vector[x_n] p = (p_angle .* p_distance);\n}\nmodel {\n    sigma ~ normal(0, 1);\n    y ~ binomial(n, p);\n}\ngenerated quantities {\n    vector[y_n] y_likelihood = binomial_lpmfs(y, n, p);\n    int y_gen = binomial_rng(n, p);\n    real sigma_degrees = ((sigma_angle * 180) / 3.141592653589793);\n}\n\n\nfunctions {\nvector normal_lpdfs(\n    array[] int obs,\n    vector loc,\n    vector scale\n) {\n    int n = dims(obs)[1];\n    return jbroadcasted_normal_lpdfs(obs, loc, scale);\n}\nvector jbroadcasted_normal_lpdfs(\n    array[] int x1,\n    vector x2,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = normal_lpdfs(broadcasted_getindex(x1, i), broadcasted_getindex(x2, i), broadcasted_getindex(x3, i));\n    }\n    return rv;\n}\nreal normal_lpdfs(\n    int args1,\n    real args2,\n    real args3\n) {\n    return normal_lpdf(args1 | args2, args3);\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n}\ndata {\n    int x_n;\n    real R;\n    real r;\n    vector[x_n] x;\n    real distance_tolerance;\n    real overshot;\n    int n_n;\n    array[n_n] int n;\n    int y_n;\n    array[y_n] int y;\n}\ntransformed data {\n    vector[x_n] threshold_angle = asin(((R - r) ./ x));\n    vector[n_n] vec_n = to_vector(n);\n}\nparameters {\n    vector&lt;lower=0.0&gt;[2] sigma;\n    real sigma_y;\n}\ntransformed parameters {\n    real sigma_angle = sigma[1];\n    vector[x_n] p_angle = ((2 * Phi((threshold_angle / sigma_angle))) - 1);\n    real sigma_distance = sigma[2];\n    vector[x_n] p_distance = (\n        Phi(((distance_tolerance - overshot) ./ ((x + overshot) * sigma_distance))) -\n        Phi(((-overshot) ./ ((x + overshot) * sigma_distance)))\n    );\n    vector[x_n] p = (p_angle .* p_distance);\n}\nmodel {\n    sigma ~ normal(0, 1);\n    sigma_y ~ normal(0, 1);\n    y ~ normal((vec_n .* p), (vec_n .* sqrt((((p .* (1 - p)) ./ to_vector(n)) + (sigma_y ^ 2)))));\n}\ngenerated quantities {\n    vector[y_n] y_likelihood = normal_lpdfs(y, (vec_n .* p), (vec_n .* sqrt((((p .* (1 - p)) ./ to_vector(n)) + (sigma_y ^ 2)))));\n    array[n_n] real y_gen = normal_rng((vec_n .* p), (vec_n .* sqrt((((p .* (1 - p)) ./ to_vector(n)) + (sigma_y ^ 2)))));\n    real sigma_degrees = ((sigma_angle * 180) / 3.141592653589793);\n}\n\n\nfunctions {\nreal normal_lpdfs(\n    real args1,\n    int args2,\n    int args3\n) {\n    return normal_lpdf(args1 | args2, args3);\n}\nvector binomial_lpmfs(\n    array[] int y,\n    array[] int args1,\n    vector args2\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_binomial_lpmfs(y, args1, args2);\n}\nvector jbroadcasted_binomial_lpmfs(\n    array[] int x1,\n    array[] int x2,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = binomial_lpmfs(\n            broadcasted_getindex(x1, i),\n            broadcasted_getindex(x2, i),\n            broadcasted_getindex(x3, i)\n        );\n    }\n    return rv;\n}\nreal binomial_lpmfs(\n    int args1,\n    int args2,\n    real args3\n) {\n    return binomial_lpmf(args1 | args2, args3);\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n}\ndata {\n    int x_n;\n    real R;\n    real r;\n    vector[x_n] x;\n    real overshot;\n    real distance_tolerance;\n    int y_n;\n    array[y_n] int y;\n    int n_n;\n    array[n_n] int n;\n}\ntransformed data {\n    vector[x_n] threshold_angle = asin(((R - r) ./ x));\n}\nparameters {\n    vector&lt;lower=0.0&gt;[2] sigma;\n}\ntransformed parameters {\n    real sigma_angle = sigma[1];\n    vector[x_n] p_angle = ((2 * Phi((threshold_angle / sigma_angle))) - 1);\n    real sigma_distance = sigma[2];\n    vector[x_n] p_distance = (\n        Phi(((distance_tolerance - overshot) ./ ((x + overshot) * sigma_distance))) -\n        Phi(((-overshot) ./ ((x + overshot) * sigma_distance)))\n    );\n    vector[x_n] p = (p_angle .* p_distance);\n}\nmodel {\n    sigma ~ normal(0, 1);\n    overshot ~ normal(1, 5);\n    distance_tolerance ~ normal(3, 5);\n    y ~ binomial(n, p);\n}\ngenerated quantities {\n    real overshot_likelihood = normal_lpdfs(overshot, 1, 5);\n    real overshot_gen = normal_rng(1, 5);\n    real distance_tolerance_likelihood = normal_lpdfs(distance_tolerance, 3, 5);\n    real distance_tolerance_gen = normal_rng(3, 5);\n    vector[y_n] y_likelihood = binomial_lpmfs(y, n, p);\n    int y_gen = binomial_rng(n, p);\n    real sigma_degrees = ((sigma_angle * 180) / 3.141592653589793);\n}"
  },
  {
    "objectID": "slic/index.html",
    "href": "slic/index.html",
    "title": "The @slic macro",
    "section": "",
    "text": "Code up your model once - simplify, extend and use in multiple contexts efficiently.\n\n\nThe aim is to make it easy to iterate on the statistical model.\n\n\nLike Turing.jls submodels (wheres the documentation for this?) or SlicStans functions that declare parameters (first example in the paper).\nGetting tired of always coding up the same hierarchical priors? @slic will support reusing model components:\nhierarchical_prior = @slic begin\n    location ~ std_normal()\n    scale ~ std_lognormal()\n    x ~ normal(location, scale; n)\n    return x\nend\nhierarchical_model = @slic begin \n    obs_location ~ hierarchical_prior(;n)\n    obs_scale ~ std_lognormal()\n    obs ~ normal(obs_location, obs_scale)\nend\nAlternative syntax proposals are appreciated!\n\n\n\nEver wanted to switch out one prior for another, but didnt want to implement this functionality when coding up your first exploratory model? @slic will support switching out arbitrary model components:\n\"My first, exploratory model to check that things work\"\npooled_model = @slic begin \n    obs_location ~ std_normal()\n    obs_scale ~ std_lognormal()\n    obs ~ normal(obs_location, obs_scale)\nend\n\"Equivalent to the previous implementation of `hierarchical_model` above\"\nhierarchical_model = pooled_model(quote \n    obs_location ~ hierarchical_prior(;n)\nend)\nAlternative syntax proposals are appreciated!\n\n\n\nLike Turing.jls Conditioning or this Stan PR.\nEver wanted to pin hierarchical scale parameters? @slic will support pinning arbitrary model components:\n\"The `hierarchical_model` with the hierarchical scale parameter fixed to 1.\"\nsemihierarchical_model = hierarchical_model(;obs_location_scale=1.)\nTurings Deconditioning could also easily be supported, but as always syntax proposals are appreciated!\n\n\n\nGetting tired of reimplementing parts of your model to perform e.g.leave-one-subject-out cross-validation? @slic will support automatic model rewrites to perform e.g.automatic leave-one-subject-out cross-validation from the same model implementation that you have used to sample from the posterior:\nhierarchical_prior = @slic begin\n    location ~ std_normal()\n    scale ~ std_lognormal()\n    x ~ normal(location, scale; n)\n    return x\nend\nhierarchical_model = @slic begin \n    n = maximum(subject_idx)\n    obs_intercept ~ hierarchical_prior(;n)\n    obs_slope ~ hierarchical_prior(;n)\n    obs_location = obs_intercept + dot(covariates, obs_slope)\n    obs_scale ~ std_lognormal()\n    obs[subject_idx] ~ normal(obs_location, obs_scale)\nend\nsamples = nuts_draws(hierarchical_model(;subject_idx, covariates, obs))\n\"Alternative 1: Constructing just the CV model\"\ncv_model = cv(\n    hierarchical_model; \n    subject_idx=held_out_subject_idx, \n    covariates=held_out_covariates, \n    obs=held_out_obs\n)\n\"Alternative 2: Constructing the CV model and computing the necessary quantities\"\ncv_info = cv(\n    samples; \n    subject_idx=held_out_subject_idx, \n    covariates=held_out_covariates, \n    obs=held_out_obs\n)\nIn the above example, the hierarchical parameters will be fixed and reused from samples, while the subject specific parameters will be automatically resampled inedependently for each draw to compute the likelihood. Determining which parameters get fixed/resued and which ones get resampled can be automatically determined by tracing through the model.\nAlternative syntax proposals are appreciated!\n\n\n\n\nThe aim is to make it easy to write computationally efficient code.\n\n\nLike SlicStan, @slic will support automatically determining whether model components are\n\ndata - passed to the model,\ntransformed data - need to be computed only once per model instantiation/conditioning/deconditioning,\nparameters - contribute to the posterior (MCMC-)dimension and potentially need to be transformed appropriately,\ntransformed parameters - have to be computed every gradient evaluation, because they do affect the likelihood,\ngenerated quantities - have to be computed only once per sample and can be sampled independently, because they do not affect the likelihood.\n\nThis allows for the natural model specification in a single place, while not sacrificing any performance.\nIn the below model,\n\nif we do not specify anything, every model component will be sampled independently,\nif we specify only obs_location or obs_scale, every other model component will still be sampled independently, and\nif we specify only obs, both obs_location and obs_scale become parameters, and obs_likelihood and obs_prediction get automatically added as generated quantities.\n\n@slic begin \n    obs_location ~ std_normal()\n    obs_scale ~ std_lognormal()\n    obs ~ normal(obs_location, obs_scale)\nend\n\n\n\nThe most efficient way to compute intermediate functional quantities will depend on the type, shape and potentially activity of the arguments passed to the function. We provide efficient primitives which explot this, e.g.:\nnormal_lpdf(y::AbstractVector, location::AbstractVector, scale::AbstractVector) = \"Do something which requires computing log.(scale).\"\nnormal_lpdf(y::AbstractVector, location::AbstractVector, scale::Real) = \"Do something which only requires computing log(scale) once.\"\n\n\n\nLike Stan, @slic models will use matrix expressions to keep the memory footprint low. This will (should) be more efficient than using something like a bump allocator and just-allocate-and-reuse away.\n\n\n\n\nThe aim is to make it easy to write correct code.\n\n\nLike Turing.jls unified way of defining parameters, unlike Stans split-across-blocks way of defining parameters.\n\n\n\nLike Turing.jl: a parameters constraints are automatically inferred from the support of its prior - but can of course be adjusted:\n@slic begin \n    \"Unconstrained\"\n    obs_intercept ~ std_normal()\n    \"Constrained to be positive\"\n    obs_scale ~ std_lognormal()\n    \"Constrained to be positive\"\n    obs_slope ~ std_normal(;lower=0.)\n    obs ~ normal(obs_intercept + obs_slope * x, obs_scale)\nend\n\n\n\nLike Turing.jl: the type of any model expression can be specified, but it need not be specified.\n\n\n\nWe will support tracing through a lot of common Julia syntax, like broadcasting, generators, maps and do blocks.\n\n\n\nWhile tracing will be limited to a subset of Julia syntax, unknown user defined functions will simply not be traced recursively. We will not allow these opaque functions to introduce model parameters, and we will assume the worst for the activity analysis.\n\n\n\n\nThe aim is to make it easy to extend @slics functionality.\n\n\nThe data/parameter/generated-quantities activity analysis comes out of two passes through the model (one forward, one reverse).\nThe cross-validation activity analysis comes out of another pass through the model.\nThere is nothing stopping us from allowing additional or alternative passes, and there should not be anything stopping you from implementing these custom passes.\nOne possible custom pass would e.g.translate the @slic model to a Stan program."
  },
  {
    "objectID": "slic/index.html#modularity",
    "href": "slic/index.html#modularity",
    "title": "The @slic macro",
    "section": "",
    "text": "The aim is to make it easy to iterate on the statistical model.\n\n\nLike Turing.jls submodels (wheres the documentation for this?) or SlicStans functions that declare parameters (first example in the paper).\nGetting tired of always coding up the same hierarchical priors? @slic will support reusing model components:\nhierarchical_prior = @slic begin\n    location ~ std_normal()\n    scale ~ std_lognormal()\n    x ~ normal(location, scale; n)\n    return x\nend\nhierarchical_model = @slic begin \n    obs_location ~ hierarchical_prior(;n)\n    obs_scale ~ std_lognormal()\n    obs ~ normal(obs_location, obs_scale)\nend\nAlternative syntax proposals are appreciated!\n\n\n\nEver wanted to switch out one prior for another, but didnt want to implement this functionality when coding up your first exploratory model? @slic will support switching out arbitrary model components:\n\"My first, exploratory model to check that things work\"\npooled_model = @slic begin \n    obs_location ~ std_normal()\n    obs_scale ~ std_lognormal()\n    obs ~ normal(obs_location, obs_scale)\nend\n\"Equivalent to the previous implementation of `hierarchical_model` above\"\nhierarchical_model = pooled_model(quote \n    obs_location ~ hierarchical_prior(;n)\nend)\nAlternative syntax proposals are appreciated!\n\n\n\nLike Turing.jls Conditioning or this Stan PR.\nEver wanted to pin hierarchical scale parameters? @slic will support pinning arbitrary model components:\n\"The `hierarchical_model` with the hierarchical scale parameter fixed to 1.\"\nsemihierarchical_model = hierarchical_model(;obs_location_scale=1.)\nTurings Deconditioning could also easily be supported, but as always syntax proposals are appreciated!\n\n\n\nGetting tired of reimplementing parts of your model to perform e.g.leave-one-subject-out cross-validation? @slic will support automatic model rewrites to perform e.g.automatic leave-one-subject-out cross-validation from the same model implementation that you have used to sample from the posterior:\nhierarchical_prior = @slic begin\n    location ~ std_normal()\n    scale ~ std_lognormal()\n    x ~ normal(location, scale; n)\n    return x\nend\nhierarchical_model = @slic begin \n    n = maximum(subject_idx)\n    obs_intercept ~ hierarchical_prior(;n)\n    obs_slope ~ hierarchical_prior(;n)\n    obs_location = obs_intercept + dot(covariates, obs_slope)\n    obs_scale ~ std_lognormal()\n    obs[subject_idx] ~ normal(obs_location, obs_scale)\nend\nsamples = nuts_draws(hierarchical_model(;subject_idx, covariates, obs))\n\"Alternative 1: Constructing just the CV model\"\ncv_model = cv(\n    hierarchical_model; \n    subject_idx=held_out_subject_idx, \n    covariates=held_out_covariates, \n    obs=held_out_obs\n)\n\"Alternative 2: Constructing the CV model and computing the necessary quantities\"\ncv_info = cv(\n    samples; \n    subject_idx=held_out_subject_idx, \n    covariates=held_out_covariates, \n    obs=held_out_obs\n)\nIn the above example, the hierarchical parameters will be fixed and reused from samples, while the subject specific parameters will be automatically resampled inedependently for each draw to compute the likelihood. Determining which parameters get fixed/resued and which ones get resampled can be automatically determined by tracing through the model.\nAlternative syntax proposals are appreciated!"
  },
  {
    "objectID": "slic/index.html#computational-efficiency",
    "href": "slic/index.html#computational-efficiency",
    "title": "The @slic macro",
    "section": "",
    "text": "The aim is to make it easy to write computationally efficient code.\n\n\nLike SlicStan, @slic will support automatically determining whether model components are\n\ndata - passed to the model,\ntransformed data - need to be computed only once per model instantiation/conditioning/deconditioning,\nparameters - contribute to the posterior (MCMC-)dimension and potentially need to be transformed appropriately,\ntransformed parameters - have to be computed every gradient evaluation, because they do affect the likelihood,\ngenerated quantities - have to be computed only once per sample and can be sampled independently, because they do not affect the likelihood.\n\nThis allows for the natural model specification in a single place, while not sacrificing any performance.\nIn the below model,\n\nif we do not specify anything, every model component will be sampled independently,\nif we specify only obs_location or obs_scale, every other model component will still be sampled independently, and\nif we specify only obs, both obs_location and obs_scale become parameters, and obs_likelihood and obs_prediction get automatically added as generated quantities.\n\n@slic begin \n    obs_location ~ std_normal()\n    obs_scale ~ std_lognormal()\n    obs ~ normal(obs_location, obs_scale)\nend\n\n\n\nThe most efficient way to compute intermediate functional quantities will depend on the type, shape and potentially activity of the arguments passed to the function. We provide efficient primitives which explot this, e.g.:\nnormal_lpdf(y::AbstractVector, location::AbstractVector, scale::AbstractVector) = \"Do something which requires computing log.(scale).\"\nnormal_lpdf(y::AbstractVector, location::AbstractVector, scale::Real) = \"Do something which only requires computing log(scale) once.\"\n\n\n\nLike Stan, @slic models will use matrix expressions to keep the memory footprint low. This will (should) be more efficient than using something like a bump allocator and just-allocate-and-reuse away."
  },
  {
    "objectID": "slic/index.html#expressiveness",
    "href": "slic/index.html#expressiveness",
    "title": "The @slic macro",
    "section": "",
    "text": "The aim is to make it easy to write correct code.\n\n\nLike Turing.jls unified way of defining parameters, unlike Stans split-across-blocks way of defining parameters.\n\n\n\nLike Turing.jl: a parameters constraints are automatically inferred from the support of its prior - but can of course be adjusted:\n@slic begin \n    \"Unconstrained\"\n    obs_intercept ~ std_normal()\n    \"Constrained to be positive\"\n    obs_scale ~ std_lognormal()\n    \"Constrained to be positive\"\n    obs_slope ~ std_normal(;lower=0.)\n    obs ~ normal(obs_intercept + obs_slope * x, obs_scale)\nend\n\n\n\nLike Turing.jl: the type of any model expression can be specified, but it need not be specified.\n\n\n\nWe will support tracing through a lot of common Julia syntax, like broadcasting, generators, maps and do blocks.\n\n\n\nWhile tracing will be limited to a subset of Julia syntax, unknown user defined functions will simply not be traced recursively. We will not allow these opaque functions to introduce model parameters, and we will assume the worst for the activity analysis."
  },
  {
    "objectID": "slic/index.html#extendability",
    "href": "slic/index.html#extendability",
    "title": "The @slic macro",
    "section": "",
    "text": "The aim is to make it easy to extend @slics functionality.\n\n\nThe data/parameter/generated-quantities activity analysis comes out of two passes through the model (one forward, one reverse).\nThe cross-validation activity analysis comes out of another pass through the model.\nThere is nothing stopping us from allowing additional or alternative passes, and there should not be anything stopping you from implementing these custom passes.\nOne possible custom pass would e.g.translate the @slic model to a Stan program."
  },
  {
    "objectID": "slic/isba-2024.html",
    "href": "slic/isba-2024.html",
    "title": "Reimplementing the Stan models from https://github.com/bob-carpenter/pcr-sensitivity-vs-time",
    "section": "",
    "text": "The full Julia code for this notebook can be accessed via the top right corner (&lt;/&gt; Code).\nThe Julia packages needed to reproduce this document are StanBlocks.jl (for the model generation) and QuartoComponents.jl (for the pretty printing). Both packages have to be installed from the latest main branch (as of Oct 14th 2025).\nI do agree that only having the option of inferring the type/transformation of a parameter from its sampling distribution can be unnecessarily magical - I have hence opened this feature reminder (I can obviously not request things from myself)."
  },
  {
    "objectID": "slic/isba-2024.html#stanblocks.jl-implementation",
    "href": "slic/isba-2024.html#stanblocks.jl-implementation",
    "title": "Reimplementing the Stan models from https://github.com/bob-carpenter/pcr-sensitivity-vs-time",
    "section": "StanBlocks.jl implementation",
    "text": "StanBlocks.jl implementation\nThe function and model definitions below make use of\n\nvariadic functions (and argument splatting) - It is often convenient to be able to write functions taking an arbitrary number of arguments.,\nfunction-type arguments and dispatch/method selection via their type - in Julia, [e]ach function has its own type, which is a subtype of Function.,\nuntyped arguments,\nautomatic type and dimension inference (see below) - partly because currently no other way of specifying type and dimension of a parameter is implemented. As stated above, I do agree that this is slightly too magical, and an optional(?) inline type annotation could be clearer.\n\nA few words on each of these points:\n\nVariadic functions\nThe below lines are two examples of a variadic method definition for the my_bernoulli_lpmfs function,\nmy_bernoulli_lpmfs(y::int[n], args...) = jbroadcasted(my_bernoulli_lpmfs, y, args...)\nmy_bernoulli_lpmfs(y::int, args...) = my_bernoulli_lpmf(y, args...)\nused in the computation of the pointwise log likelihoods. On the left hand side, args... will simply match all trailing positional arguments after the first one, and on the right hand side these arguments will be forwarded to the built-in jbroadcasted function, which mimics Julia-style broadcasting of its first (function-type) argument over all other arguments.\nIn the below models, my_bernoulli_lpmfs will be called with the following signatures:\nmy_bernoulli_lpmfs(y::int[n], f::typeof(logit), theta::vector[n])\nmy_bernoulli_lpmfs(y::int[n], f::typeof(log), theta::vector[n])\nmy_bernoulli_lpmfs(y::int, f::typeof(logit), theta::vector)\nmy_bernoulli_lpmfs(y::int, f::typeof(log), theta::vector)\nall of which will be covered by the variadic function definitions at the beginning of this section.\n\n\nFunction-type arguments and method selection via their type\nThe below are the simplest possible method definitions which depend on the type of a function-type argument:\nupper_alpha(::typeof(logit)) = negative_infinity()\nupper_alpha(::typeof(log)) = 0\nThe defined function, upper_alpha, can be called in one of two ways:\n\nEither as upper_alpha(logit), matching the first method definition and thus returning negative infinity, or\nas upper_alpha(log), matching the second method definition and thus returning zero.\n\nThe above function gets used in the regression and regression_mix model to make the upper bound of the alpha parameter depend on the link function link_f, which can be either logit or log.\nA slightly more complex example would be the following:\nmy_bernoulli_lpmf(y, ::typeof(logit), theta) = bernoulli_logit_lpmf(y, theta)\nmy_bernoulli_lpmf(y, ::typeof(log), theta) = bernoulli_lpmf(y, exp(theta))\nwhich gets used to make the likelihood implementation depend on the link function of the model, allowing us\n\nto forward y and theta to bernoulli_logit_lpmf(y, theta), or\nto forward y and theta to bernoulli_lpmf(y, exp(theta)).\n\n\n\nUntyped or abstractly typed arguments\nUntyped function arguments are simply arguments for which we dont specify the type beforehand, allowing it to match any passed in type. Do note that this can lead to Method Ambiguities - something that cannot happen in Stan because you always have to specify the concrete types of all function arguments. StanBlocks.jl implements a limited abstract type hierarchy, starting at the top with anything, and e.g.descending towards ordered as anything -&gt; any_vector -&gt; vector -&gt; ordered."
  },
  {
    "objectID": "slic/isba-2024.html#full-julia-stanblocks.jl-code-to-define-the-models",
    "href": "slic/isba-2024.html#full-julia-stanblocks.jl-code-to-define-the-models",
    "title": "Reimplementing the Stan models from https://github.com/bob-carpenter/pcr-sensitivity-vs-time",
    "section": "Full Julia + StanBlocks.jl code to define the models",
    "text": "Full Julia + StanBlocks.jl code to define the models\nThe following reproduces all of the code necessary to implement the 2x5 model matrix (printing excluded):\nusing StanBlocks\n\n@deffun begin \n    \"Needed for cross validation\"\n    my_bernoulli_lpmfs(y::int[n], args...) = jbroadcasted(my_bernoulli_lpmfs, y, args...)\n    \"Needed for cross validation\"\n    my_bernoulli_lpmfs(y::int, args...) = my_bernoulli_lpmf(y, args...)\n    \"Needed to compute the joint likelihood\"\n    my_bernoulli_lpmf(y, ::typeof(logit), theta) = bernoulli_logit_lpmf(y, theta)\n    \"Needed for posterior predictions\"\n    my_bernoulli_rng(::typeof(logit), theta) = bernoulli_logit_rng(theta)\n    \"Needed to compute the joint likelihood\"\n    my_bernoulli_lpmf(y, ::typeof(log), theta) = bernoulli_lpmf(y, exp(theta))\n    \"Needed for posterior predictions\"\n    my_bernoulli_rng(::typeof(log), theta) = bernoulli_rng(exp(theta))\n\n    \"The `Hetero` prior density after constraining - common to both link functions\"\n    hetero_lpdf(x::vector[n]) = normal_lpdf(x, 0, 3)\n    \"The `Hetero` prior density after constraining - this function definition gets used to infer the type and shape of the parameter for the logit link\"\n    hetero_lpdf(x::ordered[n], ::typeof(logit), n) = hetero_lpdf(x)\n    \"The `Hetero` prior density after constraining - this function definition gets used to infer the type and shape of the parameter for the log link\"\n    hetero_lpdf(x::positive_ordered[n], ::typeof(log), n) = hetero_lpdf(x)\n    \"The `RW(1)` prior density after constraining - common to both link functions\"\n    rw1_lpdf(x::vector[n], sigma) = normal_lpdf(x[1], 0, sigma) + normal_lpdf(x[2:n], x[1:n-1], sigma)\n    \"The `RW(1)` prior density after constraining - this function definition gets used to infer the type and shape of the parameter for the logit link\"\n    rw1_lpdf(x::ordered[n], ::typeof(logit), sigma, n) = rw1_lpdf(x, sigma)\n    \"The `RW(1)` prior density after constraining - this function definition gets used to infer the type and shape of the parameter for the log link\"\n    rw1_lpdf(x::positive_ordered[n], ::typeof(log), sigma, n) = rw1_lpdf(x, sigma)\n    \"The `RW(2)` prior density after constraining - common to both link functions\"\n    rw2_lpdf(x::vector[n], sigma) = rw1_lpdf(x[1:2], sigma) + normal_lpdf(x[3:n], 2x[2:n-1] - x[1:n-2], sigma)\n    \"The `RW(2)` prior density after constraining - this function definition gets used to infer the type and shape of the parameter for the logit link\"\n    rw2_lpdf(x::ordered[n], ::typeof(logit), sigma, n) = rw2_lpdf(x, sigma)\n    \"The `RW(2)` prior density after constraining - this function definition gets used to infer the type and shape of the parameter for the log link\"\n    rw2_lpdf(x::positive_ordered[n], ::typeof(log), sigma, n) = rw2_lpdf(x, sigma)\n    \"The upper bound of the alpha parameter (for the logit link function)\"\n    upper_alpha(::typeof(logit)) = negative_infinity()\n    \"The upper bound of the alpha parameter (for the log link function)\"\n    upper_alpha(::typeof(log)) = 0\nend\n# The main things that StanBlocks.jl has to know is that `y` is an array of `int`s, and `t` is a vector.\nmock_data = (;y=[1], t=[1.])\n\n# The base model with the mock data attached - this does not yet have an implementation for theta \nbase_model = @slic mock_data begin \n    n = dims(y)[1]\n    \"The exact implementation of the likelihood depends on the passed link function `link_f`\"\n    y ~ my_bernoulli(link_f, theta)\nend\n# The submodel for the `Hetero` models\ncentered_hetero = @slic begin \n    \"The type of the `xi` parameter depends on the passed link function `link_f`\"\n    xi ~ hetero(link_f, n)\n    \"The negation is needed to ensure that `theta` is in descending order\"\n    return -xi\nend\n# The submodel for the `RW(1)` models\ncentered_rw1 = @slic begin \n    sigma ~ std_normal(;lower=0)\n    \"The type of the `xi` parameter depends on the passed link function `link_f`\"\n    xi ~ rw1(link_f, sigma, n)\n    \"The negation is needed to ensure that `theta` is in descending order\"\n    return -xi\nend\n# The submodel for the `RW(2)` models\ncentered_rw2 = @slic begin \n    sigma ~ normal(0, .5; lower=0)\n    \"The type of the `xi` parameter depends on the passed link function `link_f`\"\n    xi ~ rw2(link_f, sigma, n)\n    \"The negation is needed to ensure that `theta` is in descending order\"\n    return -xi\nend\n# The submodel for the `regression` models - reused in the `regression_mix` models\nregression = @slic begin \n    \"The upper bound of alpha `alpha_upper` depends on the link function `link_f`\"\n    alpha_upper = upper_alpha(link_f)\n    # This could be `alpha ~ normal(0, .5; upper=upper_alpha(link_f))`, once https://github.com/nsiccha/StanBlocks.jl/issues/35 is fixed\n    alpha ~ normal(0, .5; upper=alpha_upper)\n    beta ~ normal(0, .5; upper=0.)\n    return alpha + beta * t\nend\n# The submodel for the `regression_mix` models - reusing the `regression` submodel\nregression_mix = @slic begin \n    c1 ~ regression(;link_f, t) \n    c2 ~ regression(;link_f, t)\n    lambda ~ beta(2, 2)\n    return lambda * link_f(c1) + (1-lambda) * link_f(c2)\nend\n\nbases = (;\n    hetero=base_model(quote \n        theta ~ centered_hetero(;link_f, n)\n    end),\n    rw1=base_model(quote \n        theta ~ centered_rw1(;link_f, n)\n    end),\n    rw2=base_model(quote \n        theta ~ centered_rw2(;link_f, n)\n    end),\n    regression=base_model(quote \n        theta ~ regression(;link_f, t)\n    end),\n    regression_mix=base_model(quote \n        theta ~ regression(;link_f, t)\n        y ~ bernoulli(theta)\n    end)\n)\nlink_fs = (;logit, log)\n\nposteriors = map(bases) do base \n    map(link_fs) do link_f \n        base(;link_f)\n    end\nend"
  },
  {
    "objectID": "slic/isba-2024.html#generated-stan-code",
    "href": "slic/isba-2024.html#generated-stan-code",
    "title": "Reimplementing the Stan models from https://github.com/bob-carpenter/pcr-sensitivity-vs-time",
    "section": "Generated Stan code",
    "text": "Generated Stan code\nThe generated Stan code below is accessible via two nested tabsets. The top level (with keys hetero, rw1, rw2, regression, and regression_mix) combines with the link function in the second level (with keys logit and log) to give you access to the corresponding Stan models from the poster.\n\n\n\n\n\n\nWarning\n\n\n\nDue to this issue, the below Stan codes should not actually compile as they are.\nI think removing the offending UDF definitions should make compilation work.\n\n\n\n\n\nheterorw1rw2regressionregression_mix\n\n\n\nlogitlog\n\n\nfunctions {\n// The `Hetero` prior density after constraining - this function definition gets used to infer the type and shape of the parameter for the logit link\nreal hetero_logit_lpdf(\n    vector x,\n    int n\n) {\n    return hetero_lpdf(x);\n}\n// The `Hetero` prior density after constraining - common to both link functions\nreal hetero_lpdf(\n    vector x\n) {\n    int n = dims(x)[1];\n    return normal_lpdf(x | 0, 3);\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_logit_lpmf(\n    array[] int y,\n    vector theta\n) {\n    return bernoulli_logit_lpmf(y | theta);\n}\n// Needed for cross validation\nvector my_bernoulli_logit_lpmfs(\n    array[] int y,\n    vector args1\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_my_bernoulli_logit_lpmfs(y, args1);\n}\nvector jbroadcasted_my_bernoulli_logit_lpmfs(\n    array[] int x1,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = my_bernoulli_logit_lpmfs(broadcasted_getindex(x1, i), broadcasted_getindex(x3, i));\n    }\n    return rv;\n}\n// Needed for cross validation\nreal my_bernoulli_logit_lpmfs(\n    int y,\n    real args1\n) {\n    return my_bernoulli_logit_lpmf(y | args1);\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_logit_lpmf(\n    int y,\n    real theta\n) {\n    return bernoulli_logit_lpmf(y | theta);\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nfunc broadcasted_getindex_logit(int i) {\n    return logit;\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n// Needed for posterior predictions\narray[] int my_bernoulli_logit_rng(\n    vector theta\n) {\n    return bernoulli_logit_rng(theta);\n}\n}\ndata {\n    int y_n;\n    array[y_n] int y;\n}\ntransformed data {\n    int n = dims(y)[1];\n}\nparameters {\n    // The type of the `xi` parameter depends on the passed link function `link_f`\n    ordered[n] theta_xi;\n}\ntransformed parameters {\n    // The negation is needed to ensure that `theta` is in descending order\n    vector[n] theta = (-theta_xi);\n}\nmodel {\n    // The type of the `xi` parameter depends on the passed link function `link_f`\n    theta_xi ~ hetero_logit(n);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    y ~ my_bernoulli_logit(theta);\n}\ngenerated quantities {\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    vector[y_n] y_likelihood = my_bernoulli_logit_lpmfs(y, theta);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    array[n] int y_gen = my_bernoulli_logit_rng(theta);\n}\n\n\nfunctions {\n// The `Hetero` prior density after constraining - this function definition gets used to infer the type and shape of the parameter for the log link\nreal hetero_log_lpdf(\n    vector x,\n    int n\n) {\n    return hetero_lpdf(x);\n}\n// The `Hetero` prior density after constraining - common to both link functions\nreal hetero_lpdf(\n    vector x\n) {\n    int n = dims(x)[1];\n    return normal_lpdf(x | 0, 3);\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_log_lpmf(\n    array[] int y,\n    vector theta\n) {\n    return bernoulli_lpmf(y | exp(theta));\n}\n// Needed for cross validation\nvector my_bernoulli_log_lpmfs(\n    array[] int y,\n    vector args1\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_my_bernoulli_log_lpmfs(y, args1);\n}\nvector jbroadcasted_my_bernoulli_log_lpmfs(\n    array[] int x1,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = my_bernoulli_log_lpmfs(broadcasted_getindex(x1, i), broadcasted_getindex(x3, i));\n    }\n    return rv;\n}\n// Needed for cross validation\nreal my_bernoulli_log_lpmfs(\n    int y,\n    real args1\n) {\n    return my_bernoulli_log_lpmf(y | args1);\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_log_lpmf(\n    int y,\n    real theta\n) {\n    return bernoulli_lpmf(y | exp(theta));\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nfunc broadcasted_getindex_log(int i) {\n    return log;\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n// Needed for posterior predictions\narray[] int my_bernoulli_log_rng(\n    vector theta\n) {\n    return bernoulli_rng(exp(theta));\n}\n}\ndata {\n    int y_n;\n    array[y_n] int y;\n}\ntransformed data {\n    int n = dims(y)[1];\n}\nparameters {\n    // The type of the `xi` parameter depends on the passed link function `link_f`\n    positive_ordered[n] theta_xi;\n}\ntransformed parameters {\n    // The negation is needed to ensure that `theta` is in descending order\n    vector[n] theta = (-theta_xi);\n}\nmodel {\n    // The type of the `xi` parameter depends on the passed link function `link_f`\n    theta_xi ~ hetero_log(n);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    y ~ my_bernoulli_log(theta);\n}\ngenerated quantities {\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    vector[y_n] y_likelihood = my_bernoulli_log_lpmfs(y, theta);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    array[n] int y_gen = my_bernoulli_log_rng(theta);\n}\n\n\n\n\n\n\nlogitlog\n\n\nfunctions {\n// The `RW(1)` prior density after constraining - this function definition gets used to infer the type and shape of the parameter for the logit link\nreal rw1_logit_lpdf(\n    vector x,\n    real sigma,\n    int n\n) {\n    return rw1_lpdf(x | sigma);\n}\n// The `RW(1)` prior density after constraining - common to both link functions\nreal rw1_lpdf(\n    vector x,\n    real sigma\n) {\n    int n = dims(x)[1];\n    return (normal_lpdf(x[1] | 0, sigma) + normal_lpdf(x[2:n] | x[1:(n - 1)], sigma));\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_logit_lpmf(\n    array[] int y,\n    vector theta\n) {\n    return bernoulli_logit_lpmf(y | theta);\n}\n// Needed for cross validation\nvector my_bernoulli_logit_lpmfs(\n    array[] int y,\n    vector args1\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_my_bernoulli_logit_lpmfs(y, args1);\n}\nvector jbroadcasted_my_bernoulli_logit_lpmfs(\n    array[] int x1,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = my_bernoulli_logit_lpmfs(broadcasted_getindex(x1, i), broadcasted_getindex(x3, i));\n    }\n    return rv;\n}\n// Needed for cross validation\nreal my_bernoulli_logit_lpmfs(\n    int y,\n    real args1\n) {\n    return my_bernoulli_logit_lpmf(y | args1);\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_logit_lpmf(\n    int y,\n    real theta\n) {\n    return bernoulli_logit_lpmf(y | theta);\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nfunc broadcasted_getindex_logit(int i) {\n    return logit;\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n// Needed for posterior predictions\narray[] int my_bernoulli_logit_rng(\n    vector theta\n) {\n    return bernoulli_logit_rng(theta);\n}\n}\ndata {\n    int y_n;\n    array[y_n] int y;\n}\ntransformed data {\n    int n = dims(y)[1];\n}\nparameters {\n    real&lt;lower=0&gt; theta_sigma;\n    // The type of the `xi` parameter depends on the passed link function `link_f`\n    ordered[n] theta_xi;\n}\ntransformed parameters {\n    // The negation is needed to ensure that `theta` is in descending order\n    vector[n] theta = (-theta_xi);\n}\nmodel {\n    theta_sigma ~ std_normal();\n    // The type of the `xi` parameter depends on the passed link function `link_f`\n    theta_xi ~ rw1_logit(theta_sigma, n);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    y ~ my_bernoulli_logit(theta);\n}\ngenerated quantities {\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    vector[y_n] y_likelihood = my_bernoulli_logit_lpmfs(y, theta);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    array[n] int y_gen = my_bernoulli_logit_rng(theta);\n}\n\n\nfunctions {\n// The `RW(1)` prior density after constraining - this function definition gets used to infer the type and shape of the parameter for the log link\nreal rw1_log_lpdf(\n    vector x,\n    real sigma,\n    int n\n) {\n    return rw1_lpdf(x | sigma);\n}\n// The `RW(1)` prior density after constraining - common to both link functions\nreal rw1_lpdf(\n    vector x,\n    real sigma\n) {\n    int n = dims(x)[1];\n    return (normal_lpdf(x[1] | 0, sigma) + normal_lpdf(x[2:n] | x[1:(n - 1)], sigma));\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_log_lpmf(\n    array[] int y,\n    vector theta\n) {\n    return bernoulli_lpmf(y | exp(theta));\n}\n// Needed for cross validation\nvector my_bernoulli_log_lpmfs(\n    array[] int y,\n    vector args1\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_my_bernoulli_log_lpmfs(y, args1);\n}\nvector jbroadcasted_my_bernoulli_log_lpmfs(\n    array[] int x1,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = my_bernoulli_log_lpmfs(broadcasted_getindex(x1, i), broadcasted_getindex(x3, i));\n    }\n    return rv;\n}\n// Needed for cross validation\nreal my_bernoulli_log_lpmfs(\n    int y,\n    real args1\n) {\n    return my_bernoulli_log_lpmf(y | args1);\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_log_lpmf(\n    int y,\n    real theta\n) {\n    return bernoulli_lpmf(y | exp(theta));\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nfunc broadcasted_getindex_log(int i) {\n    return log;\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n// Needed for posterior predictions\narray[] int my_bernoulli_log_rng(\n    vector theta\n) {\n    return bernoulli_rng(exp(theta));\n}\n}\ndata {\n    int y_n;\n    array[y_n] int y;\n}\ntransformed data {\n    int n = dims(y)[1];\n}\nparameters {\n    real&lt;lower=0&gt; theta_sigma;\n    // The type of the `xi` parameter depends on the passed link function `link_f`\n    positive_ordered[n] theta_xi;\n}\ntransformed parameters {\n    // The negation is needed to ensure that `theta` is in descending order\n    vector[n] theta = (-theta_xi);\n}\nmodel {\n    theta_sigma ~ std_normal();\n    // The type of the `xi` parameter depends on the passed link function `link_f`\n    theta_xi ~ rw1_log(theta_sigma, n);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    y ~ my_bernoulli_log(theta);\n}\ngenerated quantities {\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    vector[y_n] y_likelihood = my_bernoulli_log_lpmfs(y, theta);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    array[n] int y_gen = my_bernoulli_log_rng(theta);\n}\n\n\n\n\n\n\nlogitlog\n\n\nfunctions {\n// The `RW(2)` prior density after constraining - this function definition gets used to infer the type and shape of the parameter for the logit link\nreal rw2_logit_lpdf(\n    vector x,\n    real sigma,\n    int n\n) {\n    return rw2_lpdf(x | sigma);\n}\n// The `RW(2)` prior density after constraining - common to both link functions\nreal rw2_lpdf(\n    vector x,\n    real sigma\n) {\n    int n = dims(x)[1];\n    return (rw1_lpdf(x[1:2] | sigma) + normal_lpdf(x[3:n] | ((2 * x[2:(n - 1)]) - x[1:(n - 2)]), sigma));\n}\n// The `RW(1)` prior density after constraining - common to both link functions\nreal rw1_lpdf(\n    vector x,\n    real sigma\n) {\n    int n = dims(x)[1];\n    return (normal_lpdf(x[1] | 0, sigma) + normal_lpdf(x[2:n] | x[1:(n - 1)], sigma));\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_logit_lpmf(\n    array[] int y,\n    vector theta\n) {\n    return bernoulli_logit_lpmf(y | theta);\n}\n// Needed for cross validation\nvector my_bernoulli_logit_lpmfs(\n    array[] int y,\n    vector args1\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_my_bernoulli_logit_lpmfs(y, args1);\n}\nvector jbroadcasted_my_bernoulli_logit_lpmfs(\n    array[] int x1,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = my_bernoulli_logit_lpmfs(broadcasted_getindex(x1, i), broadcasted_getindex(x3, i));\n    }\n    return rv;\n}\n// Needed for cross validation\nreal my_bernoulli_logit_lpmfs(\n    int y,\n    real args1\n) {\n    return my_bernoulli_logit_lpmf(y | args1);\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_logit_lpmf(\n    int y,\n    real theta\n) {\n    return bernoulli_logit_lpmf(y | theta);\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nfunc broadcasted_getindex_logit(int i) {\n    return logit;\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n// Needed for posterior predictions\narray[] int my_bernoulli_logit_rng(\n    vector theta\n) {\n    return bernoulli_logit_rng(theta);\n}\n}\ndata {\n    int y_n;\n    array[y_n] int y;\n}\ntransformed data {\n    int n = dims(y)[1];\n}\nparameters {\n    real&lt;lower=0&gt; theta_sigma;\n    // The type of the `xi` parameter depends on the passed link function `link_f`\n    ordered[n] theta_xi;\n}\ntransformed parameters {\n    // The negation is needed to ensure that `theta` is in descending order\n    vector[n] theta = (-theta_xi);\n}\nmodel {\n    theta_sigma ~ normal(0, 0.5);\n    // The type of the `xi` parameter depends on the passed link function `link_f`\n    theta_xi ~ rw2_logit(theta_sigma, n);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    y ~ my_bernoulli_logit(theta);\n}\ngenerated quantities {\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    vector[y_n] y_likelihood = my_bernoulli_logit_lpmfs(y, theta);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    array[n] int y_gen = my_bernoulli_logit_rng(theta);\n}\n\n\nfunctions {\n// The `RW(2)` prior density after constraining - this function definition gets used to infer the type and shape of the parameter for the log link\nreal rw2_log_lpdf(\n    vector x,\n    real sigma,\n    int n\n) {\n    return rw2_lpdf(x | sigma);\n}\n// The `RW(2)` prior density after constraining - common to both link functions\nreal rw2_lpdf(\n    vector x,\n    real sigma\n) {\n    int n = dims(x)[1];\n    return (rw1_lpdf(x[1:2] | sigma) + normal_lpdf(x[3:n] | ((2 * x[2:(n - 1)]) - x[1:(n - 2)]), sigma));\n}\n// The `RW(1)` prior density after constraining - common to both link functions\nreal rw1_lpdf(\n    vector x,\n    real sigma\n) {\n    int n = dims(x)[1];\n    return (normal_lpdf(x[1] | 0, sigma) + normal_lpdf(x[2:n] | x[1:(n - 1)], sigma));\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_log_lpmf(\n    array[] int y,\n    vector theta\n) {\n    return bernoulli_lpmf(y | exp(theta));\n}\n// Needed for cross validation\nvector my_bernoulli_log_lpmfs(\n    array[] int y,\n    vector args1\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_my_bernoulli_log_lpmfs(y, args1);\n}\nvector jbroadcasted_my_bernoulli_log_lpmfs(\n    array[] int x1,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = my_bernoulli_log_lpmfs(broadcasted_getindex(x1, i), broadcasted_getindex(x3, i));\n    }\n    return rv;\n}\n// Needed for cross validation\nreal my_bernoulli_log_lpmfs(\n    int y,\n    real args1\n) {\n    return my_bernoulli_log_lpmf(y | args1);\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_log_lpmf(\n    int y,\n    real theta\n) {\n    return bernoulli_lpmf(y | exp(theta));\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nfunc broadcasted_getindex_log(int i) {\n    return log;\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n// Needed for posterior predictions\narray[] int my_bernoulli_log_rng(\n    vector theta\n) {\n    return bernoulli_rng(exp(theta));\n}\n}\ndata {\n    int y_n;\n    array[y_n] int y;\n}\ntransformed data {\n    int n = dims(y)[1];\n}\nparameters {\n    real&lt;lower=0&gt; theta_sigma;\n    // The type of the `xi` parameter depends on the passed link function `link_f`\n    positive_ordered[n] theta_xi;\n}\ntransformed parameters {\n    // The negation is needed to ensure that `theta` is in descending order\n    vector[n] theta = (-theta_xi);\n}\nmodel {\n    theta_sigma ~ normal(0, 0.5);\n    // The type of the `xi` parameter depends on the passed link function `link_f`\n    theta_xi ~ rw2_log(theta_sigma, n);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    y ~ my_bernoulli_log(theta);\n}\ngenerated quantities {\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    vector[y_n] y_likelihood = my_bernoulli_log_lpmfs(y, theta);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    array[n] int y_gen = my_bernoulli_log_rng(theta);\n}\n\n\n\n\n\n\nlogitlog\n\n\nfunctions {\n// The upper bound of the alpha parameter (for the logit link function)\nreal upper_alpha_logit(\n    \n) {\n    return negative_infinity();\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_logit_lpmf(\n    array[] int y,\n    vector theta\n) {\n    return bernoulli_logit_lpmf(y | theta);\n}\n// Needed for cross validation\nvector my_bernoulli_logit_lpmfs(\n    array[] int y,\n    vector args1\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_my_bernoulli_logit_lpmfs(y, args1);\n}\nvector jbroadcasted_my_bernoulli_logit_lpmfs(\n    array[] int x1,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = my_bernoulli_logit_lpmfs(broadcasted_getindex(x1, i), broadcasted_getindex(x3, i));\n    }\n    return rv;\n}\n// Needed for cross validation\nreal my_bernoulli_logit_lpmfs(\n    int y,\n    real args1\n) {\n    return my_bernoulli_logit_lpmf(y | args1);\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_logit_lpmf(\n    int y,\n    real theta\n) {\n    return bernoulli_logit_lpmf(y | theta);\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nfunc broadcasted_getindex_logit(int i) {\n    return logit;\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n// Needed for posterior predictions\narray[] int my_bernoulli_logit_rng(\n    vector theta\n) {\n    return bernoulli_logit_rng(theta);\n}\n}\ndata {\n    int y_n;\n    array[y_n] int y;\n    int t_n;\n    vector[t_n] t;\n}\ntransformed data {\n    int n = dims(y)[1];\n    // The upper bound of alpha `alpha_upper` depends on the link function `link_f`\n    real theta_alpha_upper = upper_alpha_logit();\n}\nparameters {\n    real&lt;upper=theta_alpha_upper&gt; theta_alpha;\n    real&lt;upper=0.0&gt; theta_beta;\n}\ntransformed parameters {\n    vector[t_n] theta = (theta_alpha + (theta_beta * t));\n}\nmodel {\n    theta_alpha ~ normal(0, 0.5);\n    theta_beta ~ normal(0, 0.5);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    y ~ my_bernoulli_logit(theta);\n}\ngenerated quantities {\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    vector[y_n] y_likelihood = my_bernoulli_logit_lpmfs(y, theta);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    array[t_n] int y_gen = my_bernoulli_logit_rng(theta);\n}\n\n\nfunctions {\n// The upper bound of the alpha parameter (for the log link function)\nint upper_alpha_log(\n    \n) {\n    return 0;\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_log_lpmf(\n    array[] int y,\n    vector theta\n) {\n    return bernoulli_lpmf(y | exp(theta));\n}\n// Needed for cross validation\nvector my_bernoulli_log_lpmfs(\n    array[] int y,\n    vector args1\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_my_bernoulli_log_lpmfs(y, args1);\n}\nvector jbroadcasted_my_bernoulli_log_lpmfs(\n    array[] int x1,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = my_bernoulli_log_lpmfs(broadcasted_getindex(x1, i), broadcasted_getindex(x3, i));\n    }\n    return rv;\n}\n// Needed for cross validation\nreal my_bernoulli_log_lpmfs(\n    int y,\n    real args1\n) {\n    return my_bernoulli_log_lpmf(y | args1);\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_log_lpmf(\n    int y,\n    real theta\n) {\n    return bernoulli_lpmf(y | exp(theta));\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nfunc broadcasted_getindex_log(int i) {\n    return log;\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n// Needed for posterior predictions\narray[] int my_bernoulli_log_rng(\n    vector theta\n) {\n    return bernoulli_rng(exp(theta));\n}\n}\ndata {\n    int y_n;\n    array[y_n] int y;\n    int t_n;\n    vector[t_n] t;\n}\ntransformed data {\n    int n = dims(y)[1];\n    // The upper bound of alpha `alpha_upper` depends on the link function `link_f`\n    int theta_alpha_upper = upper_alpha_log();\n}\nparameters {\n    real&lt;upper=theta_alpha_upper&gt; theta_alpha;\n    real&lt;upper=0.0&gt; theta_beta;\n}\ntransformed parameters {\n    vector[t_n] theta = (theta_alpha + (theta_beta * t));\n}\nmodel {\n    theta_alpha ~ normal(0, 0.5);\n    theta_beta ~ normal(0, 0.5);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    y ~ my_bernoulli_log(theta);\n}\ngenerated quantities {\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    vector[y_n] y_likelihood = my_bernoulli_log_lpmfs(y, theta);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    array[t_n] int y_gen = my_bernoulli_log_rng(theta);\n}\n\n\n\n\n\n\nlogitlog\n\n\nfunctions {\n// The upper bound of the alpha parameter (for the logit link function)\nreal upper_alpha_logit(\n    \n) {\n    return negative_infinity();\n}\nvector bernoulli_lpmfs(\n    array[] int y,\n    vector args1\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_bernoulli_lpmfs(y, args1);\n}\nvector jbroadcasted_bernoulli_lpmfs(\n    array[] int x1,\n    vector x2\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = bernoulli_lpmfs(broadcasted_getindex(x1, i), broadcasted_getindex(x2, i));\n    }\n    return rv;\n}\nreal bernoulli_lpmfs(int y, real args1) {\n    return bernoulli_lpmf(y | args1);\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_logit_lpmf(\n    array[] int y,\n    vector theta\n) {\n    return bernoulli_logit_lpmf(y | theta);\n}\n// Needed for cross validation\nvector my_bernoulli_logit_lpmfs(\n    array[] int y,\n    vector args1\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_my_bernoulli_logit_lpmfs(y, args1);\n}\nvector jbroadcasted_my_bernoulli_logit_lpmfs(\n    array[] int x1,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = my_bernoulli_logit_lpmfs(broadcasted_getindex(x1, i), broadcasted_getindex(x3, i));\n    }\n    return rv;\n}\n// Needed for cross validation\nreal my_bernoulli_logit_lpmfs(\n    int y,\n    real args1\n) {\n    return my_bernoulli_logit_lpmf(y | args1);\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_logit_lpmf(\n    int y,\n    real theta\n) {\n    return bernoulli_logit_lpmf(y | theta);\n}\nfunc broadcasted_getindex_logit(int i) {\n    return logit;\n}\n// Needed for posterior predictions\narray[] int my_bernoulli_logit_rng(\n    vector theta\n) {\n    return bernoulli_logit_rng(theta);\n}\n}\ndata {\n    int t_n;\n    vector[t_n] t;\n    int y_n;\n    array[y_n] int y;\n}\ntransformed data {\n    // The upper bound of alpha `alpha_upper` depends on the link function `link_f`\n    real theta_alpha_upper = upper_alpha_logit();\n    int n = dims(y)[1];\n}\nparameters {\n    real&lt;upper=theta_alpha_upper&gt; theta_alpha;\n    real&lt;upper=0.0&gt; theta_beta;\n}\ntransformed parameters {\n    vector[t_n] theta = (theta_alpha + (theta_beta * t));\n}\nmodel {\n    theta_alpha ~ normal(0, 0.5);\n    theta_beta ~ normal(0, 0.5);\n    y ~ bernoulli(theta);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    y ~ my_bernoulli_logit(theta);\n}\ngenerated quantities {\n    vector[y_n] y_likelihood = bernoulli_lpmfs(y, theta);\n    array[t_n] int y_gen = bernoulli_rng(theta);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    vector[y_n] y_likelihood = my_bernoulli_logit_lpmfs(y, theta);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    array[t_n] int y_gen = my_bernoulli_logit_rng(theta);\n}\n\n\nfunctions {\n// The upper bound of the alpha parameter (for the log link function)\nint upper_alpha_log(\n    \n) {\n    return 0;\n}\nvector bernoulli_lpmfs(\n    array[] int y,\n    vector args1\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_bernoulli_lpmfs(y, args1);\n}\nvector jbroadcasted_bernoulli_lpmfs(\n    array[] int x1,\n    vector x2\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = bernoulli_lpmfs(broadcasted_getindex(x1, i), broadcasted_getindex(x2, i));\n    }\n    return rv;\n}\nreal bernoulli_lpmfs(int y, real args1) {\n    return bernoulli_lpmf(y | args1);\n}\nint broadcasted_getindex(array[] int x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\nreal broadcasted_getindex(vector x, int i) {\n    int m = dims(x)[1];\n    return x[i];\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_log_lpmf(\n    array[] int y,\n    vector theta\n) {\n    return bernoulli_lpmf(y | exp(theta));\n}\n// Needed for cross validation\nvector my_bernoulli_log_lpmfs(\n    array[] int y,\n    vector args1\n) {\n    int n = dims(y)[1];\n    return jbroadcasted_my_bernoulli_log_lpmfs(y, args1);\n}\nvector jbroadcasted_my_bernoulli_log_lpmfs(\n    array[] int x1,\n    vector x3\n) {\n    int n = dims(x1)[1];\n    vector[n] rv;\n    for(i in 1:n) {\n        rv[i] = my_bernoulli_log_lpmfs(broadcasted_getindex(x1, i), broadcasted_getindex(x3, i));\n    }\n    return rv;\n}\n// Needed for cross validation\nreal my_bernoulli_log_lpmfs(\n    int y,\n    real args1\n) {\n    return my_bernoulli_log_lpmf(y | args1);\n}\n// Needed to compute the joint likelihood\nreal my_bernoulli_log_lpmf(\n    int y,\n    real theta\n) {\n    return bernoulli_lpmf(y | exp(theta));\n}\nfunc broadcasted_getindex_log(int i) {\n    return log;\n}\n// Needed for posterior predictions\narray[] int my_bernoulli_log_rng(\n    vector theta\n) {\n    return bernoulli_rng(exp(theta));\n}\n}\ndata {\n    int t_n;\n    vector[t_n] t;\n    int y_n;\n    array[y_n] int y;\n}\ntransformed data {\n    // The upper bound of alpha `alpha_upper` depends on the link function `link_f`\n    int theta_alpha_upper = upper_alpha_log();\n    int n = dims(y)[1];\n}\nparameters {\n    real&lt;upper=theta_alpha_upper&gt; theta_alpha;\n    real&lt;upper=0.0&gt; theta_beta;\n}\ntransformed parameters {\n    vector[t_n] theta = (theta_alpha + (theta_beta * t));\n}\nmodel {\n    theta_alpha ~ normal(0, 0.5);\n    theta_beta ~ normal(0, 0.5);\n    y ~ bernoulli(theta);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    y ~ my_bernoulli_log(theta);\n}\ngenerated quantities {\n    vector[y_n] y_likelihood = bernoulli_lpmfs(y, theta);\n    array[t_n] int y_gen = bernoulli_rng(theta);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    vector[y_n] y_likelihood = my_bernoulli_log_lpmfs(y, theta);\n    // The exact implementation of the likelihood depends on the passed link function `link_f`\n    array[t_n] int y_gen = my_bernoulli_log_rng(theta);\n}"
  },
  {
    "objectID": "slic/isba-2024.html#how-does-stanblocks.jl-infer-type-and-dimension",
    "href": "slic/isba-2024.html#how-does-stanblocks.jl-infer-type-and-dimension",
    "title": "Reimplementing the Stan models from https://github.com/bob-carpenter/pcr-sensitivity-vs-time",
    "section": "How does StanBlocks.jl infer type and dimension?",
    "text": "How does StanBlocks.jl infer type and dimension?\nThe way StanBlocks.jl currently operates to figure out the type and dimension of a model parameter defined by\ntheta ~ distribution(foo, bar)\nis via looking for a function definition\ndistribution_lpdf(theta::theta_type[theta_dim1, ...], foo::foo_type[...], bar::bar_type[...]) = ...\nmatching the types of foo and bar. theta_type then has to be one of Stans built-in types, and theta_dim, ... can be expressions depending on foo, foo_type[...], bar, and bar_type[...].\nFor example, the built-in definition\nmulti_normal_lpdf(obs::vector[n], loc::vector[n], cov)\ntells StanBlocks.jl that any parameter theta initialized via\ntheta ~ multi_normal(loc, cov)\nwill be a vector of the same shape as loc.\nOnce I add this feature, an alternative syntax to specify types and dimensions could be\ntheta::vector[n] ~ distribution(foo, bar)\nwhich would communicate intent more clearly, both to the programmer and the compiler."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "index.html#differences-in-the-returned-log-density",
    "href": "index.html#differences-in-the-returned-log-density",
    "title": "",
    "section": "Differences in the returned log-density",
    "text": "Differences in the returned log-density\nStans default sampling statement (e.g.y ~ normal(mu, sigma);) automatically drops constant terms (unless configured differently), see https://mc-stan.org/docs/reference-manual/statements.html#log-probability-increment-vs.-distribution-statement. Constant terms are terms which do not depend on model parameters, and this packages macros and functions currently do not try to figure out which terms do not depend on model parameters, and as such we never drop them. This may lead to (constant) differences in the computed log-densities from the Stan and Julia implementations."
  },
  {
    "objectID": "index.html#some-models-are-not-implemented-yet-or-may-have-smaller-or-bigger-errors",
    "href": "index.html#some-models-are-not-implemented-yet-or-may-have-smaller-or-bigger-errors",
    "title": "",
    "section": "Some models are not implemented yet, or may have smaller or bigger errors",
    "text": "Some models are not implemented yet, or may have smaller or bigger errors\nIve implemented many of the models, but I havent implemented all of them, and I probably have made some mistakes in implementing some of them."
  },
  {
    "objectID": "index.html#some-models-may-have-been-implemented-suboptimally",
    "href": "index.html#some-models-may-have-been-implemented-suboptimally",
    "title": "",
    "section": "Some models may have been implemented suboptimally",
    "text": "Some models may have been implemented suboptimally\nJust that."
  }
]