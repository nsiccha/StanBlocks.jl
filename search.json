[
  {
    "objectID": "implementations.html",
    "href": "implementations.html",
    "title": "Julia posteriordb implementations",
    "section": "",
    "text": "See the PosteriorDB.jl extension at https://github.com/nsiccha/StanBlocks.jl/blob/main/ext/PosteriorDBExt.jl, also included below:\nmodule PosteriorDBExt\n\nimport PosteriorDB\nimport StanBlocks\nimport StanBlocks: julia_implementation, @stan, @parameters, @transformed_parameters, @model, @broadcasted\nimport StanBlocks: bernoulli_lpmf, binomial_lpmf, log_sum_exp, logit, binomial_logit_lpmf, bernoulli_logit_lpmf, inv_logit, log_inv_logit, rep_vector, square, normal_lpdf, sd, multi_normal_lpdf, student_t_lpdf, gp_exp_quad_cov, log_mix, append_row, append_col, pow, diag_matrix, normal_id_glm_lpdf, rep_matrix, rep_row_vector, cholesky_decompose, dot_self, cumulative_sum, softmax, log1m_inv_logit, matrix_constrain, integrate_ode_rk45, integrate_ode_bdf, poisson_lpdf, nothrow_log, categorical_lpmf, dirichlet_lpdf, exponential_lpdf, sub_col, stan_tail, dot_product, segment, inv_gamma_lpdf, diag_pre_multiply, multi_normal_cholesky_lpdf, logistic_lpdf\nusing Statistics, LinearAlgebra\n\n@inline PosteriorDB.implementation(model::PosteriorDB.Model, ::Val{:stan_blocks}) = julia_implementation(Val(Symbol(PosteriorDB.name(model))))\n# @inline StanBlocks.stan_implementation(posterior::PosteriorDB.Posterior) = StanProblem(\n#     PosteriorDB.path(PosteriorDB.implementation(PosteriorDB.model(posterior), \"stan\")), \n#     PosteriorDB.load(PosteriorDB.dataset(posterior), String);\n#     nan_on_error=true\n# )\n\njulia_implementation(posterior::PosteriorDB.Posterior) = julia_implementation(\n    Val(Symbol(PosteriorDB.name(PosteriorDB.model(posterior))));\n    Dict([Symbol(k)=&gt;v for (k, v) in pairs(PosteriorDB.load(PosteriorDB.dataset(posterior)))])...\n)\n\njulia_implementation(::Val{:earn_height}; N, earn, height, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0.)\n        end\n        @model begin\n            earn ~ normal(@broadcasted(beta[1] + beta[2] * height), sigma);\n        end\n    end\nend\n\njulia_implementation(::Val{:wells_dist}; N, switched, dist, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n        end\n        @model begin\n            switched ~ bernoulli_logit(@broadcasted(beta[1] + beta[2] * dist));\n        end\n    end\nend\njulia_implementation(::Val{:sesame_one_pred_a}; N, encouraged, watched, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0.)\n        end\n        @model begin\n            watched ~ normal(@broadcasted(beta[1] + beta[2] * encouraged), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:Rate_1_model}; n, k, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0.,upper=1.)\n        end\n        @model begin\n            theta ~ beta(1, 1)\n            k ~ binomial(n, theta)\n        end\n    end\nend\njulia_implementation(::Val{:nes_logit_model}; N, income, vote, kwargs...) = begin \n    X = reshape(income, (N, 1))\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[1]\n        end\n        @model begin\n            vote ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_momiq}; N, kid_score, mom_iq, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0)\n        end\n        @model begin\n            sigma ~ cauchy(0, 2.5);\n            kid_score ~ normal(@broadcasted(beta[1] + beta[2] * mom_iq), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_momhs}; N, kid_score, mom_hs, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0)\n        end\n        @model begin\n            sigma ~ cauchy(0, 2.5);\n            kid_score ~ normal(@broadcasted(beta[1] + beta[2] * mom_hs), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:logearn_height}; N, earn, height, kwargs...) = begin \n    log_earn = @. log(earn)\n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0)\n        end\n        @model begin\n            log_earn ~ normal(@broadcasted(beta[1] + beta[2] * height), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:blr}; N, D, X, y, kwargs...) = begin \n    @assert size(X) == (N,D)\n    @stan begin \n        @parameters begin\n            beta::vector[D]\n            sigma::real(lower=0)\n        end\n        @model begin\n            target += normal_lpdf(beta, 0, 10);\n            target += normal_lpdf(sigma, 0, 10);\n            target += normal_lpdf(y, X * beta, sigma);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dist100_model}; N, switched, dist, kwargs...) = begin \n    dist100 = @. dist / 100.\n    X = reshape(dist100, (N, 1))\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[1]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:Rate_3_model}; n1, n2, k1, k2, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0,upper=1)\n        end\n        @model begin\n            theta ~ beta(1, 1)\n            k1 ~ binomial(n1, theta)\n            k2 ~ binomial(n2, theta)\n        end\n    end\nend\njulia_implementation(::Val{:logearn_height_male}; N, earn, height, male, kwargs...) = begin \n    log_earn = @. log(earn)\n    @stan begin \n        @parameters begin\n            beta::vector[3]\n            sigma::real(lower=0)\n        end\n        @model begin\n            log_earn ~ normal(@broadcasted(beta[1] + beta[2] * height + beta[3] * male), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_momhsiq}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[3]\n            sigma::real(lower=0)\n        end\n        @model begin\n            sigma ~ cauchy(0, 2.5);\n            kid_score ~ normal(@broadcasted(beta[1] + beta[2] * mom_hs + beta[3] * mom_iq), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:log10earn_height}; N, earn, height, kwargs...) = begin \n    log10_earn = @. log10(earn)\n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0)\n        end\n        @model begin\n            log10_earn ~ normal(@broadcasted(beta[1] + beta[2] * height), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dist100ars_model}; N, switched, dist, arsenic, kwargs...) = begin \n    dist100 = @. dist / 100.\n    X = hcat(dist100, arsenic)\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[2]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:low_dim_gauss_mix_collapse}; N, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::vector[2]\n            sigma::vector(lower=0)[2]\n            theta::real(lower=0,upper=1)\n        end\n        @model begin\n            sigma ~ normal(0, 2);\n            mu ~ normal(0, 2);\n            theta ~ beta(5, 5);\n            for n in 1:N\n              target += log_mix(theta, normal_lpdf(y[n], mu[1], sigma[1]),\n                                normal_lpdf(y[n], mu[2], sigma[2]));\n            end\n        end\n    end\nend\njulia_implementation(::Val{:normal_mixture_k}; K, N, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::simplex[K]\n            mu::vector[K]\n            sigma::vector(lower=0.,upper=10.)[K]\n        end\n        @model begin\n            mu ~ normal(0., 10.);\n            for n in 1:N\n                ps = @broadcasted(log(theta) + normal_lpdf(y[n], mu, sigma))\n                target += log_sum_exp(ps);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:low_dim_gauss_mix}; N, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::ordered[2]\n            sigma::vector(lower=0)[2]\n            theta::real(lower=0,upper=1)\n        end\n        @model begin\n            sigma ~ normal(0, 2);\n            mu ~ normal(0, 2);\n            theta ~ beta(5, 5);\n            for n in 1:N\n                target += log_mix(theta, normal_lpdf(y[n], mu[1], sigma[1]),\n                                normal_lpdf(y[n], mu[2], sigma[2]));\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_county}; N, J, county, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            a::vector[J]\n            mu_a::real\n            sigma_a::real(lower=0., upper=100.)\n            sigma_y::real(lower=0., upper=100.)\n        end\n        @model @views begin\n            # y_hat = a[county]\n            y_hat = StanBlocks.constview(a, county)\n            \n            mu_a ~ normal(0., 1.);\n            a ~ normal(mu_a, sigma_a);\n            y ~ normal(y_hat, sigma_y);\n        end\n    end\nend\njulia_implementation(::Val{:logearn_logheight_male}; N, earn, height, male, kwargs...) = begin \n    log_earn = @. log(earn)\n    log_height = @. log(height)\n    @stan begin \n        @parameters begin\n            beta::vector[3]\n            sigma::real(lower=0)\n        end\n        @model begin\n            log_earn ~ normal(@broadcasted(beta[1] + beta[2] * log_height + beta[3] * male), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dae_model}; N, switched, dist, arsenic, educ, kwargs...) = begin \n    dist100 = @. dist / 100.\n    educ4 = @. educ / 4.\n    X = hcat(dist100, arsenic, educ4)\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[3]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:arK}; K, T, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[K]\n            sigma::real(lower=0)\n        end\n        @model begin\n            alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            sigma ~ cauchy(0, 2.5);\n            for t in K+1:T\n                mu = alpha\n                for k in 1:K\n                    mu += beta[k] * y[t-k]\n                end\n                y[t] ~ normal(mu, sigma)\n            end\n        end\n    end\nend\njulia_implementation(::Val{:wells_interaction_model}; N, switched, dist, arsenic, kwargs...) = begin \n    dist100 = @. dist / 100.\n    inter = @. dist100 * arsenic\n    X = hcat(dist100, arsenic, inter)\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[3]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:radon_pooled}; N, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::real\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_y ~ normal(0, 1);\n            alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n\n            log_radon ~ normal(@broadcasted(alpha + beta * floor_measure), sigma_y)\n        end\n    end\nend\njulia_implementation(::Val{:logearn_interaction}; N, earn, height, male, kwargs...) = begin \n        log_earn = @. log(earn)\n        inter = @. height * male\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_earn ~ normal(@broadcasted(beta[1] + beta[2] * height + beta[3] * male + beta[4] * inter), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:logmesquite_logvolume}; N, weight, diam1, diam2, canopy_height, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_canopy_volume = @. log(diam1 * diam2 * canopy_height);\n@stan begin \n            @parameters begin\n                beta::vector[2]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_canopy_volume), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:garch11}; T, y, sigma1, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::real\n            alpha0::real(lower=0.)\n            alpha1::real(lower=0., upper=1.)\n            beta1::real(lower=0., upper=1. - alpha1)\n        end\n        @model begin\n            sigma = sigma1\n            y[1] ~ normal(mu, sigma)\n            for t in 2:T\n                sigma = sqrt(alpha0 + alpha1 * square(y[t - 1] - mu) + beta1 * square(sigma))\n                y[t] ~ normal(mu, sigma)\n            end\n        end\n    end\nend\njulia_implementation(::Val{:eight_schools_centered}; J, y, sigma, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::vector[J]\n            mu::real\n            tau::real(lower=0)\n        end\n        @model begin\n            tau ~ cauchy(0, 5);\n            theta ~ normal(mu, tau);\n            y ~ normal(theta, sigma);\n            mu ~ normal(0, 5);\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_interaction}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n        inter = @. mom_hs * mom_iq;\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                sigma ~ cauchy(0, 2.5);\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * mom_hs + beta[3] * mom_iq + beta[4] * inter), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:mesquite}; N, weight, diam1, diam2, canopy_height, total_height, density, group, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[7]\n            sigma::real(lower=0)\n        end\n        @model begin\n            weight ~ normal(@broadcasted(beta[1] + beta[2] * diam1 + beta[3] * diam2\n            + beta[4] * canopy_height + beta[5] * total_height\n            + beta[6] * density + beta[7] * group), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:gp_regr}; N, x, y, kwargs...) = begin \n@stan begin \n            @parameters begin\n                rho::real(lower=0)\n                alpha::real(lower=0)\n                sigma::real(lower=0)\n            end\n            @model begin\n                cov = gp_exp_quad_cov(x, alpha, rho) + diag_matrix(rep_vector(sigma, N));\n                # L_cov = cholesky_decompose(cov);\n  \n                rho ~ gamma(25, 4);\n                alpha ~ normal(0, 2);\n                sigma ~ normal(0, 1);\n                \n                y ~ multi_normal(rep_vector(0., N), cov);\n                # Think about how to do this\n                # y ~ multi_normal_cholesky(rep_vector(0, N), L_cov);\n            end\n    end\nend\njulia_implementation(::Val{:kidscore_mom_work}; N, kid_score, mom_work, kwargs...) = begin \n        work2 = @. Float64(mom_work == 2)\n        work3 = @. Float64(mom_work == 3)\n        work4 = @. Float64(mom_work == 4)\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * work2 + beta[3] * work3\n                + beta[4] * work4), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:Rate_2_model}; n1, n2, k1, k2, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta1::real(lower=0,upper=1)\n            theta2::real(lower=0,upper=1)\n        end\n        delta = theta1 - theta2\n        @model begin\n            theta1 ~ beta(1, 1)\n            theta2 ~ beta(1, 1)\n            k1 ~ binomial(n1, theta1)\n            k2 ~ binomial(n2, theta2)\n        end\n    end\nend\njulia_implementation(::Val{:wells_interaction_c_model}; N, switched, dist, arsenic, kwargs...) = begin \n        c_dist100 = @. (dist - $mean(dist)) / 100.0;\n        c_arsenic = @. arsenic - $mean(arsenic);\n        inter = @. c_dist100 * c_arsenic\n        X = hcat(c_dist100, c_arsenic, inter)\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::vector[3]\n            end\n            @model begin\n                switched ~ bernoulli_logit_glm(X, alpha, beta);\n            end\n        end\nend\njulia_implementation(::Val{:radon_county_intercept}; N, J, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[J]\n            beta::real\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_y ~ normal(0, 1);\n            alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            for n in 1:N\n                mu = alpha[county_idx[n]] + beta * floor_measure[n];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_interaction_c}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n        c_mom_hs = @. mom_hs - $mean(mom_hs);\n        c_mom_iq = @. mom_iq - $mean(mom_iq);\n        inter = @. c_mom_hs * c_mom_iq;\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * c_mom_hs + beta[3] * c_mom_iq + beta[4] * inter), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:kidscore_interaction_c2}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n        c_mom_hs = @. mom_hs - .5;\n        c_mom_iq = @. mom_iq - 100;\n        inter = @. c_mom_hs * c_mom_iq;\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * c_mom_hs + beta[3] * c_mom_iq + beta[4] * inter), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:gp_pois_regr}; N, x, k, kwargs...) = begin \n        nugget = diag_matrix(rep_vector(1e-10, N))\n@stan begin \n            @parameters begin\n                rho::real(lower=0)\n                alpha::real(lower=0)\n                f_tilde::vector[N]\n            end\n            @transformed_parameters begin \n                cov = gp_exp_quad_cov(x, alpha, rho)\n                cov .+= nugget;\n                L_cov = cholesky_decompose(cov);\n                f = L_cov * f_tilde;\n            end\n            @model begin\n                rho ~ gamma(25, 4);\n                alpha ~ normal(0, 2);\n                f_tilde ~ normal(0, 1);\n                \n                k ~ poisson_log(f);\n            end\n    end\nend\njulia_implementation(::Val{:surgical_model}; N, r, n, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::real\n            sigmasq::real(lower=0)\n            b::vector[N]\n        end\n        @transformed_parameters begin \n            sigma = sqrt(sigmasq)\n            p = @broadcasted inv_logit(b)\n        end\n        @model begin\n            mu ~ normal(0.0, 1000.0);\n            sigmasq ~ inv_gamma(0.001, 0.001);\n            b ~ normal(mu, sigma);\n            r ~ binomial_logit(n, b);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dae_c_model}; N, switched, dist, arsenic, educ, kwargs...) = begin \n    c_dist100 = @. (dist - $mean(dist)) / 100.0;\n    c_arsenic = @. arsenic - $mean(arsenic);\n    da_inter = @. c_dist100 * c_arsenic;\n    educ4 = @. educ / 4.\n    X = hcat(c_dist100, c_arsenic, da_inter, educ4)\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[4]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:Rate_4_model}; n, k, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0,upper=1)\n            thetaprior::real(lower=0,upper=1)\n        end\n        @model begin\n            theta ~ beta(1, 1);\n            thetaprior ~ beta(1, 1);\n            k ~ binomial(n, theta);\n        end\n    end\nend\njulia_implementation(::Val{:logearn_interaction_z}; N, earn, height, male, kwargs...) = begin \n        log_earn = @. log(earn)\n        z_height = @. (height - $mean(height)) / $sd(height);\n        inter = @. z_height * male\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_earn ~ normal(@broadcasted(beta[1] + beta[2] * z_height + beta[3] * male + beta[4] * inter), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:normal_mixture}; N, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0,upper=1)\n            mu::vector[2]\n        end\n        @model begin\n            theta ~ uniform(0, 1); \n            for k in 1:2\n                mu[k] ~ normal(0, 10);\n            end\n            for n in 1:N\n                target += log_mix(theta, normal_lpdf(y[n], mu[1], 1.0),\n                                normal_lpdf(y[n], mu[2], 1.0));\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_partially_pooled_centered}; N, J, county_idx, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[J]\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n\n            alpha ~ normal(mu_alpha, sigma_alpha);\n            for n in 1:N\n                mu = alpha[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_interaction_z}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n        c_mom_hs = @. (mom_hs - $mean(mom_hs)) / (2 * $sd(mom_hs));\n        c_mom_iq = @. (mom_iq - $mean(mom_iq)) / (2 * $sd(mom_iq));\n        inter = @. c_mom_hs * c_mom_iq;\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * c_mom_hs + beta[3] * c_mom_iq + beta[4] * inter), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:kilpisjarvi}; N, x, y, xpred, pmualpha, psalpha, pmubeta, psbeta, kwargs...) = begin \n        x_ = x\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::real\n                sigma::real(lower=0)\n            end\n            @model begin\n                alpha ~ normal(pmualpha, psalpha);\n                beta ~ normal(pmubeta, psbeta);\n                y ~ normal(@broadcasted(alpha + beta * x_), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:wells_daae_c_model}; N, switched, dist, arsenic, assoc, educ, kwargs...) = begin \n        c_dist100 = @. (dist - $mean(dist)) / 100.0;\n        c_arsenic = @. arsenic - $mean(arsenic);\n        da_inter = @. c_dist100 * c_arsenic;\n        educ4 = @. educ / 4.\n        X = hcat(c_dist100, c_arsenic, da_inter, assoc, educ4)\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::vector[5]\n            end\n            @model begin\n                switched ~ bernoulli_logit_glm(X, alpha, beta);\n            end\n    end\nend\njulia_implementation(::Val{:eight_schools_noncentered}; J, y, sigma, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta_trans::vector[J]\n            mu::real\n            tau::real(lower=0)\n        end\n        theta = @broadcasted(theta_trans * tau + mu);\n        @model begin\n            theta_trans ~ normal(0, 1);\n            y ~ normal(theta, sigma);\n            mu ~ normal(0, 5);\n            tau ~ cauchy(0, 5);\n        end\n    end\nend\njulia_implementation(::Val{:Rate_5_model}; n1, n2, k1, k2, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0,upper=1)\n        end\n        @model begin\n            theta ~ beta(1, 1);\n            k1 ~ binomial(n1, theta);\n            k2 ~ binomial(n2, theta);\n        end\n    end\nend\njulia_implementation(::Val{:dugongs_model}; N, x, Y, kwargs...) = begin \n        x_ = x\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::real\n                lambda::real(lower=.5,upper=1.)\n                tau::real(lower=0.)\n            end\n            @transformed_parameters begin\n                sigma = 1. / sqrt(tau);\n                U3 = logit(lambda);\n            end\n            @model begin\n                for i in 1:N\n                    m = alpha - beta * pow(lambda, x_[i]);\n                    Y[i] ~ normal(m, sigma);\n                end\n                \n                alpha ~ normal(0.0, 1000.);\n                beta ~ normal(0.0, 1000.);\n                lambda ~ uniform(.5, 1.);\n                tau ~ gamma(.0001, .0001);\n            end\n    end\nend\njulia_implementation(::Val{:irt_2pl}; I, J, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            sigma_theta::real(lower=0);\n            theta::vector[J];\n\n            sigma_a::real(lower=0);\n            a::vector(lower=0)[I];\n\n            mu_b::real;\n            sigma_b::real(lower=0);\n            b::vector[I];\n        end\n        @model begin\n            sigma_theta ~ cauchy(0, 2);\n            theta ~ normal(0, sigma_theta);\n            \n            sigma_a ~ cauchy(0, 2);\n            a ~ lognormal(0, sigma_a);\n            \n            mu_b ~ normal(0, 5);\n            sigma_b ~ cauchy(0, 2);\n            b ~ normal(mu_b, sigma_b);\n            \n            for i in 1:I\n                y[i,:] ~ bernoulli_logit(@broadcasted(a[i] * (theta - b[i])));\n            end\n        end\n    end\nend\njulia_implementation(::Val{:logmesquite_logva}; N, weight, diam1, diam2, canopy_height, group, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_canopy_volume = @. log(diam1 * diam2 * canopy_height);\n        log_canopy_area = @. log(diam1 * diam2)\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_canopy_volume\n                + beta[3] * log_canopy_area + beta[4] * group), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:radon_variable_slope_centered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[J]\n            mu_beta::real\n            sigma_beta::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            alpha ~ normal(0, 10);\n            sigma_y ~ normal(0, 1);\n            sigma_beta ~ normal(0, 1);\n            mu_beta ~ normal(0, 10);\n            \n            beta ~ normal(mu_beta, sigma_beta);\n            for n in 1:N\n                mu = alpha + floor_measure[n] * beta[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_variable_intercept_centered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[J]\n            beta::real\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            \n            alpha ~ normal(mu_alpha, sigma_alpha);\n            for n in 1:N\n                mu = alpha[county_idx[n]] + floor_measure[n] * beta;\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:seeds_stanified_model}; I, n, N, x1, x2, kwargs...) = begin \n        x1x2 = @. x1 * x2;\n@stan begin \n            @parameters begin\n                alpha0::real;\n                alpha1::real;\n                alpha12::real;\n                alpha2::real;\n                b::vector[I];\n                sigma::real(lower=0);\n            end\n            @model begin\n                alpha0 ~ normal(0.0, 1.0);\n                alpha1 ~ normal(0.0, 1.0);\n                alpha2 ~ normal(0.0, 1.0);\n                alpha12 ~ normal(0.0, 1.0);\n                sigma ~ cauchy(0, 1);\n                \n                b ~ normal(0.0, sigma);\n                n ~ binomial_logit(N,\n                                   @broadcasted(alpha0 + alpha1 * x1 + alpha2 * x2 + alpha12 * x1x2 + b));\n            end\n    end\nend\njulia_implementation(::Val{:state_space_stochastic_level_stochastic_seasonal}; n, y, x, w, kwargs...) = begin \n        x_ = x\n        mu_lower = mean(y) - 3 * sd(y)\n        mu_upper = mean(y) + 3 * sd(y)\n@stan begin \n            @parameters begin\n                mu::vector(lower=mu_lower, upper=mu_upper)[n]\n                seasonal::vector[n]\n                beta::real\n                lambda::real\n                sigma::positive_ordered[3]\n            end\n            @model @views begin\n                for t in 12:n\n                    seasonal[t] ~ normal(-sum(seasonal[t-11:t-1]), sigma[1]);\n                end\n                \n                for t in 2:n\n                    mu[t] ~ normal(mu[t - 1], sigma[2]);\n                end\n                \n                y ~ normal(@broadcasted(mu + beta * x_ + lambda * w + seasonal), sigma[3]);\n                \n                sigma ~ student_t(4, 0, 1);\n            end\n        end\nend\njulia_implementation(::Val{:radon_partially_pooled_noncentered}; N, J, county_idx, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha_raw::vector[J]\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        alpha = @.(mu_alpha + sigma_alpha * alpha_raw);\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            alpha_raw ~ normal(0, 1);\n\n            for n in 1:N\n                mu = alpha[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:seeds_model}; I, n, N, x1, x2, kwargs...) = begin \n        x1x2 = @. x1 * x2;\n@stan begin \n            @parameters begin\n                alpha0::real;\n                alpha1::real;\n                alpha12::real;\n                alpha2::real;\n                tau::real(lower=0);\n                b::vector[I];\n            end\n            sigma = 1.0 / sqrt(tau);\n            @model begin\n                alpha0 ~ normal(0.0, 1.0E3);\n                alpha1 ~ normal(0.0, 1.0E3);\n                alpha2 ~ normal(0.0, 1.0E3);\n                alpha12 ~ normal(0.0, 1.0E3);\n                tau ~ gamma(1.0E-3, 1.0E-3);\n                \n                b ~ normal(0.0, sigma);\n                n ~ binomial_logit(N,\n                                   @broadcasted(alpha0 + alpha1 * x1 + alpha2 * x2 + alpha12 * x1x2 + b));\n            end\n        end\nend\njulia_implementation(::Val{:arma11}; T, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::real\n            phi::real\n            theta::real\n            sigma::real(lower=0)\n        end\n        @model begin\n            mu ~ normal(0, 10);\n            phi ~ normal(0, 2);\n            theta ~ normal(0, 2);\n            sigma ~ cauchy(0, 2.5);\n            nu = mu + phi * mu\n            err = y[1] - nu\n            err ~ normal(0, sigma);\n            for t in 2:T\n                nu = mu + phi * y[t-1] + theta * err\n                err = y[t] - nu\n                err ~ normal(0, sigma);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_hierarchical_intercept_centered}; J, N, county_idx, log_uppm, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[J]\n            beta::vector[2]\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_alpha ~ normal(0, 1);\n            sigma_y ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            \n            alpha ~ normal(mu_alpha, sigma_alpha);\n            for n in 1:N\n                muj = alpha[county_idx[n]] + log_uppm[n] * beta[1]\n                mu = muj + floor_measure[n] * beta[2];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:seeds_centered_model}; I, n, N, x1, x2, kwargs...) = begin \n        x1x2 = @. x1 * x2;\n@stan begin \n            @parameters begin\n                alpha0::real;\n                alpha1::real;\n                alpha12::real;\n                alpha2::real;\n                c::vector[I];\n                sigma::real(lower=0);\n            end\n            b = @. c - $mean(c);\n            @model begin\n                alpha0 ~ normal(0.0, 1.0);\n                alpha1 ~ normal(0.0, 1.0);\n                alpha2 ~ normal(0.0, 1.0);\n                alpha12 ~ normal(0.0, 1.0);\n                sigma ~ cauchy(0, 1);\n                \n                c ~ normal(0.0, sigma);\n                n ~ binomial_logit(N,\n                                   @broadcasted(alpha0 + alpha1 * x1 + alpha2 * x2 + alpha12 * x1x2 + b));\n            end\n    end\nend\njulia_implementation(::Val{:pilots}; N, n_groups, n_scenarios, group_id, scenario_id, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            a::vector[n_groups];\n            b::vector[n_scenarios];\n            mu_a::real;\n            mu_b::real;\n            sigma_a::real(lower=0, upper=100);\n            sigma_b::real(lower=0, upper=100);\n            sigma_y::real(lower=0, upper=100);\n        end\n        y_hat = @broadcasted(a[group_id] + b[scenario_id]);\n        @model begin\n            mu_a ~ normal(0, 1);\n            a ~ normal(10 * mu_a, sigma_a);\n            \n            mu_b ~ normal(0, 1);\n            b ~ normal(10 * mu_b, sigma_b);\n            \n            y ~ normal(y_hat, sigma_y);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dae_inter_model}; N, switched, dist, arsenic, educ, kwargs...) = begin \n        c_dist100 = @. (dist - $mean(dist)) / 100.0;\n        c_arsenic = @. arsenic - $mean(arsenic);\n        c_educ4 = @. (educ - $mean(educ)) / 4.\n        da_inter = @. c_dist100 * c_arsenic;\n        de_inter = @. c_dist100 * c_educ4;\n        ae_inter = @. c_arsenic * c_educ4;\n        X = hcat(c_dist100, c_arsenic, c_educ4, da_inter, de_inter, ae_inter, )\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::vector[6]\n            end\n            @model begin\n                switched ~ bernoulli_logit_glm(X, alpha, beta);\n            end\n    end\nend\njulia_implementation(::Val{:radon_variable_slope_noncentered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::real\n            beta_raw::vector[J]\n            mu_beta::real\n            sigma_beta::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        beta = @. mu_beta + sigma_beta * beta_raw;\n        @model begin\n            alpha ~ normal(0, 10);\n            sigma_y ~ normal(0, 1);\n            sigma_beta ~ normal(0, 1);\n            mu_beta ~ normal(0, 10);\n            beta_raw ~ normal(0, 1);\n\n            for n in 1:N\n                mu = alpha + floor_measure[n] * beta[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_variable_intercept_slope_centered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n@stan begin \n            @parameters begin\n                sigma_y::real(lower=0)\n                sigma_alpha::real(lower=0)\n                sigma_beta::real(lower=0)\n                alpha::vector[J]\n                beta::vector[J]\n                mu_alpha::real\n                mu_beta::real\n            end\n            @model begin\n                sigma_y ~ normal(0, 1);\n                sigma_beta ~ normal(0, 1);\n                sigma_alpha ~ normal(0, 1);\n                mu_alpha ~ normal(0, 10);\n                mu_beta ~ normal(0, 10);\n                \n                alpha ~ normal(mu_alpha, sigma_alpha);\n                beta ~ normal(mu_beta, sigma_beta);\n                for n in 1:N\n                    mu = alpha[county_idx[n]] + floor_measure[n] * beta[county_idx[n]];\n                    target += normal_lpdf(log_radon[n], mu, sigma_y);\n                end\n            end\n        end\nend\njulia_implementation(::Val{:radon_variable_intercept_noncentered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha_raw::vector[J]\n            beta::real\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        alpha = @. mu_alpha + sigma_alpha * alpha_raw;\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            alpha_raw ~ normal(0, 1);\n\n            for n in 1:N\n                mu = alpha[county_idx[n]] + floor_measure[n] * beta;\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:GLM_Poisson_model}; n, C, year, kwargs...) = begin \n        year_squared = year .^ 2\n        year_cubed = year .^ 3\n@stan begin \n            @parameters begin\n                alpha::real(lower=-20, upper=+20)\n                beta1::real(lower=-10, upper=+10)\n                beta2::real(lower=-10, upper=+10)\n                beta3::real(lower=-10, upper=+10)\n            end\n            log_lambda = @broadcasted(alpha + beta1 * year + beta2 * year_squared + beta3 * year_cubed)\n            @model begin\n                C ~ poisson_log(log_lambda);\n            end\n    end\nend\njulia_implementation(::Val{:ldaK5}; V, M, N, w, doc, alpha, beta, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::simplex[M,5];\n            phi::simplex[5,V];\n        end\n        @model @views begin\n            for m in 1:M\n                theta[m,:] ~ dirichlet(alpha);\n            end\n            for k in 1:5\n                phi[k,:] ~ dirichlet(beta);\n            end\n            for n in 1:N\n                gamma = @broadcasted(log(theta[doc[n], :]) + log(phi[:, w[n]]))\n                target += log_sum_exp(gamma);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:dogs}; n_dogs, n_trials, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[3];\n        end\n        @model begin\n            beta ~ normal(0, 100);\n            for i in 1:n_dogs\n                n_avoid = 0\n                n_shock = 0\n                for j in 1:n_trials\n                    p = beta[1] + beta[2] * n_avoid + beta[3] * n_shock\n                    y[i, j] ~ bernoulli_logit(p);\n                    n_avoid += 1 - y[i,j]\n                    n_shock += y[i,j]\n                end\n            end\n        end\n    end\nend\njulia_implementation(::Val{:nes}; N, partyid7, real_ideo, race_adj, educ1, gender, income, age_discrete, kwargs...) = begin \n        age30_44 = @. Float64(age_discrete == 2);\n        age45_64 = @. Float64(age_discrete == 3);\n        age65up = @. Float64(age_discrete == 4);\n@stan begin \n            @parameters begin\n                beta::vector[9]\n                sigma::real(lower=0)\n            end\n            @model begin\n                partyid7 ~ normal(@broadcasted(beta[1] + beta[2] * real_ideo + beta[3] * race_adj\n                + beta[4] * age30_44 + beta[5] * age45_64\n                + beta[6] * age65up + beta[7] * educ1 + beta[8] * gender\n                + beta[9] * income), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:dogs_log}; n_dogs, n_trials, y, kwargs...) = begin \n    @assert size(y) == (n_dogs, n_trials)\n@stan begin \n        @parameters begin\n            beta::vector[2];\n        end\n        @model begin\n            beta[1] ~ uniform(-100, 0);\n            beta[2] ~ uniform(0, 100);\n            for i in 1:n_dogs\n                n_avoid = 0\n                n_shock = 0\n                for j in 1:n_trials\n                    p = inv_logit(beta[1] * n_avoid + beta[2] * n_shock)\n                    y[i, j] ~ bernoulli(p);\n                    n_avoid += 1 - y[i,j]\n                    n_shock += y[i,j]\n                end\n            end\n        end\n    end\nend\njulia_implementation(::Val{:GLM_Binomial_model};nyears, C, N, year, kwargs...) = begin \n        year_squared = year .^ 2\n@stan begin \n            @parameters begin\n                alpha::real\n                beta1::real\n                beta2::real\n            end\n            logit_p = @broadcasted alpha + beta1 * year + beta2 * year_squared;\n            @model begin\n                alpha ~ normal(0, 100)\n                beta1 ~ normal(0, 100)\n                beta2 ~ normal(0, 100)\n                C ~ binomial_logit(N, logit_p)\n            end\n    end\nend\njulia_implementation(::Val{:radon_hierarchical_intercept_noncentered}; J, N, county_idx, log_uppm, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha_raw::vector[J]\n            beta::vector[2]\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        alpha = @. mu_alpha + sigma_alpha * alpha_raw;\n        @model begin\n            sigma_alpha ~ normal(0, 1);\n            sigma_y ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            alpha_raw ~ normal(0, 1);\n\n            for n in 1:N\n                muj = alpha[county_idx[n]] + log_uppm[n] * beta[1]\n                mu = muj + floor_measure[n] * beta[2];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:logmesquite_logvash}; N, weight, diam1, diam2, canopy_height, total_height, group, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_canopy_volume = @. log(diam1 * diam2 * canopy_height);\n        log_canopy_area = @. log(diam1 * diam2)\n        log_canopy_shape = @. log(diam1 / diam2);\n        log_total_height = @. log(total_height);\n@stan begin \n            @parameters begin\n                beta::vector[6]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_canopy_volume\n                + beta[3] * log_canopy_area\n                + beta[4] * log_canopy_shape\n                + beta[5] * log_total_height + beta[6] * group), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:ldaK2}; V, M, N, w, doc, kwargs...) = begin \n        K = 2\n        alpha = fill(1, K)\n        beta = fill(1, V)\n@stan begin \n            @parameters begin\n                theta::simplex[M,K];\n                phi::simplex[K,V];\n            end\n            @model @views begin\n                for m in 1:M\n                  theta[m,:] ~ dirichlet(alpha);\n                end\n                for k in 1:K\n                  phi[k,:] ~ dirichlet(beta);\n                end\n                for n in 1:N\n                    gamma = @broadcasted log(theta[doc[n], :]) + log(phi[:, w[n]])\n                  target += log_sum_exp(gamma);\n                end\n            end\n    end\nend\njulia_implementation(::Val{:logmesquite}; N, weight, diam1, diam2, canopy_height, total_height, density, group, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_diam1 = @. log(diam1);\n        log_diam2 = @. log(diam2);\n        log_canopy_height = @. log(canopy_height);\n        log_total_height = @. log(total_height);\n        log_density = @. log(density);\n@stan begin \n            @parameters begin\n                beta::vector[7]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_diam1 + beta[3] * log_diam2\n                + beta[4] * log_canopy_height\n                + beta[5] * log_total_height + beta[6] * log_density\n                + beta[7] * group), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:rats_model}; N, Npts, rat, x, y, xbar, kwargs...) = begin \n        x_ = x\n@stan begin \n            @parameters begin\n                alpha::vector[N];\n                beta::vector[N];\n                \n                mu_alpha::real;\n                mu_beta::real;\n                sigma_y::real(lower=0);\n                sigma_alpha::real(lower=0);\n                sigma_beta::real(lower=0);\n            end\n            @model begin\n                mu_alpha ~ normal(0, 100);\n                mu_beta ~ normal(0, 100);\n                alpha ~ normal(mu_alpha, sigma_alpha);\n                beta ~ normal(mu_beta, sigma_beta);\n                for n in 1:Npts\n                  irat = rat[n];\n                  y[n] ~ normal(alpha[irat] + beta[irat] * (x_[n] - xbar), sigma_y);\n                end\n            end\n    end\nend\njulia_implementation(::Val{:logmesquite_logvas}; N, weight, diam1, diam2, canopy_height, total_height, density, group, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_canopy_volume = @. log(diam1 * diam2 * canopy_height);\n        log_canopy_area = @. log(diam1 * diam2)\n        log_canopy_shape = @. log(diam1 / diam2);\n        log_total_height = @. log(total_height);\n        log_density = @. log(density);\n@stan begin \n            @parameters begin\n                beta::vector[7]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_canopy_volume\n                + beta[3] * log_canopy_area\n                + beta[4] * log_canopy_shape\n                + beta[5] * log_total_height + beta[6] * log_density\n                + beta[7] * group), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:lsat_model}; N, R, T, culm, response, kwargs...) = begin \n    r = zeros(Int64, (T, N))\n    for j in 1:culm[1], k in 1:T\n        r[k, j] = response[1, k];\n    end\n    for i in 2:R\n        for j in (culm[i-1]+1):culm[i], k in 1:T\n            r[k, j] = response[i, k];\n        end\n    end\n    ones = fill(1., N)\n    @stan begin \n        @parameters begin\n            alpha::vector[T];\n            theta::vector[N];\n            beta::real(lower=0);\n        end\n        @model @views begin\n            alpha ~ normal(0, 100.);\n            theta ~ normal(0, 1);\n            beta ~ normal(0.0, 100.);\n            for k in 1:T\n                r[k,:] ~ bernoulli_logit(beta * theta - alpha[k] * ones);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_variable_intercept_slope_noncentered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            sigma_y::real(lower=0)\n            sigma_alpha::real(lower=0)\n            sigma_beta::real(lower=0)\n            alpha_raw::vector[J]\n            beta_raw::vector[J]\n            mu_alpha::real\n            mu_beta::real\n        end\n        alpha = @. mu_alpha + sigma_alpha * alpha_raw;\n        beta = @. mu_beta + sigma_beta * beta_raw;\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_beta ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            mu_beta ~ normal(0, 10);\n            alpha_raw ~ normal(0, 1);\n            beta_raw ~ normal(0, 1);\n\n            for n in 1:N\n                mu = alpha[county_idx[n]] + floor_measure[n] * beta[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:GLMM_Poisson_model};n, C, year) = begin \n    year_squared = year .^ 2\n    year_cubed = year .^ 3\n    @stan begin\n        @parameters begin\n            alpha::real(lower=-20, upper=+20)\n            beta1::real(lower=-10, upper=+10)\n            beta2::real(lower=-10, upper=+20)\n            beta3::real(lower=-10, upper=+10)\n            eps::vector[n]\n            sigma::real(lower=0, upper=5)\n        end\n        log_lambda = @broadcasted alpha + beta1 * year + beta2 * year_squared + beta3 * year_cubed + eps\n        @model begin\n            C ~ poisson_log(log_lambda)\n            eps ~ normal(0, sigma)\n        end\n    end\nend\njulia_implementation(::Val{:GLMM1_model};nsite, nobs, obs, obsyear, obssite, misyear, missite, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[nsite]\n            mu_alpha::real\n            sd_alpha::real(lower=0,upper=5)\n        end\n    #   log_lambda = rep_matrix(alpha', nyear);\n        @model begin\n            alpha ~ normal(mu_alpha, sd_alpha)\n            mu_alpha ~ normal(0, 10)\n            for i in 1:nobs\n                # obs[i] ~ poisson_log(log_lambda[obsyear[i], obssite[i]])\n                obs[i] ~ poisson_log(alpha[obssite[i]])\n            end\n        end\n    end\nend\njulia_implementation(::Val{:hier_2pl}; I, J, N, ii, jj, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::vector[J];\n            xi1::vector[I];\n            xi2::vector[I];\n            mu::vector[2];\n            tau::vector(lower=0)[2];\n            L_Omega::cholesky_factor_corr[2]\n        end\n        xi = hcat(xi1, xi2)\n        alpha = @. exp(xi1);\n        beta = xi2;\n        @model begin\n            L_Sigma = diag_pre_multiply(tau, L_Omega);\n            for i in 1:I\n                target += multi_normal_cholesky_lpdf(xi[i, :], mu, L_Sigma);\n            end\n            theta ~ normal(0, 1);\n            L_Omega ~ lkj_corr_cholesky(4);\n            mu[1] ~ normal(0, 1);\n            tau[1] ~ exponential(.1);\n            mu[2] ~ normal(0, 5);\n            tau[2] ~ exponential(.1);\n            y ~ bernoulli_logit(alpha[ii] .* (theta[jj] - beta[ii]));\n        end\n    end\nend\njulia_implementation(::Val{:dogs_hierarchical}; n_dogs, n_trials, y, kwargs...) = begin \n        J = n_dogs;\n        T = n_trials;\n        prev_shock = zeros((J,T));\n        prev_avoid = zeros((J,T));\n        \n        for j in 1:J\n            prev_shock[j, 1] = 0;\n            prev_avoid[j, 1] = 0;\n            for t in 2:T\n                prev_shock[j, t] = prev_shock[j, t - 1] + y[j, t - 1];\n                prev_avoid[j, t] = prev_avoid[j, t - 1] + 1 - y[j, t - 1];\n            end\n        end\n@stan begin \n            @parameters begin\n                a::real(lower=0, upper=1);\n                b::real(lower=0, upper=1);\n            end\n            @model begin\n                y ~ bernoulli(@broadcasted(a ^ prev_shock * b ^ prev_avoid));\n            end\n    end\nend\n\n\njulia_implementation(::Val{:dogs_nonhierarchical}; n_dogs, n_trials, y, kwargs...) = begin \n    J = n_dogs;\n    T = n_trials;\n    prev_shock = zeros((J,T));\n    prev_avoid = zeros((J,T));\n    \n    for j in 1:J\n        prev_shock[j, 1] = 0;\n        prev_avoid[j, 1] = 0;\n        for t in 2:T\n            prev_shock[j, t] = prev_shock[j, t - 1] + y[j, t - 1];\n            prev_avoid[j, t] = prev_avoid[j, t - 1] + 1 - y[j, t - 1];\n        end\n    end\n    @stan begin \n        @parameters begin\n            mu_logit_ab::vector[2]\n            sigma_logit_ab::vector(lower=0)[2]\n            L_logit_ab::cholesky_factor_corr[2]\n            z::matrix[J, 2]\n        end\n        @model @views begin\n            logit_ab = rep_vector(1, J) * mu_logit_ab' + z * diag_pre_multiply(sigma_logit_ab, L_logit_ab);\n            a = inv_logit.(logit_ab[ : , 1]);\n            b = inv_logit.(logit_ab[ : , 2]);\n            y ~ bernoulli(@broadcasted(a ^ prev_shock * b ^ prev_avoid));\n            mu_logit_ab ~ logistic(0, 1);\n            sigma_logit_ab ~ normal(0, 1);\n            L_logit_ab ~ lkj_corr_cholesky(2);\n            (z) ~ normal(0, 1);\n        end\n    end\nend\njulia_implementation(::Val{:M0_model}; M, T, y, kwargs...) = begin\n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int64, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0,upper=1)\n                p::real(lower=0,upper=1)\n            end\n            @model begin\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + binomial_lpmf(s[i], T, p)\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + binomial_lpmf(0, T, p),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n    end\nend\njulia_implementation(::Val{:diamonds}; N, Y, K, X, prior_only, kwargs...) = begin\n        Kc = K - 1;\n        Xc = zeros((N, Kc))\n        means_X = zeros(Kc)\n        for i in 2:K\n            means_X[i - 1] = mean(X[ : , i]);\n            @. Xc[ : , i - 1] = X[ : , i] - means_X[i - 1];\n        end\n@stan begin \n            @parameters begin\n                b::vector[Kc];\n                Intercept::real;\n                sigma::real(lower=0.)\n            end\n            @model begin\n                target += normal_lpdf(b, 0., 1.);\n                target += student_t_lpdf(Intercept, 3., 8., 10.);\n                target += student_t_lpdf(sigma, 3., 0., 10.)# - 1 * student_t_lccdf(0, 3, 0, 10);\n                if !(prior_only == 1)\n                    target += normal_id_glm_lpdf(Y, Xc, Intercept, b, sigma);\n                end\n            end\n    end\nend\njulia_implementation(::Val{:Mt_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int64, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0,upper=1)\n                p::vector(lower=0,upper=1)[T]\n            end\n            @model @views begin\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + bernoulli_lpmf(y[i,:], p)\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + bernoulli_lpmf(y[i,:], p),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n    end\nend\njulia_implementation(::Val{:election88_full}; \n    N,\n    n_age,\n    n_age_edu,\n    n_edu,\n    n_region_full,\n    n_state,\n    age,\n    age_edu,\n    black,\n    edu,\n    female,\n    region_full,\n    state,\n    v_prev_full,\n    y,\n    kwargs...) = begin \n@stan begin \n            @parameters begin\n                a::vector[n_age];\n                b::vector[n_edu];\n                c::vector[n_age_edu];\n                d::vector[n_state];\n                e::vector[n_region_full];\n                beta::vector[5];\n                sigma_a::real(lower=0, upper=100);\n                sigma_b::real(lower=0, upper=100);\n                sigma_c::real(lower=0, upper=100);\n                sigma_d::real(lower=0, upper=100);\n                sigma_e::real(lower=0, upper=100);\n            end\n            @model @views begin\n                y_hat = @broadcasted (beta[1] + beta[2] * black + beta[3] * female\n                    + beta[5] * female * black + beta[4] * v_prev_full\n                    + a[age] + b[edu] + c[age_edu] + d[state]\n                    + e[region_full])\n                a ~ normal(0, sigma_a);\n                b ~ normal(0, sigma_b);\n                c ~ normal(0, sigma_c);\n                d ~ normal(0, sigma_d);\n                e ~ normal(0, sigma_e);\n                beta ~ normal(0, 100);\n                y ~ bernoulli_logit(y_hat);\n            end\n        end\nend\njulia_implementation(::Val{:nn_rbm1bJ10}; N, M, x, K, y, kwargs...) = begin \n            J = 10\n            nu_alpha = 0.5;\n            s2_0_alpha = (0.05 / M ^ (1 / nu_alpha)) ^ 2;\n            nu_beta = 0.5;\n            s2_0_beta = (0.05 / J ^ (1 / nu_beta)) ^ 2;\n            \n            ones = rep_vector(1., N);\n            x1 = append_col(ones, x);\n\n    return @stan begin end\n    @stan begin \n        @parameters begin\n            sigma2_alpha::real(lower=0);\n            sigma2_beta::real(lower=0);\n            alpha::matrix[M, J];\n            beta::matrix[J, K - 1];\n            alpha1::row_vector[J];\n            beta1::row_vector[K - 1];\n        end\n        @model @views begin\n            v = append_col(\n                ones,\n                append_col(\n                    ones,\n                    tanh.(x1 * append_row(alpha1, alpha))\n                ) * append_row(beta1, beta)\n            );\n            alpha1 ~ normal(0, 1);\n            beta1 ~ normal(0, 1);\n            sigma2_alpha ~ inv_gamma(nu_alpha / 2, nu_alpha * s2_0_alpha / 2);\n            sigma2_beta ~ inv_gamma(nu_beta / 2, nu_beta * s2_0_beta / 2);\n            \n            (alpha) ~ normal(0, sqrt(sigma2_alpha));\n            (beta) ~ normal(0, sqrt(sigma2_beta));\n            for n in 1:N\n                y[n] ~ categorical_logit(v[n, :]);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:nn_rbm1bJ100}; N, M, x, K, y, kwargs...) = begin \n    J = 100\n    nu_alpha = 0.5;\n    s2_0_alpha = (0.05 / M ^ (1 / nu_alpha)) ^ 2;\n    nu_beta = 0.5;\n    s2_0_beta = (0.05 / J ^ (1 / nu_beta)) ^ 2;\n    \n    ones = rep_vector(1., N);\n    x1 = append_col(ones, x);\n    return @stan begin end\n    @stan begin \n        @parameters begin\n            sigma2_alpha::real(lower=0);\n            sigma2_beta::real(lower=0);\n            alpha::matrix[M, J];\n            beta::matrix[J, K - 1];\n            alpha1::row_vector[J];\n            beta1::row_vector[K - 1];\n        end\n        @model @views begin\n            v = append_col(\n                ones,\n                append_col(\n                    ones,\n                    tanh.(x1 * append_row(alpha1, alpha))\n                ) * append_row(beta1, beta)\n            );\n            alpha1 ~ normal(0, 1);\n            beta1 ~ normal(0, 1);\n            sigma2_alpha ~ inv_gamma(nu_alpha / 2, nu_alpha * s2_0_alpha / 2);\n            sigma2_beta ~ inv_gamma(nu_beta / 2, nu_beta * s2_0_beta / 2);\n            \n            (alpha) ~ normal(0, sqrt(sigma2_alpha));\n            (beta) ~ normal(0, sqrt(sigma2_beta));\n            for n in 1:N\n                y[n] ~ categorical_logit(v[n, :]);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:Survey_model}; nmax, m, k) = begin \n    nmin = maximum(k)\n    @stan begin \n        @parameters begin \n            theta::real(lower=0, upper=1)\n        end\n        @model begin \n            target += log_sum_exp([\n                n &lt; nmin ? log(1.0 / nmax) - Inf : log(1.0 / nmax) + binomial_lpmf(k, n, theta)\n                for n in 1:nmax\n            ])\n        end\n    end\nend\n\njulia_implementation(::Val{:sir}; N_t, t, y0, stoi_hat, B_hat) = begin \n    y0 = collect(Float64, y0)\n    simple_SIR(t, y, theta, x_r, x_i)  = begin\n        dydt = zero(y)\n        \n        dydt[1] = -theta[1] * y[4] / (y[4] + theta[2]) * y[1];\n        dydt[2] = theta[1] * y[4] / (y[4] + theta[2]) * y[1] - theta[3] * y[2];\n        dydt[3] = theta[3] * y[2];\n        dydt[4] = theta[4] * y[2] - theta[5] * y[4];\n        \n        return dydt;\n    end\n    t0 = 0.\n    kappa = 1000000.\n    @stan begin \n        @parameters begin \n            beta::real(lower=0)\n            gamma::real(lower=0)\n            xi::real(lower=0)\n            delta::real(lower=0)\n        end\n        @model @views begin \n            y = integrate_ode_rk45(simple_SIR, y0, t0, t, [beta, kappa, gamma, xi, delta]);\n\n            beta ~ cauchy(0, 2.5);\n            gamma ~ cauchy(0, 1);\n            xi ~ cauchy(0, 25);\n            delta ~ cauchy(0, 1);\n\n            stoi_hat[1] ~ poisson(y0[1] - y[1, 1]);\n            for n in 2:N_t\n                stoi_hat[n] ~ poisson(max(1e-16, y[n - 1, 1] - y[n, 1]));\n            end\n            \n            B_hat ~ lognormal(@broadcasted(nothrow_log(y[:, 4])), 0.15);\n        \n        end\n    end\nend\n\njulia_implementation(::Val{:lotka_volterra}; N, ts, y_init, y) = begin \n    dz_dt(t, z, theta, x_r, x_i)  = begin\n        u, v = z\n        \n        alpha, beta, gamma, delta = theta\n\n        du_dt = (alpha - beta * v) * u\n        dv_dt = (-gamma +delta * u) * v\n        [du_dt, dv_dt]\n    end\n    @stan begin \n        @parameters begin \n            theta::real(lower=0)[4]\n            z_init::real(lower=0)[2]\n            sigma::real(lower=0)[2]\n        end\n        @model @views begin \n            z = integrate_ode_rk45(dz_dt, z_init, 0, ts, theta,\n                missing, missing, 1e-5, 1e-3, 5e2);\n\n            theta[[1,3]] ~ normal(1, 0.5);\n            theta[[2,4]] ~ normal(0.05, 0.05);\n            sigma ~ lognormal(-1, 1);\n            z_init ~ lognormal(log(10), 1);\n            y_init ~ lognormal(@broadcasted(log(z_init)), sigma);\n            y ~ lognormal(@broadcasted(nothrow_log(z)), sigma');\n        end\n    end\nend\n\njulia_implementation(::Val{:soil_incubation}; totalC_t0, t0, N_t, ts, eCO2mean, kwargs...) = begin \n    two_pool_feedback(t, C, theta, x_r, x_i)  = begin\n        k1, k2, alpha21, alpha12 = theta\n        [\n            -k1 * C[1] + alpha12 * k2 * C[2]\n            -k2 * C[2] + alpha21 * k1 * C[1]\n        ]\n    end\n    evolved_CO2(N_t, t0, ts, gamma, totalC_t0, k1, k2, alpha21, alpha12) = begin \n        C_t0 = [gamma * totalC_t0, (1 - gamma) * totalC_t0]\n        theta = k1, k2, alpha21, alpha12\n        C_hat = integrate_ode_rk45(two_pool_feedback, C_t0, t0, ts, theta)\n        totalC_t0 .- sum.(eachrow(C_hat))\n    end\n    @stan begin \n        @parameters begin \n            k1::real(lower=0)\n            k2::real(lower=0)\n            alpha21::real(lower=0)\n            alpha12::real(lower=0)\n            gamma::real(lower=0, upper=1)\n            sigma::real(lower=0)\n        end\n        @model @views begin \n            eCO2_hat = evolved_CO2(N_t, t0, ts, gamma, totalC_t0, k1, k2, alpha21, alpha12)\n            gamma ~ beta(10, 1); \n            k1 ~ normal(0, 1);\n            k2 ~ normal(0, 1);\n            alpha21 ~ normal(0, 1);\n            alpha12 ~ normal(0, 1);\n            sigma ~ cauchy(0, 1);\n            eCO2mean ~ normal(eCO2_hat, sigma);\n        end\n    end\nend\n\njulia_implementation(::Val{:one_comp_mm_elim_abs}; t0, D, V, N_t, times, C_hat) = begin \n    one_comp_mm_elim_abs(t, y, theta, x_r, x_i)  = begin\n        k_a, K_m, V_m = theta\n        D, V = x_r\n        dose = 0.\n        elim = (V_m / V) * y[1] / (K_m + y[1]);\n        if t &gt; 0\n            dose = exp(-k_a * t) * D * k_a / V;\n        end\n        [dose - elim]\n    end\n    C0 = [0.]\n    x_r = [D,V]\n    x_i = missing\n    @stan begin \n        @parameters begin \n            k_a::real(lower=0)\n            K_m::real(lower=0)\n            V_m::real(lower=0)\n            sigma::real(lower=0)\n        end\n        @model @views begin \n            theta = [k_a, K_m, V_m]\n            C = integrate_ode_bdf(one_comp_mm_elim_abs, C0, t0, times, theta, x_r, x_i)\n            k_a ~ cauchy(0, 1);\n            K_m ~ cauchy(0, 1);\n            V_m ~ cauchy(0, 1);\n            sigma ~ cauchy(0, 1);\n            \n            C_hat ~ lognormal(@broadcasted(nothrow_log(C[:, 1])), sigma);\n        end\n    end\nend\n\n\njulia_implementation(::Val{:bym2_offset_only}; N, N_edges, node1, node2, y, E, scaling_factor, kwargs...) = begin \n        log_E = @. log(E)\n@stan begin \n            @parameters begin\n                beta0::real;\n                sigma::real(lower=0);\n                rho::real(lower=0, upper=1);\n                theta::vector[N];\n                phi::vector[N];\n            end\n            convolved_re = @broadcasted(sqrt(1 - rho) * theta + sqrt(rho / scaling_factor) * phi)\n            @model @views begin\n                y ~ poisson_log(@broadcasted(log_E + beta0 + convolved_re * sigma));\n                \n                target += -0.5 * dot_self(phi[node1] - phi[node2]);\n                \n                beta0 ~ normal(0, 1);\n                theta ~ normal(0, 1);\n                sigma ~ normal(0, 1);\n                rho ~ beta(0.5, 0.5);\n                sum(phi) ~ normal(0, 0.001 * N);\n            end\n        end\nend\njulia_implementation(::Val{:bones_model}; nChild, nInd, gamma, delta, ncat, grade, kwargs...) = begin \n        # error(ncat)\n    @stan begin \n        @parameters begin\n            theta::real[nChild]\n        end\n        @model @views begin\n            theta ~ normal(0.0, 36.);\n            p = zeros((nChild, nInd, 5))\n            Q = zeros((nChild, nInd, 4))\n            for i in 1:nChild\n                for j in 1:nInd\n                    for k in 1:(ncat[j]-1)\n                        Q[i,j,k] = inv_logit(delta[j] * (theta[i] - gamma[j, k]))\n                    end\n                    p[i,j,1] = 1 - Q[i,j,1]\n                    for k in 2:(ncat[j]-1)\n                        p[i, j, k] = Q[i, j, k - 1] - Q[i, j, k];\n                    end\n                    p[i, j, ncat[j]] = Q[i, j, ncat[j] - 1];\n                    if grade[i, j] != -1\n                        target += log(p[i, j, grade[i, j]]);\n                    end\n                end\n            end\n        end\n    end\nend\njulia_implementation(::Val{:logistic_regression_rhs}; n, d, y, x, scale_icept,\n    scale_global,\n    nu_global,\n    nu_local, \n    slab_scale,\n    slab_df, kwargs...) = begin \n        x = Matrix{Float64}(x)\n@stan begin \n            @parameters begin\n                beta0::real;\n                z::vector[d];\n                tau::real(lower=0.);\n                lambda::vector(lower=0.)[d];\n                caux::real(lower=0.);\n            end\n            c = slab_scale * sqrt(caux);\n            lambda_tilde = @broadcasted sqrt(c ^ 2 * square(lambda) / (c ^ 2 + tau ^ 2 * square(lambda)));\n            beta = @. z * lambda_tilde * tau;\n            @model begin\n                z ~ std_normal();\n                lambda ~ student_t(nu_local, 0., 1.);\n                tau ~ student_t(nu_global, 0, 2. * scale_global);\n                caux ~ inv_gamma(0.5 * slab_df, 0.5 * slab_df);\n                beta0 ~ normal(0., scale_icept);\n                \n                y ~ bernoulli_logit_glm(x, beta0, beta);\n            end\n        end\nend\n\njulia_implementation(::Val{:hmm_example}; N, K, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta1::simplex[K]\n            theta2::simplex[K]\n            mu::positive_ordered[K]\n        end\n        theta = hcat(theta1, theta2)'\n        @model @views begin\n            target += normal_lpdf(mu[1], 3, 1);\n            target += normal_lpdf(mu[2], 10, 1);\n            acc = zeros(K)\n            gamma = zeros((K,N))'\n            gamma[1, :] .= @. normal_lpdf(y[1], mu, 1) \n            for t in 2:N\n                for k in 1:K\n                    for j in 1:K\n                        acc[j] =  gamma[t - 1, j] + log(theta[j, k]) + normal_lpdf(y[t], mu[k], 1);\n                    end\n                    gamma[t, k] = log_sum_exp(acc);\n                end\n            end\n            target += log_sum_exp(gamma[N, :])\n        end\n    end\nend\njulia_implementation(::Val{:Mb_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int64, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0.,upper=1.)\n                p::real(lower=0.,upper=1.)\n                c::real(lower=0.,upper=1.)\n            end\n            @model @views begin\n                p_eff = hcat(\n                    fill(p, M), \n                    @. (1 - y[:, 1:end-1]) * p + y[:, 1:end-1] * c\n                )\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + bernoulli_lpmf(y[i, :], p_eff[i, :])\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + bernoulli_lpmf(y[i, :], p_eff[i, :]),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n        end\nend\njulia_implementation(::Val{:Mh_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, ) \n        C = 0\n        for i in 1:M\n            if y[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0,upper=1)\n                mean_p::real(lower=0,upper=1)\n                sigma::real(lower=0,upper=5)\n                eps_raw::vector[M]\n            end\n            eps = @. logit(mean_p) + sigma * eps_raw\n            @model begin\n                eps_raw ~ normal(0, 1)\n                for i in 1:M\n                    if y[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + binomial_logit_lpmf(y[i], T, eps[i])\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + binomial_logit_lpmf(0, T, eps[i]),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n        end\nend\njulia_implementation(::Val{:Mth_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int64, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n        @stan begin \n            @parameters begin\n                omega::real(lower=0.,upper=1.)\n                mean_p::vector(lower=0.,upper=1.)[T]\n                sigma::real(lower=0., upper=5.)\n                eps_raw::vector[M]\n            end\n            @model @views begin\n                logit_p = @. logit(mean_p)' .+ sigma * eps_raw\n                eps_raw ~ normal(0., 1.)\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + bernoulli_logit_lpmf(y[i, :], logit_p[i, :])\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + bernoulli_logit_lpmf(y[i, :], logit_p[i, :]),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n    end\nend\njulia_implementation(::Val{Symbol(\"2pl_latent_reg_irt\")}; I, J, N, ii, jj, y, K, W, kwargs...) = begin \n    obtain_adjustments(W) = begin \n        local M, K = size(W)\n        adj = zeros((2, K))\n        adj[1,1] = 0\n        adj[2, 1] = 1\n        for k in 2:K\n            min_w = minimum(W[:,k])\n            max_w = maximum(W[:,k])\n            minmax_count = sum(w-&gt;w in (min_w, max_w), W[:, k])\n            if minmax_count == M\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = max_w - min_w;\n            else\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = sd(W[:, k]) * 2;\n            end\n        end\n        return adj\n    end\n    adj = obtain_adjustments(W);\n    W_adj = @. ((W - adj[1:1,:])/adj[2:2,:])\n    @stan begin \n        @parameters begin\n            alpha::vector(lower=0)[I];\n            beta_free::vector[I - 1] ;\n            theta::vector[J];\n            lambda_adj::vector[K];\n        end\n        beta = vcat(beta_free, -sum(beta_free))\n        @model @views begin\n            alpha ~ lognormal(1, 1);\n            target += normal_lpdf(beta, 0, 3);\n            lambda_adj ~ student_t(3, 0, 1);\n            theta ~ normal(W_adj * lambda_adj, 1);\n            y ~ bernoulli_logit(@broadcasted(alpha[ii] * theta[jj] - beta[ii]));\n        end\n    end\nend\njulia_implementation(::Val{:Mtbh_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int64, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0,upper=1)\n                mean_p::vector(lower=0,upper=1)[T]\n                gamma::real\n                sigma::real(lower=0, upper=3)\n                eps_raw::vector[M]\n            end\n            @model @views begin\n                eps = @. sigma * eps_raw\n                alpha = @. logit(mean_p)\n                logit_p = hcat(\n                    @.(alpha[1] + eps),\n                    @.(alpha[2:end]' + eps + gamma * y[:, 1:end-1])\n                )\n                gamma ~ normal(0, 10)\n                eps_raw ~ normal(0, 1)\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + bernoulli_logit_lpmf(y[i, :], logit_p[i, :])\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + bernoulli_logit_lpmf(y[i, :], logit_p[i, :]),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n    end\nend\njulia_implementation(::Val{:multi_occupancy}; J, K, n, X, S, kwargs...) = begin \n    cov_matrix_2d(sigma, rho) = begin\n        rv12 = sigma[1] * sigma[2] * rho\n        [\n            square(sigma[1]) rv12\n            rv12 square(sigma[2])\n        ]\n    end\n    lp_observed(X, K, logit_psi, logit_theta) = log_inv_logit(logit_psi) + binomial_logit_lpmf(X, K, logit_theta);\n    lp_unobserved(K, logit_psi, logit_theta) = log_sum_exp(\n        lp_observed(0, K, logit_psi, logit_theta),\n        log1m_inv_logit(logit_psi)\n    );\n    lp_never_observed(J, K, logit_psi, logit_theta, Omega) = begin\n        lp_unavailable = bernoulli_lpmf(0, Omega);\n        lp_available = bernoulli_lpmf(1, Omega) + J * lp_unobserved(K, logit_psi, logit_theta);\n        return log_sum_exp(lp_unavailable, lp_available);\n    end\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::real\n                Omega::real(lower=0, upper=+1)\n                rho_uv::real(lower=-1, upper=+1)\n                sigma_uv::vector(lower=0,upper=+Inf)[2]\n                uv1::vector[S]\n                uv2::vector[S]\n            end\n            @transformed_parameters begin \n                uv = hcat(uv1, uv2)\n                logit_psi = @. uv1 + alpha\n                logit_theta = @. uv2 + beta\n            end\n            @model begin\n                alpha ~ cauchy(0, 2.5);\n                beta ~ cauchy(0, 2.5);\n                sigma_uv ~ cauchy(0, 2.5);\n                ((rho_uv + 1) / 2) ~ beta(2, 2);\n                target += multi_normal_lpdf(uv, rep_vector(0., 2), cov_matrix_2d(sigma_uv, rho_uv));\n                Omega ~ beta(2, 2);\n  \n                for i in 1:n \n                    1 ~ bernoulli(Omega); \n                    for j in 1:J\n                        if X[i, j] &gt; 0\n                            target += lp_observed(X[i, j], K, logit_psi[i], logit_theta[i]);\n                        else\n                            target += lp_unobserved(K, logit_psi[i], logit_theta[i]);\n                        end\n                    end\n                end\n                for i in (n + 1):S\n                  target += lp_never_observed(J, K, logit_psi[i], logit_theta[i], Omega);\n                end\n            end\n        end\nend\njulia_implementation(::Val{:losscurve_sislob};\ngrowthmodel_id, \nn_data,\nn_time,\nn_cohort,\ncohort_id,\nt_idx,\ncohort_maxtime,\nt_value,\npremium,\nloss,\nkwargs...) = begin \n    growth_factor_weibull(t, omega, theta) = begin\n        return 1 - exp(-(t / theta) ^ omega);\n    end\n\n    growth_factor_loglogistic(t, omega, theta) = begin\n        pow_t_omega = t ^ omega;\n        return pow_t_omega / (pow_t_omega + theta ^ omega);\n    end\n    @stan begin \n            @parameters begin\n                omega::real(lower=0);\n                theta::real(lower=0);\n                \n                LR::vector(lower=0)[n_cohort];\n                \n                mu_LR::real;\n                sd_LR::real(lower=0);\n                \n                loss_sd::real(lower=0);\n            end\n            gf = if growthmodel_id == 1\n                @. growth_factor_weibull(t_value, omega, theta)\n            else\n                @. growth_factor_loglogistic(t_value, omega, theta)\n            end\n            @model @views begin\n                mu_LR ~ normal(0, 0.5);\n                sd_LR ~ lognormal(0, 0.5);\n                \n                LR ~ lognormal(mu_LR, sd_LR);\n                \n                loss_sd ~ lognormal(0, 0.7);\n                \n                omega ~ lognormal(0, 0.5);\n                theta ~ lognormal(0, 0.5);\n                \n                loss ~ normal(@broadcasted(LR[cohort_id] * premium[cohort_id] * gf[t_idx]), (loss_sd * premium)[cohort_id]);\n            end\n    end\nend\n\njulia_implementation(::Val{:accel_splines}; N,Y,Ks,Xs,knots_1,Zs_1_1, Ks_sigma, Xs_sigma,knots_sigma_1,Zs_sigma_1_1,prior_only, kwargs...) = begin \n@stan begin \n            @parameters begin\n                Intercept::real;\n                bs::vector[Ks];\n                zs_1_1::vector[knots_1];\n                sds_1_1::real(lower=0);\n                Intercept_sigma::real;\n                bs_sigma::vector[Ks_sigma];\n                zs_sigma_1_1::vector[knots_sigma_1];\n                sds_sigma_1_1::real(lower=0);\n            end\n            s_1_1 = @. sds_1_1 * zs_1_1\n            s_sigma_1_1 = @. sds_sigma_1_1 * zs_sigma_1_1;\n            @model begin\n                mu = Intercept .+ Xs * bs + Zs_1_1 * s_1_1;\n                sigma = exp.(Intercept_sigma .+ Xs_sigma * bs_sigma\n                                  + Zs_sigma_1_1 * s_sigma_1_1);\n                target += student_t_lpdf(Intercept, 3, -13, 36);\n                target += normal_lpdf(zs_1_1, 0, 1);\n                target += student_t_lpdf(sds_1_1, 3, 0, 36)\n                        #   - 1 * student_t_lccdf(0, 3, 0, 36);\n                target += student_t_lpdf(Intercept_sigma, 3, 0, 10);\n                target += normal_lpdf(zs_sigma_1_1, 0, 1);\n                target += student_t_lpdf(sds_sigma_1_1, 3, 0, 36)\n                        #   - 1 * student_t_lccdf(0, 3, 0, 36);\n                if !(prior_only == 1)\n                    target += normal_lpdf(Y, mu, sigma);\n                end\n            end\n        end\nend\njulia_implementation(::Val{Symbol(\"grsm_latent_reg_irt\")}; I, J, N, ii, jj, y, K, W, kwargs...) = begin \n    rsm(y, theta, beta, kappa) = begin\n      unsummed = vcat(0, theta .- beta .- kappa);\n      probs = softmax(cumulative_sum(unsummed));\n      return categorical_lpmf(y + 1, probs);\n    end\n    obtain_adjustments(W) = begin \n        local M, K = size(W)\n        adj = zeros((2, K))\n        adj[1,1] = 0\n        adj[2, 1] = 1\n        for k in 2:K\n            min_w = minimum(W[:,k])\n            max_w = maximum(W[:,k])\n            minmax_count = sum(w-&gt;w in (min_w, max_w), W[:, k])\n            if minmax_count == M\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = max_w - min_w;\n            else\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = sd(W[:, k]) * 2;\n            end\n        end\n        return adj\n    end\n        m = maximum(y)\n        adj = obtain_adjustments(W);\n        W_adj = @. ((W - adj[1:1,:])/adj[2:2,:])\n@stan begin \n            @parameters begin\n                alpha::vector(lower=0)[I];\n                beta_free::vector[I - 1] ;\n                kappa_free::vector[m - 1] ;\n                theta::vector[J];\n                lambda_adj::vector[K];\n            end\n            beta = vcat(beta_free, -sum(beta_free))\n            kappa = vcat(kappa_free, -sum(kappa_free))\n            @model @views begin\n                alpha ~ lognormal(1, 1);\n                target += normal_lpdf(beta, 0, 3);\n                target += normal_lpdf(kappa, 0, 3);\n                theta ~ normal(W_adj * lambda_adj, 1);\n                lambda_adj ~ student_t(3, 0, 1);\n                for n in 1:N\n                    target += rsm(y[n], theta[jj[n]] .* alpha[ii[n]], beta[ii[n]], kappa)\n                end\n            end\n    end\nend\njulia_implementation(::Val{:prophet};\n    T,\n    K,\n    t,\n    cap,\n    y,\n    S,\n    t_change,\n    X,\n    sigmas,\n    tau,\n    trend_indicator,\n    s_a,\n    s_m, \n    kwargs...) = begin \n    get_changepoint_matrix(t, t_change, T, S) = begin\n        local A = rep_matrix(0, T, S);\n        a_row = rep_row_vector(0, S);\n        cp_idx = 1;\n        \n        for i in 1:T\n          while ((cp_idx &lt;= S) && (t[i] &gt;= t_change[cp_idx])) \n            a_row[cp_idx] = 1;\n            cp_idx = cp_idx + 1;\n          end\n          A[i,:] = a_row;\n        end\n        return A;\n      end\n      \n      \n      logistic_gamma(k, m, delta, t_change, S) = begin\n        local gamma = zeros(S)\n        k_s = append_row(k, k + cumulative_sum(delta));\n        \n        m_pr = m; \n        for i in 1:S\n          gamma[i] = (t_change[i] - m_pr) * (1 - k_s[i] / k_s[i + 1]);\n          m_pr = m_pr + gamma[i]; \n        end\n        return gamma;\n      end\n      \n      logistic_trend(k, m, delta, t, cap, A, t_change, S) = begin\n        local gamma = logistic_gamma(k, m, delta, t_change, S);\n        return cap .* inv_logit.((k .+ A * delta) .* (t .- m .- A * gamma));\n      end\n      \n      linear_trend(k, m, delta, t, A, t_change) = begin\n        return (k .+ A * delta) .* t .+ (m .+ A * (-t_change .* delta));\n      end\n        A = get_changepoint_matrix(t, t_change, T, S)\n@stan begin \n            @parameters begin\n                k::real\n                m::real\n                delta::vector[S]\n                sigma_obs::real(lower=0)\n                beta::vector[K]\n            end\n            @model begin\n                k ~ normal(0, 5);\n                m ~ normal(0, 5);\n                delta ~ double_exponential(0, tau);\n                sigma_obs ~ normal(0, 0.5);\n                beta ~ normal(0, sigmas);\n                \n                if trend_indicator == 0\n                    y ~ normal(linear_trend(k, m, delta, t, A, t_change)\n                             .* (1 .+ X * (beta .* s_m)) + X * (beta .* s_a), sigma_obs);\n                elseif trend_indicator == 1\n                    y ~ normal(logistic_trend(k, m, delta, t, cap, A, t_change, S)\n                             .* (1 .+ X * (beta .* s_m)) + X * (beta .* s_a), sigma_obs);\n                end\n            end\n        end\nend\n\njulia_implementation(::Val{:hmm_gaussian}; T, K, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            pi1::simplex[K]\n            A::simplex[K,K]\n            mu::ordered[K]\n            sigma::vector(lower=0)[K]\n        end\n        @model @views begin\n            logalpha = zeros((K,T))'\n            accumulator = zeros(K)\n            logalpha[1,:] .= log.(pi1) .+ normal_lpdf(y[1], mu, sigma);\n            for t in 2 : T\n                for j in 1:K\n                    for i in 1:K\n                        accumulator[i] = logalpha[t - 1, i] + log(A[i, j]) + normal_lpdf(y[t], mu[j], sigma[j]);\n                    end\n                logalpha[t, j] = log_sum_exp(accumulator);\n                end\n            end\n            target += log_sum_exp(logalpha[T,:]);\n        end\n    end\nend\n\njulia_implementation(::Val{:hmm_drive_0}; K, N, u, v, alpha, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta1::simplex[K]\n            theta2::simplex[K]\n            phi::positive_ordered[K]\n            lambda::positive_ordered[K]\n        end\n        theta = hcat(theta1, theta2)'\n        @model @views begin\n            for k in 1:K\n              target += dirichlet_lpdf(theta[k, :], alpha[k, :]);\n            end\n            target += normal_lpdf(phi[1], 0, 1);\n            target += normal_lpdf(phi[2], 3, 1);\n            target += normal_lpdf(lambda[1], 0, 1);\n            target += normal_lpdf(lambda[2], 3, 1);\n\n            acc = zeros(K)\n            gamma = zeros((K,N))'\n            gamma[1,:] .= @.(exponential_lpdf(u[1], phi) + exponential_lpdf(v[1], lambda))\n            for t in 2:N\n                for k in 1:K\n                    for j in 1:K\n                        acc[j] = gamma[t - 1, j] + log(theta[j, k]) + exponential_lpdf(u[t], phi[k]) + exponential_lpdf(v[t], lambda[k]);\n                    end\n                    gamma[t, k] = log_sum_exp(acc);\n                end\n            end\n            target += log_sum_exp(gamma[N, :])\n        end\n    end\nend\n\njulia_implementation(::Val{:hmm_drive_1}; K, N, u, v, alpha, tau, rho, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta1::simplex[K]\n            theta2::simplex[K]\n            phi::ordered[K]\n            lambda::ordered[K]\n        end\n        theta = hcat(theta1, theta2)'\n        @model @views begin\n            for k in 1:K\n              target += dirichlet_lpdf(theta[k, :], alpha[k, :]);\n            end\n            target += normal_lpdf(phi[1], 0, 1);\n            target += normal_lpdf(phi[2], 3, 1);\n            target += normal_lpdf(lambda[1], 0, 1);\n            target += normal_lpdf(lambda[2], 3, 1);\n\n            acc = zeros(K)\n            gamma = zeros((K,N))'\n            gamma[1,:] .= @.(normal_lpdf(u[1], phi, tau) + normal_lpdf(v[1], lambda, rho))\n            for t in 2:N\n                for k in 1:K\n                    for j in 1:K\n                        acc[j] = gamma[t - 1, j] + log(theta[j, k]) + normal_lpdf(u[t], phi[k], tau) + normal_lpdf(v[t], lambda[k], rho);\n                    end\n                    gamma[t, k] = log_sum_exp(acc);\n                end\n            end\n            target += log_sum_exp(gamma[N, :])\n        end\n    end\nend\n\njulia_implementation(::Val{:iohmm_reg}; T, K, M, y, u, kwargs...) = begin \n    @inline normalize(x) = x ./ sum(x)\n    @stan begin \n        @parameters begin\n            pi1::simplex[K]\n            w::vector[K,M]\n            b::vector[K,M]\n            sigma::vector(lower=0)[K]\n        end\n        @model @views begin\n            unA = hcat(pi1, w * u'[:, 2:end])'\n            A = copy(unA)\n            for t in 2:T\n                A[t, :] .= softmax(unA[t, :])\n            end\n            logA = log.(A)\n            logoblik = normal_lpdf.(y, u * b', sigma')\n            accumulator = zeros(K)\n            logalpha = zeros((K,T))'\n            logalpha[1,:] .= @.(log(pi1) + logoblik[1,:])\n            for t in 2:T\n                for j in 1:K\n                    for i in 1:K\n                        accumulator[i] = logalpha[t - 1, i] + logA[t,i] + logoblik[t,j];\n                    end\n                    logalpha[t, j] = log_sum_exp(accumulator);\n                end\n            end\n            w ~ normal(0, 5);\n            b ~ normal(0, 5);\n            sigma ~ normal(0, 3);\n            target += log_sum_exp(logalpha[T,:]);\n        end\n    end\nend\n\njulia_implementation(::Val{:accel_gp}; N, Y, Kgp_1, Dgp_1, NBgp_1, Xgp_1, slambda_1, Kgp_sigma_1, Dgp_sigma_1, NBgp_sigma_1, Xgp_sigma_1, slambda_sigma_1, prior_only, kwargs...) = begin \n    @inline sqrt_spd_cov_exp_quad(slambda, sdgp, lscale) = begin \n        NB, D = size(slambda)\n        Dls = length(lscale)\n        if Dls == 1\n            constant = (sdgp) * (sqrt(2 * pi) * lscale[1]) ^ (D/2)\n            neg_half_lscale2 = -0.25 * square(lscale[1])\n            @.(constant * exp(neg_half_lscale2 * dot_self($eachrow(slambda))))\n        else\n            error()\n        end\n    end\n    @inline gpa(X, sdgp, lscale, zgp, slambda) = X * (sqrt_spd_cov_exp_quad(slambda, sdgp, lscale) .* zgp)\n    @stan begin \n        @parameters begin\n            Intercept::real\n            sdgp_1::real(lower=0)\n            lscale_1::real(lower=0)\n            zgp_1::vector[NBgp_1]\n            Intercept_sigma::real\n            sdgp_sigma_1::real(lower=0)\n            lscale_sigma_1::real(lower=0)\n            zgp_sigma_1::vector[NBgp_sigma_1]\n        end\n        vsdgp_1 = fill(sdgp_1, 1)\n        vlscale_1 = fill(lscale_1, (1, 1))\n        vsdgp_sigma_1 = fill(sdgp_sigma_1, 1)\n        vlscale_sigma_1 = fill(lscale_sigma_1, (1, 1))\n        @model @views begin\n            mu = Intercept .+ gpa(Xgp_1, vsdgp_1[1], vlscale_1[1,:], zgp_1, slambda_1);\n            sigma = exp.(Intercept_sigma .+ gpa(Xgp_sigma_1, vsdgp_sigma_1[1], vlscale_sigma_1[1,:], zgp_sigma_1, slambda_sigma_1));\n            target += student_t_lpdf(Intercept, 3, -13, 36);\n            target += student_t_lpdf(vsdgp_1, 3, 0, 36)\n                    #   - 1 * student_t_lccdf(0, 3, 0, 36);\n            target += normal_lpdf(zgp_1, 0, 1);\n            target += inv_gamma_lpdf(vlscale_1[1,:], 1.124909, 0.0177);\n            target += student_t_lpdf(Intercept_sigma, 3, 0, 10);\n            target += student_t_lpdf(vsdgp_sigma_1, 3, 0, 36)\n                    #   - 1 * student_t_lccdf(0, 3, 0, 36);\n            target += normal_lpdf(zgp_sigma_1, 0, 1);\n            target += inv_gamma_lpdf(vlscale_sigma_1[1,:], 1.124909, 0.0177);\n            if prior_only == 0\n              target += normal_lpdf(Y, mu, sigma);\n            end\n        end\n    end\nend\n\n\n\njulia_implementation(::Val{:hierarchical_gp};\n        N,\n        N_states,\n        N_regions,\n        N_years_obs,\n        N_years,\n        state_region_ind,\n        state_ind,\n        region_ind,\n        year_ind,\n        y,\n        kwargs...) = begin \n    @stan begin \n        years = 1:N_years\n        counts = fill(2, 17)\n        @parameters begin\n            GP_region_std::matrix[N_years, N_regions]\n            GP_state_std::matrix[N_years, N_states]\n            year_std::vector[N_years_obs]\n            state_std::vector[N_states]\n            region_std::vector[N_regions]\n            tot_var::real(lower=0)\n            prop_var::simplex[17]\n            mu::real\n            length_GP_region_long::real(lower=0)\n            length_GP_state_long::real(lower=0)\n            length_GP_region_short::real(lower=0)\n            length_GP_state_short::real(lower=0)\n        end\n\n\n        vars = 17 * prop_var * tot_var;\n        sigma_year = sqrt(vars[1]);\n        sigma_region = sqrt(vars[2]);\n        sigma_state = @.(sqrt(vars[3:end]))\n        \n        sigma_GP_region_long = sqrt(vars[13]);\n        sigma_GP_state_long = sqrt(vars[14]);\n        sigma_GP_region_short = sqrt(vars[15]);\n        sigma_GP_state_short = sqrt(vars[16]);\n        sigma_error_state_2 = sqrt(vars[17]);\n        \n        region_re = sigma_region * region_std;\n        year_re = sigma_year * year_std;\n        state_re = sigma_state[state_region_ind] .* state_std;\n        \n        begin\n            cov_region = gp_exp_quad_cov(years, sigma_GP_region_long,\n                                        length_GP_region_long)\n                        + gp_exp_quad_cov(years, sigma_GP_region_short,\n                                        length_GP_region_short);\n            cov_state = gp_exp_quad_cov(years, sigma_GP_state_long,\n                                        length_GP_state_long)\n                        + gp_exp_quad_cov(years, sigma_GP_state_short,\n                                        length_GP_state_short);\n            for year in 1 : N_years\n                cov_region[year, year] = cov_region[year, year] + 1e-6;\n                cov_state[year, year] = cov_state[year, year] + 1e-6;\n            end\n            \n            L_cov_region = cholesky_decompose(cov_region);\n            L_cov_state = cholesky_decompose(cov_state);\n            GP_region = L_cov_region * GP_region_std;\n            GP_state = L_cov_state * GP_state_std;\n        end\n        @model begin\n            obs_mu = zeros(N)\n            for n in 1 : N\n                obs_mu[n] = mu + year_re[year_ind[n]] + state_re[state_ind[n]]\n                            + region_re[region_ind[n]]\n                            + GP_region[year_ind[n], region_ind[n]]\n                            + GP_state[year_ind[n], state_ind[n]];\n                end\n                y ~ normal(obs_mu, sigma_error_state_2); \n                \n                (GP_region_std) ~ normal(0, 1);\n                (GP_state_std) ~ normal(0, 1);\n                year_std ~ normal(0, 1);\n                state_std ~ normal(0, 1);\n                region_std ~ normal(0, 1);\n                mu ~ normal(.5, .5);\n                tot_var ~ gamma(3, 3);\n                prop_var ~ dirichlet(counts);\n                length_GP_region_long ~ weibull(30, 8);\n                length_GP_state_long ~ weibull(30, 8);\n                length_GP_region_short ~ weibull(30, 3);\n                length_GP_state_short ~ weibull(30, 3);\n        end\n    end\nend\njulia_implementation(::Val{:kronecker_gp}; n1, n2, x1, y, kwargs...) = begin \n    kron_mvprod(A, B, V) = adjoint(A * adjoint(B * V))\n    calculate_eigenvalues(A, B, sigma2) = A .* B' .+ sigma2\n    xd  = -(x1 .- x1') .^ 2\n    return @stan begin end\n    @stan begin \n        @parameters begin\n            var1::real(lower=0)\n            bw1::real(lower=0)\n            L::cholesky_factor_corr[n2]\n            sigma1::real(lower=.00001)\n        end\n        Lambda = multiply_lower_tri_self_transpose(L)\n        Sigma1 = Symmetric(var1 .* exp.(xd .* bw1) + .00001 * I);\n        R1, Q1 = eigen(Sigma1);\n        R2, Q2 = eigen(Lambda);\n        eigenvalues = calculate_eigenvalues(R2, R1, sigma1);\n        @model @views begin\n            var1 ~ lognormal(0, 1);\n            bw1 ~ cauchy(0, 2.5);\n            sigma1 ~ lognormal(0, 1);\n            L ~ lkj_corr_cholesky(2);\n            target += -0.5 * sum(y .* kron_mvprod(Q1, Q2, kron_mvprod(transpose(Q1), transpose(Q2), y) ./ eigenvalues)) - 0.5 * sum(log(eigenvalues))\n        end\n    end\nend\n\n\njulia_implementation(::Val{:covid19imperial_v2}; M, P, N0, N, N2, cases, deaths, f, X, EpidemicStart, pop, SI, kwargs...) = begin \n    SI_rev = reverse(SI)\n    f_rev = mapreduce(reverse, hcat, eachcol(f))'\n    @stan begin \n        @parameters begin\n            mu::real(lower=0)[M]\n            alpha_hier::real(lower=0)[P]\n            kappa::real(lower=0)\n            y::real(lower=0)[M]\n            phi::real(lower=0)\n            tau::real(lower=0)\n            ifr_noise::real(lower=0)[M]\n        end\n        @model @views begin\n            prediction = rep_matrix(0., N2, M);\n            E_deaths = rep_matrix(0., N2, M);\n            Rt = rep_matrix(0., N2, M);\n            Rt_adj = copy(Rt);\n            cumm_sum = rep_matrix(0., N2, M);\n            alpha = alpha_hier .- (log(1.05) / 6.)\n            for m in 1:M\n                prediction[1:N0, m] .= y[m]\n                cumm_sum[2:N0, m] .= cumulative_sum(prediction[2:N0, m]);\n                Rt[:, m] .= mu[m] * exp.(-X[m,:,:] * alpha);\n                Rt_adj[1:N0, m] .= Rt[1:N0, m];\n                for i in (N0+1):N2\n                    convolution = dot_product(sub_col(prediction, 1, m, i - 1), stan_tail(SI_rev, i - 1))\n                    cumm_sum[i, m] = cumm_sum[i - 1, m] + prediction[i - 1, m];\n                    Rt_adj[i, m] = ((pop[m] - cumm_sum[i, m]) / pop[m]) * Rt[i, m];\n                    prediction[i, m] = Rt_adj[i, m] * convolution;\n                end\n                E_deaths[1, m] = 1e-15 * prediction[1, m];\n                for i in 2:N2\n                    E_deaths[i, m] = ifr_noise[m] * dot_product(sub_col(prediction, 1, m, i - 1), stan_tail(f_rev[m, :], i - 1));\n                end\n            end\n            tau ~ exponential(0.03);\n            y ~ exponential(1 / tau)\n            phi ~ normal(0, 5);\n            kappa ~ normal(0, 0.5);\n            mu ~ normal(3.28, kappa);\n            alpha_hier ~ gamma(.1667, 1);\n            ifr_noise ~ normal(1, 0.1);\n            for m in 1:M\n                deaths[EpidemicStart[m] : N[m], m] ~ neg_binomial_2(E_deaths[EpidemicStart[m] : N[m], m], phi);\n            end\n        end\n    end\nend\n\n\njulia_implementation(::Val{:covid19imperial_v3}; kwargs...) = julia_implementation(Val{:covid19imperial_v2}(); kwargs...)\n\njulia_implementation(::Val{:gpcm_latent_reg_irt}; I, J, N, ii, jj, y, K, W, kwargs...) = begin \n    pcm(y, theta, beta) = begin \n        unsummed = append_row(rep_vector(0.0, 1), theta .- beta)\n        probs = softmax(cumulative_sum(unsummed))\n        categorical_lpmf(y + 1, probs)\n    end\n    obtain_adjustments(W) = begin \n        local M, K = size(W)\n        adj = zeros((2, K))\n        adj[1,1] = 0\n        adj[2, 1] = 1\n        for k in 2:K\n            min_w = minimum(W[:,k])\n            max_w = maximum(W[:,k])\n            minmax_count = sum(w-&gt;w in (min_w, max_w), W[:, k])\n            if minmax_count == M\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = max_w - min_w;\n            else\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = sd(W[:, k]) * 2;\n            end\n        end\n        return adj\n    end\n    m = fill(0, I)\n    pos = fill(1, I)\n    for n in 1:N\n        if y[n] &gt; m[ii[n]]\n            m[ii[n]] = y[n]\n        end\n    end\n    for i in 2:I\n        pos[i] = m[i - 1] + pos[i - 1]\n    end\n    adj = obtain_adjustments(W);\n    W_adj = @. ((W - adj[1:1,:])/adj[2:2,:])\n    @stan begin \n        @parameters begin\n            alpha::real(lower=0)[I]\n            beta_free::vector[sum(m) - 1]\n            theta::vector[J]\n            lambda_adj::vector[K]\n        end\n        beta = vcat(beta_free, -sum(beta_free))\n        @model @views begin\n            alpha ~ lognormal(1, 1);\n            target += normal_lpdf(beta, 0, 3);\n            theta ~ normal(W_adj * lambda_adj, 1);\n            lambda_adj ~ student_t(3, 0, 1);\n            for n in 1:N\n                target += pcm(y[n], theta[jj[n]] .* alpha[ii[n]], segment(beta, pos[ii[n]], m[ii[n]]));\n            end\n        end\n    end\nend\nend"
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "Julia vs Stan performance comparison",
    "section": "",
    "text": "Caveats\nThis page compares the performance of Julia’s and Stan’s log density and log density gradient computations for the implemented posteriors. Several caveats apply:\n\nThe posteriordb Stan implementations were never meant to represent “perfect and best-performant” practices.\nThe StanBlocks.jl implementations are not line-by-line translations of the Stan implementations. Sometimes small optimizations were applied, to make the implementation fall more in line with common Julia practices, or to make the code more friendly for Julia’s AD packages, e.g. by avoiding mutation.\nStan often automatically drops constant terms (unless configured differently), see https://mc-stan.org/docs/reference-manual/statements.html#log-probability-increment-vs.-distribution-statement, thus avoiding superfluous (for its purposes) computation, while the StanBlocks.jl implementations do not.\nStan implements a lot of custom AD rules, while StanBlocks.jl does not at all, and Enzyme.jl does rarely (if ever?). I suspect that adding AD rules for _glm_ type functions would further improve Julia’s performance.\nThe StanBlocks.jl “sampling” statements try to be clever about avoiding repeated computations. While I am not sure whether Stan applies the same optimizations, in principle it could do that without extra work by the user.\nWhile preliminary benchmark runs included “all” Julia AD packages, all of them are almost always much slower than Enzyme.jl for the implemented posteriors, which on top of that performance advantage also supports more Julia language features than some of the other AD packages. As such, I am only comparing Enzyme and Stan. Enzyme outperforming every other AD package for these posteriors/loss functions does of course not mean that it will necessarily do as well for other applications.\nEnzyme’s development is progressing quite quickly. While it currently sometimes crashes Julia, or it sometimes errors while trying to compute a gradient, in general Enzyme’s performance and reliability are continuously and quickly improving.\nStan’s benchmark is done from Julia via BridgeStan.jl. While I think that any performance penalty should be extremely small, I am not 100% sure. BridgeStan uses the -O3 compiler flag by default, but no additional ones.\nAll benchmarks are happening with a single thread on my local machine.\nThere are probably more caveats!\n\n\n\n\n\n\n\nWarning\n\n\n\nIn general, doing performance comparisons is quite tricky, for more reasons than just the ones mentioned above. The below plot and tables should most definitely NOT be interpreted as “A is X-times faster than B”.\n\n\n\n\n\nThe WebIO Jupyter extension was not detected. See the\n\n    WebIO Jupyter integration documentation\n\nfor more information.\n\n\n\n\n\nVisualization\n\n\n\n\n\n\nWarning\n\n\n\nIn general, doing performance comparisons is quite tricky, for more reasons than just the ones mentioned above. The below plot and tables should most definitely NOT be interpreted as “A is X-times faster than B”.\n\n\nThe below plot shows the relative primitive runtime (x-axis, Julia vs Stan, left: Julia is faster) and the relative gradient runtime (y-axis, Julia+X vs Stan, bottom: Julia is faster) for the posteriordb models for which the overview table has a value less than 1e-8 in the median relative ulpdf error column. The color of the points represents the Julia AD framework used, which currently includes Enzyme.jl and Mooncake.jl. Hovering over the data points will show the posterior name, its dimension, the allocations required by Julia during the primitive and gradient run and a short explanation, e.g. mesquite-logmesquite_logvash (D=7, #allocs=0-&gt;70) - Julia's primitive is ~4.5 times faster, but Enzyme's gradient is ~16.0 times slower. Any time spent on garbage collection has been subtracted from the measured wall times. All mean runtime estimates were run until the estimated standard error of the mean was smaller than roughly .5% of the estimated mean. Due to this, I have removed the credible intervals and standard errors from the plot and table.\n\n\n┌ Warning: attempting to remove probably stale pidfile\n│   path = \"/home/niko/.jlassetregistry.lock\"\n└ @ Pidfile ~/.julia/packages/Pidfile/DDu3M/src/Pidfile.jl:260\n\n\n    \n    \n\n\n\n\nTabular data\nThe below table shows information about the implemented posteriors. Will elaborate on the exact meaning of columns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nposterior name\nimplementations\ndimension\nmedian relative ulpdf error\nrelative mean primitive Julia runtime\nrelative mean primitive Stan runtime\nrelative mean Enzyme runtime\nrelative mean Mooncake runtime\nrelative mean Stan gradient runtime\nprimitive Julia allocations\nEnzyme allocations\nMooncake allocations\nmedian lpdf difference\nmedian Enzyme relative gradient error\nmedian Mooncake relative gradient error\nBridgestan\nEnzyme\nMooncake\n\n\n\n\nGLMM_data-GLMM1_model\n\nStan, Julia\n\n237\n1.2e-16\n1.0\n3.9\n1.0\n3.7\n2.2\n0\n0\n18\n-2.3\n1.2e-17\n1.2e-17\n2.6.1\n0.13.30\n0.4.86\n\n\nGLM_Binomial_data-GLM_Binomial_model\n\nStan, Julia\n\n3\n1.4e-16\n1.0\n1.5\n1.2\n3.1\n1.0\n0\n0\n47\n-14.0\n3.3e-16\n3.8e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nGLM_Poisson_Data-GLM_Poisson_model\n\nStan, Julia\n\n4\n3.6e-16\n1.0\n2.3\n1.0\n6.6\n1.5\n0\n0\n47\n0.0\n4.3e-16\n4.0e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nM0_data-M0_model\n\nStan, Julia\n\n2\n1.7e-16\n1.0\n1.2\n1.1\n2.0\n1.0\n0\n0\n5\n0.0\n3.0e-15\n8.9e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nMb_data-Mb_model\n\nStan, Julia\n\n3\n0.0\n1.0\n2.6\n2.3\n3.2\n1.0\n3\n32\n5770\n0.0\n6.6e-15\n4.8e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nMh_data-Mh_model\n\nStan, Julia\n\n388\n4.8e-16\n1.0\n1.7\n1.0\n1.6\n1.2\n1\n2\n14\n-160.0\n1.2e-15\n7.4e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nMt_data-Mt_model\n\nStan, Julia\n\n4\n1.5e-16\n1.0\n1.9\n3.9\n3.9\n1.0\n1\n26\n4297\n0.0\n1.9e-15\n2.6e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nMtbh_data-Mtbh_model\n\nStan, Julia\n\n154\n1.8e-16\n1.0\n2.4\n1.8\n2.5\n1.0\n6\n38\n2718\n-2.3\n6.1e-16\n3.4e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nMth_data-Mth_model\n\nStan, Julia\n\n394\n1.7e-16\n1.0\n2.2\n2.0\n2.9\n1.0\n3\n30\n7015\n0.0\n1.9e-15\n6.6e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nRate_1_data-Rate_1_model\n\nStan, Julia\n\n1\n9.2e-17\n1.0\n2.3\n1.0\n2.3\n2.1\n0\n0\n0\n5.5\n1.5e-16\n2.8e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nRate_2_data-Rate_2_model\n\nStan, Julia\n\n2\n8.7e-17\n1.0\n2.0\n1.0\n2.6\n1.7\n0\n0\n0\n10.0\n2.1e-16\n2.7e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nRate_3_data-Rate_3_model\n\nStan, Julia\n\n1\n0.0\n1.0\n1.7\n1.0\n2.6\n1.6\n0\n0\n0\n10.0\n2.0e-16\n2.0e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nRate_4_data-Rate_4_model\n\nStan, Julia\n\n2\n8.7e-17\n1.0\n2.5\n1.0\n2.5\n2.2\n0\n0\n0\n2.7\n1.6e-16\n1.9e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nRate_5_data-Rate_5_model\n\nStan, Julia\n\n1\n0.0\n1.0\n2.0\n1.0\n3.1\n2.1\n0\n0\n0\n0.0\n1.8e-16\n2.4e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nSurvey_data-Survey_model\n\nStan, Julia\n\n1\n5.4e-16\n1.0\n1.1\n2.8\n6.8\n1.0\n1\n40\n20865\n1.4e-14\n7.4e-14\n5.4e-14\n2.6.1\n0.13.30\n0.4.86\n\n\narK-arK\n\nStan, Julia\n\n7\n1.4e-16\n1.0\n5.7\n1.0\n5.6\n4.6\n0\n0\n15\n-15.0\n1.4e-15\n1.5e-15\n2.6.1\n0.13.30\n0.4.86\n\n\narma-arma11\n\nStan, Julia\n\n4\n2.3e-16\n1.0\n3.8\n1.0\n5.4\n5.5\n0\n0\n5\n-4.6\n1.5e-16\n2.6e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nbball_drive_event_0-hmm_drive_0\n\nStan, Julia\n\n6\n1.2e-16\n1.0\n2.4\n3.8\n2.8\n1.0\n9\n40\n77\n-2.3\n4.0e-12\n2.9e-12\n2.6.1\n0.13.30\n0.4.86\n\n\nbball_drive_event_1-hmm_drive_1\n\nStan, Julia\n\n6\n9.1e-16\n1.0\n2.6\n2.7\n2.5\n1.0\n11\n44\n84\n760.0\n3.2e-12\n3.8e-12\n2.6.1\n0.13.30\n0.4.86\n\n\nbones_data-bones_model\n\nStan, Julia\n\n13\n1.3e-16\n1.0\n2.6\n1.1\n1.6\n1.0\n3\n6\n25\n-47.0\n1.4e-16\n1.4e-16\n2.6.1\n0.13.30\n0.4.86\n\n\ndiamonds-diamonds\n\nStan, Julia\n\n26\n1.2e-15\n1.0\n1.5\n1.6\n5.0\n1.0\n2\n4\n43\n4600.0\n2.1e-15\n1.9e-15\n2.6.1\n0.13.30\n0.4.86\n\n\ndogs-dogs\n\nStan, Julia\n\n3\n4.2e-16\n1.0\n5.1\n1.0\n1.6\n3.4\n0\n0\n5\n-14.0\n1.8e-16\n1.8e-16\n2.6.1\n0.13.30\n0.4.86\n\n\ndogs-dogs_hierarchical\n\nStan, Julia\n\n2\n1.1e-15\n1.0\n1.7\n2.1\n1.9\n1.0\n0\n0\n23\n2.3e-13\n1.2e-15\n1.2e-15\n2.6.1\n0.13.30\n0.4.86\n\n\ndogs-dogs_log\n\nStan, Julia\n\n2\n5.0e-16\n1.0\n2.5\n1.0\n1.2\n1.9\n0\n0\n5\n-9.2\n3.7e-15\n1.8e-15\n2.6.1\n0.13.30\n0.4.86\n\n\ndugongs_data-dugongs_model\n\nStan, Julia\n\n4\n1.7e-16\n1.0\n1.7\n1.0\n2.6\n1.1\n0\n0\n5\n-22.0\n1.7e-16\n2.9e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nearnings-earn_height\n\nStan, Julia\n\n3\n6.6e-16\n1.0\n9.3\n1.0\n16.0\n7.4\n0\n0\n25\n0.0\n8.2e-16\n7.6e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nearnings-log10earn_height\n\nStan, Julia\n\n3\n8.1e-16\n1.0\n9.5\n1.0\n12.0\n5.7\n0\n0\n25\n0.0\n1.0e-15\n1.0e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nearnings-logearn_height\n\nStan, Julia\n\n3\n8.1e-16\n1.0\n9.2\n1.0\n12.0\n5.6\n0\n0\n20\n0.0\n1.0e-15\n9.9e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nearnings-logearn_height_male\n\nStan, Julia\n\n4\n7.6e-16\n1.0\n2.8\n1.0\n20.0\n6.7\n0\n0\n29\n0.0\n9.7e-16\n1.1e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nearnings-logearn_interaction\n\nStan, Julia\n\n5\n1.0e-15\n1.0\n3.2\n1.0\n17.0\n6.9\n0\n0\n29\n-1.2e-10\n1.0e-15\n9.0e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nearnings-logearn_interaction_z\n\nStan, Julia\n\n5\n7.6e-16\n1.0\n3.2\n1.0\n19.0\n7.6\n0\n0\n24\n0.0\n1.0e-15\n8.1e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nearnings-logearn_logheight_male\n\nStan, Julia\n\n4\n6.9e-16\n1.0\n3.0\n1.0\n19.0\n6.6\n0\n0\n29\n-1.8e-12\n8.9e-16\n9.5e-16\n2.6.1\n0.13.30\n0.4.86\n\n\neight_schools-eight_schools_centered\n\nStan, Julia\n\n10\n2.1e-16\n1.0\n1.9\n1.0\n9.8\n1.6\n0\n0\n47\n-23.0\n1.8e-16\n1.9e-16\n2.6.1\n0.13.30\n0.4.86\n\n\neight_schools-eight_schools_noncentered\n\nStan, Julia\n\n10\n0.0\n1.0\n2.4\n1.0\n9.9\n2.0\n0\n0\n44\n-23.0\n3.5e-17\n3.6e-17\n2.6.1\n0.13.30\n0.4.86\n\n\nelection88-election88_full\n\nStan, Julia\n\n90\n9.1e-17\n1.0\n1.9\n1.0\n3.7\n1.8\n10\n10\n123\n-23.0\n1.1e-16\n1.4e-16\n2.6.1\n0.13.30\n0.4.86\n\n\ngarch-garch11\n\nStan, Julia\n\n4\n3.2e-16\n1.0\n3.4\n1.0\n1.9\n2.6\n0\n0\n0\n0.0\n5.0e-16\n5.3e-16\n2.6.1\n0.13.30\n0.4.86\n\n\ngp_pois_regr-gp_pois_regr\n\nStan, Julia\n\n13\n1.8e-16\n1.0\n1.5\n1.8\n5.5\n1.0\n6\n10\n88\n-21.0\n2.3e-16\n2.9e-16\n2.6.1\n0.13.30\n0.4.86\n\n\ngp_pois_regr-gp_regr\n\nStan, Julia\n\n3\n2.0e-16\n1.0\n1.9\n1.6\n4.4\n1.0\n9\n15\n62\n-31.0\n2.0e-16\n2.1e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nhmm_example-hmm_example\n\nStan, Julia\n\n4\n3.2e-16\n1.0\n2.8\n2.2\n1.0\n2.2\n7\n23\n28\n94.0\n1.8e-13\n2.0e-13\n2.6.1\n0.13.30\n0.4.86\n\n\nhmm_gaussian_simulated-hmm_gaussian\n\nStan, Julia\n\n14\n6.9e-16\n1.0\n2.3\n1.9\n1.8\n1.0\n8\n17\n67\n460.0\n1.8e-12\n2.1e-12\n2.6.1\n0.13.30\n0.4.86\n\n\nirt_2pl-irt_2pl\n\nStan, Julia\n\n144\n3.4e-16\n1.0\n1.5\n1.2\n1.9\n1.0\n21\n34\n501\n15.0\n8.2e-16\n1.4e-12\n2.6.1\n0.13.30\n0.4.86\n\n\nkidiq-kidscore_interaction\n\nStan, Julia\n\n5\n4.0e-16\n1.0\n3.4\n1.0\n17.0\n5.9\n0\n0\n29\n-0.92\n6.2e-16\n5.8e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nkidiq-kidscore_momhs\n\nStan, Julia\n\n3\n5.3e-16\n1.0\n9.0\n1.0\n13.0\n6.3\n0\n0\n20\n-0.92\n5.0e-16\n5.9e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nkidiq-kidscore_momhsiq\n\nStan, Julia\n\n4\n3.8e-16\n1.0\n2.9\n1.0\n20.0\n5.8\n0\n0\n29\n-0.92\n5.7e-16\n5.3e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nkidiq-kidscore_momiq\n\nStan, Julia\n\n3\n4.1e-16\n1.0\n9.2\n1.0\n17.0\n5.1\n0\n0\n20\n-0.92\n5.5e-16\n6.3e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nkidiq_with_mom_work-kidscore_interaction_c\n\nStan, Julia\n\n5\n4.3e-16\n1.0\n3.4\n1.0\n16.0\n6.3\n0\n0\n24\n0.0\n4.5e-16\n5.8e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nkidiq_with_mom_work-kidscore_interaction_c2\n\nStan, Julia\n\n5\n4.8e-16\n1.0\n3.5\n1.0\n16.0\n6.4\n0\n0\n24\n0.0\n3.8e-16\n4.3e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nkidiq_with_mom_work-kidscore_interaction_z\n\nStan, Julia\n\n5\n4.4e-16\n1.0\n3.2\n1.0\n16.0\n6.1\n0\n0\n24\n0.0\n4.4e-16\n4.3e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nkidiq_with_mom_work-kidscore_mom_work\n\nStan, Julia\n\n5\n4.0e-16\n1.0\n3.2\n1.0\n16.0\n6.1\n0\n0\n24\n0.0\n6.0e-16\n4.6e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nkilpisjarvi_mod-kilpisjarvi\n\nStan, Julia\n\n3\n2.0e-16\n1.0\n7.0\n1.0\n9.7\n4.1\n0\n0\n20\n-1.2\n2.0e-16\n2.3e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nloss_curves-losscurve_sislob\n\nStan, Julia\n\n15\n1.8e-16\n1.0\n1.6\n6.5\n4.2\n1.0\n3\n48\n68\n16.0\n1.9e-16\n1.8e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nlow_dim_gauss_mix-low_dim_gauss_mix\n\nStan, Julia\n\n5\n5.1e-16\n1.0\n2.2\n1.0\n1.6\n1.4\n4\n9\n38\n920.0\n3.9e-16\n2.8e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nlow_dim_gauss_mix_collapse-low_dim_gauss_mix_collapse\n\nStan, Julia\n\n5\n5.2e-16\n1.0\n2.1\n1.0\n1.8\n1.4\n1\n2\n14\n920.0\n4.6e-16\n3.0e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nlsat_data-lsat_model\n\nStan, Julia\n\n1006\n1.5e-16\n1.0\n1.4\n1.3\n2.9\n1.0\n15\n74\n195\n-28.0\n1.1e-15\n1.2e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nmcycle_gp-accel_gp\n\nStan, Julia\n\n66\n3.3e-16\n1.0\n2.5\n1.0\n3.7\n1.2\n12\n24\n716\n180.0\n3.8e-16\n3.7e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nmcycle_splines-accel_splines\n\nStan, Julia\n\n82\n6.2e-16\n1.0\n2.5\n1.0\n3.6\n1.3\n11\n22\n102\n190.0\n6.2e-16\n6.3e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nmesquite-logmesquite\n\nStan, Julia\n\n8\n1.5e-16\n1.0\n3.6\n14.0\n3.2\n1.0\n0\n84\n29\n0.0\n2.0e-16\n2.1e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nmesquite-logmesquite_logva\n\nStan, Julia\n\n5\n1.8e-16\n1.0\n2.9\n1.0\n13.0\n4.3\n0\n0\n24\n0.0\n2.3e-16\n2.4e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nmesquite-logmesquite_logvas\n\nStan, Julia\n\n8\n1.3e-16\n1.0\n3.5\n14.0\n2.6\n1.0\n0\n84\n24\n0.0\n2.4e-16\n2.2e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nmesquite-logmesquite_logvash\n\nStan, Julia\n\n7\n1.8e-16\n1.0\n3.6\n14.0\n3.5\n1.0\n0\n70\n29\n0.0\n2.3e-16\n2.3e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nmesquite-logmesquite_logvolume\n\nStan, Julia\n\n3\n1.6e-16\n1.0\n7.3\n1.0\n13.0\n3.7\n0\n0\n25\n0.0\n2.2e-16\n2.1e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nmesquite-mesquite\n\nStan, Julia\n\n8\n1.5e-16\n1.0\n3.5\n11.0\n3.3\n1.0\n0\n70\n29\n0.0\n1.8e-16\n1.7e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nnes1972-nes\n\nStan, Julia\n\n10\n6.4e-16\n1.0\n3.8\n6.1\n2.4\n1.0\n0\n88\n29\n0.0\n1.3e-15\n1.1e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nnes1976-nes\n\nStan, Julia\n\n10\n6.1e-16\n1.0\n3.8\n6.6\n2.2\n1.0\n0\n88\n29\n0.0\n9.8e-16\n1.2e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nnes1980-nes\n\nStan, Julia\n\n10\n4.4e-16\n1.0\n3.9\n6.2\n2.1\n1.0\n0\n88\n29\n0.0\n8.6e-16\n8.1e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nnes1984-nes\n\nStan, Julia\n\n10\n6.0e-16\n1.0\n3.8\n6.1\n2.2\n1.0\n0\n88\n29\n0.0\n1.1e-15\n1.1e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nnes1988-nes\n\nStan, Julia\n\n10\n5.1e-16\n1.0\n3.9\n6.0\n2.1\n1.0\n0\n88\n29\n4.5e-13\n1.1e-15\n1.0e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nnes1992-nes\n\nStan, Julia\n\n10\n6.4e-16\n1.0\n3.9\n6.2\n2.1\n1.0\n0\n88\n29\n-4.5e-13\n1.1e-15\n1.1e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nnes1996-nes\n\nStan, Julia\n\n10\n6.0e-16\n1.0\n3.8\n6.1\n2.1\n1.0\n0\n88\n29\n0.0\n1.0e-15\n1.0e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nnes2000-nes\n\nStan, Julia\n\n10\n3.0e-16\n1.0\n4.0\n6.1\n2.1\n1.0\n0\n56\n29\n0.0\n6.7e-16\n6.7e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nnes_logit_data-nes_logit_model\n\nStan, Julia\n\n2\n0.0\n1.0\n1.0\n2.2\n4.5\n1.0\n1\n2\n29\n0.0\n5.8e-15\n6.7e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nnormal_2-normal_mixture\n\nStan, Julia\n\n3\n1.0e-15\n1.0\n2.7\n1.0\n1.9\n1.8\n0\n0\n0\n910.0\n1.4e-16\n1.2e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nnormal_5-normal_mixture_k\n\nStan, Julia\n\n14\n7.9e-16\n1.0\n1.8\n3.1\n2.8\n1.0\n2\n10\n35761\n1600.0\n8.1e-16\n8.1e-16\n2.6.1\n0.13.30\n0.4.86\n\n\novarian-logistic_regression_rhs\n\nStan, Julia\n\n3075\n1.5e-13\n1.0\n1.3\n2.0\n4.1\n1.0\n3\n23\n58\n-1700.0\n8.0e-11\n8.7e-11\n2.6.1\n0.13.30\n0.4.86\n\n\npilots-pilots\n\nStan, Julia\n\n18\n0.0\n1.0\n2.3\n1.0\n7.3\n1.2\n2\n4\n67\n0.0\n1.7e-16\n1.6e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nprideprejudice_chapter-ldaK5\n\nStan, Julia\n\n7714\n5.5e-15\n1.0\n1.3\n2.8\n3.2\n1.0\n3\n24\n658743\n-3.5e-10\n5.2e-16\n5.4e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nprideprejudice_paragraph-ldaK5\n\nStan, Julia\n\n15570\n3.5e-15\n1.0\n1.4\n2.8\n2.7\n1.0\n4\n32\n694097\n2.9e-11\n3.0e-16\n3.1e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nprostate-logistic_regression_rhs\n\nStan, Julia\n\n11935\n4.4e-15\n1.0\n1.5\n1.6\n2.9\n1.0\n5\n27\n62\n-6800.0\n5.6e-15\n5.4e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_all-radon_county_intercept\n\nStan, Julia\n\n388\n4.5e-15\n1.0\n4.1\n1.0\n11.0\n12.0\n0\n0\n10\n11000.0\n6.6e-15\n1.2e-13\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_all-radon_hierarchical_intercept_centered\n\nStan, Julia\n\n391\n3.9e-15\n1.0\n4.6\n1.0\n12.0\n13.0\n0\n0\n28\n12000.0\n8.7e-15\n1.1e-13\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_all-radon_hierarchical_intercept_noncentered\n\nStan, Julia\n\n391\n3.2e-15\n1.0\n4.6\n1.0\n9.5\n11.0\n1\n2\n29\n12000.0\n5.3e-15\n9.7e-14\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_all-radon_partially_pooled_centered\n\nStan, Julia\n\n389\n3.6e-15\n1.0\n3.6\n1.0\n12.0\n11.0\n0\n0\n18\n12000.0\n8.5e-15\n1.6e-13\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_all-radon_partially_pooled_noncentered\n\nStan, Julia\n\n389\n3.5e-15\n1.0\n3.7\n1.0\n9.6\n11.0\n1\n2\n19\n12000.0\n6.9e-15\n1.2e-13\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_all-radon_pooled\n\nStan, Julia\n\n3\n1.4e-14\n1.0\n33.0\n1.0\n12.0\n20.0\n0\n0\n20\n12000.0\n1.7e-14\n1.9e-14\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_all-radon_variable_intercept_centered\n\nStan, Julia\n\n390\n3.6e-15\n1.0\n4.2\n1.0\n11.0\n12.0\n0\n0\n13\n12000.0\n7.9e-15\n1.1e-13\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_all-radon_variable_intercept_noncentered\n\nStan, Julia\n\n390\n4.0e-15\n1.0\n4.4\n1.0\n8.1\n11.0\n1\n2\n14\n12000.0\n6.3e-15\n9.7e-14\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_all-radon_variable_intercept_slope_centered\n\nStan, Julia\n\n777\n3.7e-15\n1.0\n4.0\n1.0\n11.0\n11.0\n0\n0\n26\n12000.0\n6.1e-15\n9.8e-14\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_all-radon_variable_intercept_slope_noncentered\n\nStan, Julia\n\n777\n5.1e-15\n1.0\n4.2\n1.0\n6.8\n8.6\n2\n4\n28\n12000.0\n5.5e-15\n8.2e-14\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_all-radon_variable_slope_centered\n\nStan, Julia\n\n390\n1.4e-14\n1.0\n4.3\n1.0\n11.0\n11.0\n0\n0\n13\n12000.0\n1.1e-14\n1.0e-13\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_all-radon_variable_slope_noncentered\n\nStan, Julia\n\n390\n1.6e-14\n1.0\n4.5\n1.0\n7.7\n10.0\n1\n2\n14\n12000.0\n8.1e-15\n7.8e-14\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_mn-radon_county_intercept\n\nStan, Julia\n\n87\n6.9e-16\n1.0\n4.0\n1.0\n9.8\n10.0\n0\n0\n10\n650.0\n1.3e-15\n1.0e-14\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_mn-radon_hierarchical_intercept_centered\n\nStan, Julia\n\n90\n7.3e-16\n1.0\n4.6\n1.0\n11.0\n11.0\n0\n0\n28\n840.0\n8.3e-16\n8.7e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_mn-radon_hierarchical_intercept_noncentered\n\nStan, Julia\n\n90\n6.4e-16\n1.0\n4.7\n1.0\n9.1\n9.6\n1\n2\n29\n840.0\n9.2e-16\n9.0e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_mn-radon_partially_pooled_centered\n\nStan, Julia\n\n88\n8.4e-16\n1.0\n3.5\n1.0\n12.0\n9.7\n0\n0\n18\n840.0\n1.0e-15\n7.1e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_mn-radon_partially_pooled_noncentered\n\nStan, Julia\n\n88\n8.3e-16\n1.0\n3.6\n1.0\n8.8\n8.7\n1\n2\n19\n840.0\n7.8e-16\n8.4e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_mn-radon_pooled\n\nStan, Julia\n\n3\n1.2e-15\n1.0\n28.0\n1.0\n11.0\n16.0\n0\n0\n20\n840.0\n1.5e-15\n1.3e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_mn-radon_variable_intercept_centered\n\nStan, Julia\n\n89\n7.1e-16\n1.0\n4.1\n1.0\n10.0\n11.0\n0\n0\n13\n840.0\n7.2e-16\n8.3e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_mn-radon_variable_intercept_noncentered\n\nStan, Julia\n\n89\n8.0e-16\n1.0\n4.4\n1.0\n7.4\n9.7\n1\n2\n14\n840.0\n1.0e-15\n8.2e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_mn-radon_variable_intercept_slope_centered\n\nStan, Julia\n\n175\n8.3e-16\n1.0\n3.9\n1.0\n11.0\n9.3\n0\n0\n26\n840.0\n7.4e-16\n8.3e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_mn-radon_variable_intercept_slope_noncentered\n\nStan, Julia\n\n175\n8.6e-16\n1.0\n4.3\n1.0\n6.5\n7.5\n2\n4\n28\n840.0\n8.1e-16\n8.0e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_mn-radon_variable_slope_centered\n\nStan, Julia\n\n89\n1.1e-15\n1.0\n4.2\n1.0\n10.0\n10.0\n0\n0\n13\n840.0\n9.7e-16\n6.5e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_mn-radon_variable_slope_noncentered\n\nStan, Julia\n\n89\n1.1e-15\n1.0\n4.5\n1.0\n7.0\n9.2\n1\n2\n14\n840.0\n9.0e-16\n7.0e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nradon_mod-radon_county\n\nStan, Julia\n\n389\n0.0\n1.0\n6.5\n1.0\n7.4\n2.3\n0\n0\n33\n0.0\n6.8e-16\n7.8e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nrats_data-rats_model\n\nStan, Julia\n\n65\n9.7e-17\n1.0\n3.4\n1.0\n9.8\n6.8\n0\n0\n31\n-9.2\n4.5e-16\n2.7e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nsblrc-blr\n\nStan, Julia\n\n6\n2.6e-16\n1.0\n4.1\n1.0\n7.7\n2.0\n1\n2\n40\n97.0\n2.3e-16\n2.2e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nsblri-blr\n\nStan, Julia\n\n6\n2.3e-16\n1.0\n4.4\n1.0\n7.8\n2.0\n1\n2\n40\n97.0\n2.5e-16\n2.0e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nscience_irt-grsm_latent_reg_irt\n\nStan, Julia\n\n408\n1.1e-16\n1.0\n1.7\n1.5\n2.6\n1.0\n10980\n30202\n49500\n15.0\n1.5e-16\n1.4e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nseeds_data-seeds_centered_model\n\nStan, Julia\n\n26\n1.5e-16\n1.0\n1.6\n1.0\n3.9\n1.0\n1\n2\n62\n0.0\n2.1e-16\n2.3e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nseeds_data-seeds_model\n\nStan, Julia\n\n26\n1.4e-16\n1.0\n1.3\n1.1\n4.2\n1.0\n0\n0\n58\n-35.0\n2.3e-16\n2.2e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nseeds_data-seeds_stanified_model\n\nStan, Julia\n\n26\n1.4e-16\n1.0\n1.4\n1.1\n4.1\n1.0\n0\n0\n58\n0.0\n2.5e-16\n2.2e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nsesame_data-sesame_one_pred_a\n\nStan, Julia\n\n3\n7.4e-16\n1.0\n9.4\n1.0\n12.0\n5.9\n0\n0\n20\n0.0\n1.7e-15\n1.3e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nsurgical_data-surgical_model\n\nStan, Julia\n\n14\n1.5e-16\n1.0\n2.1\n1.0\n2.7\n1.2\n0\n0\n53\n-14.0\n1.2e-16\n1.4e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nthree_docs1200-ldaK2\n\nStan, Julia\n\n7\n4.4e-16\n1.0\n1.5\n4.8\n4.8\n1.0\n2\n34\n24103\n0.0\n2.9e-16\n3.0e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nthree_men1-ldaK2\n\nStan, Julia\n\n502\n1.7e-15\n1.0\n1.4\n3.0\n4.4\n1.0\n2\n34\n100137\n2.2e-11\n5.6e-16\n5.8e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nthree_men2-ldaK2\n\nStan, Julia\n\n510\n1.6e-15\n1.0\n1.4\n3.0\n4.4\n1.0\n2\n34\n100281\n-7.3e-12\n4.1e-16\n4.0e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nthree_men3-ldaK2\n\nStan, Julia\n\n505\n1.7e-15\n1.0\n1.4\n3.1\n4.5\n1.0\n2\n34\n100191\n-7.3e-12\n5.0e-16\n4.6e-16\n2.6.1\n0.13.30\n0.4.86\n\n\ntraffic_accident_nyc-bym2_offset_only\n\nStan, Julia\n\n3845\n1.1e-15\n1.0\n2.6\n1.0\n5.6\n1.3\n6\n8\n79\n-0.65\n1.4e-15\n1.4e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nwells_data-wells_daae_c_model\n\nStan, Julia\n\n6\n0.0\n1.0\n1.0\n2.0\n2.7\n1.0\n2\n4\n31\n0.0\n9.1e-16\n9.3e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nwells_data-wells_dae_c_model\n\nStan, Julia\n\n5\n1.5e-16\n1.0\n1.0\n2.1\n2.6\n1.0\n2\n4\n31\n0.0\n9.3e-16\n9.9e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nwells_data-wells_dae_inter_model\n\nStan, Julia\n\n7\n1.5e-16\n1.0\n1.0\n2.0\n2.6\n1.0\n2\n4\n31\n0.0\n9.8e-16\n1.1e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nwells_data-wells_dae_model\n\nStan, Julia\n\n4\n0.0\n1.0\n1.0\n2.0\n2.4\n1.0\n2\n4\n31\n0.0\n7.2e-16\n8.5e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nwells_data-wells_dist\n\nStan, Julia\n\n2\n2.6e-13\n1.0\n1.7\n1.2\n1.5\n1.0\n0\n0\n18\n-4.6e-8\n4.2e-11\n3.6e-11\n2.6.1\n0.13.30\n0.4.86\n\n\nwells_data-wells_dist100_model\n\nStan, Julia\n\n2\n0.0\n1.0\n1.0\n2.0\n2.5\n1.0\n2\n4\n31\n0.0\n9.9e-16\n8.8e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nwells_data-wells_dist100ars_model\n\nStan, Julia\n\n3\n0.0\n1.0\n1.0\n2.0\n2.5\n1.0\n2\n4\n31\n0.0\n8.9e-16\n7.8e-16\n2.6.1\n0.13.30\n0.4.86\n\n\nwells_data-wells_interaction_c_model\n\nStan, Julia\n\n4\n1.5e-16\n1.0\n1.0\n2.0\n2.6\n1.0\n2\n4\n31\n0.0\n1.1e-15\n1.0e-15\n2.6.1\n0.13.30\n0.4.86\n\n\nwells_data-wells_interaction_model\n\nStan, Julia\n\n4\n0.0\n1.0\n1.0\n2.0\n2.5\n1.0\n2\n4\n31\n0.0\n8.6e-16\n8.7e-16\n2.6.1\n0.13.30\n0.4.86"
  },
  {
    "objectID": "slic/index.html",
    "href": "slic/index.html",
    "title": "The @slic macro",
    "section": "",
    "text": "Code up your model once - simplify, extend and use in multiple contexts efficiently.\n\n\nThe aim is to make it easy to iterate on the statistical model.\n\n\nLike Turing.jl’s submodels (where’s the documentation for this?) or SlicStan’s functions that declare parameters (first example in the paper).\nGetting tired of always coding up the same hierarchical priors? @slic will support reusing model components:\nhierarchical_prior = @slic begin\n    location ~ std_normal()\n    scale ~ std_lognormal()\n    x ~ normal(location, scale; n)\n    return x\nend\nhierarchical_model = @slic begin \n    obs_location ~ hierarchical_prior(;n)\n    obs_scale ~ std_lognormal()\n    obs ~ normal(obs_location, obs_scale)\nend\nAlternative syntax proposals are appreciated!\n\n\n\nEver wanted to switch out one prior for another, but didn’t want to implement this functionality when coding up your first exploratory model? @slic will support switching out arbitrary model components:\n\"My first, exploratory model to check that things work\"\npooled_model = @slic begin \n    obs_location ~ std_normal()\n    obs_scale ~ std_lognormal()\n    obs ~ normal(obs_location, obs_scale)\nend\n\"Equivalent to the previous implementation of `hierarchical_model` above\"\nhierarchical_model = pooled_model(quote \n    obs_location ~ hierarchical_prior(;n)\nend)\nAlternative syntax proposals are appreciated!\n\n\n\nLike Turing.jl’s Conditioning or this Stan PR.\nEver wanted to pin hierarchical scale parameters? @slic will support pinning arbitrary model components:\n\"The `hierarchical_model` with the hierarchical scale parameter fixed to 1.\"\nsemihierarchical_model = hierarchical_model(;obs_location_scale=1.)\nTuring’s Deconditioning could also easily be supported, but as always syntax proposals are appreciated!\n\n\n\nGetting tired of reimplementing parts of your model to perform e.g. leave-one-subject-out cross-validation? @slic will support automatic model rewrites to perform e.g. automatic leave-one-subject-out cross-validation from the same model implementation that you have used to sample from the posterior:\nhierarchical_prior = @slic begin\n    location ~ std_normal()\n    scale ~ std_lognormal()\n    x ~ normal(location, scale; n)\n    return x\nend\nhierarchical_model = @slic begin \n    n = maximum(subject_idx)\n    obs_intercept ~ hierarchical_prior(;n)\n    obs_slope ~ hierarchical_prior(;n)\n    obs_location = obs_intercept + dot(covariates, obs_slope)\n    obs_scale ~ std_lognormal()\n    obs[subject_idx] ~ normal(obs_location, obs_scale)\nend\nsamples = nuts_draws(hierarchical_model(;subject_idx, covariates, obs))\n\"Alternative 1: Constructing just the CV model\"\ncv_model = cv(\n    hierarchical_model; \n    subject_idx=held_out_subject_idx, \n    covariates=held_out_covariates, \n    obs=held_out_obs\n)\n\"Alternative 2: Constructing the CV model and computing the necessary quantities\"\ncv_info = cv(\n    samples; \n    subject_idx=held_out_subject_idx, \n    covariates=held_out_covariates, \n    obs=held_out_obs\n)\nIn the above example, the hierarchical parameters will be fixed and reused from samples, while the subject specific parameters will be automatically resampled inedependently for each draw to compute the likelihood. Determining which parameters get fixed/resued and which get resampled is done by tracing through the model.\nAlternative syntax proposals are appreciated!\n\n\n\n\nThe aim is to make it easy to write computationally efficient code.\n\n\nLike SlicStan, @slic will support automatically determining whether model components are\n\ndata - passed to the model,\ntransformed data - need to be computed only once per model instantiation/conditioning/deconditioning,\nparameters - contribute to the posterior (MCMC-)dimension and potentially need to be transformed appropriately,\ntransformed parameters - have to be computed every gradient evaluation, because they do affect the likelihood,\ngenerated quantities - have to be computed only once per sample and can be sampled independently, because they do not affect the likelihood.\n\nThis allows for the natural model specification in a single place, while not sacrificing any performance.\nIn the below model,\n\nif we do not specify anything, every model component will be sampled independently,\nif we specify only obs_location or obs_scale, every other model component will still be sampled independently, and\nif we specify only obs, both obs_location and obs_scale become parameters, and obs_likelihood and obs_prediction get automatically added as generated quantities.\n\n@slic begin \n    obs_location ~ std_normal()\n    obs_scale ~ std_lognormal()\n    obs ~ normal(obs_location, obs_scale)\nend\n\n\n\nThe most efficient way to compute intermediate functional quantities will depend on the type, shape and potentially activity of the arguments passed to the function. We provide efficient primitives which explot this, e.g.:\nnormal_lpdf(y::AbstractVector, location::AbstractVector, scale::AbstractVector) = \"Do something which requires computing log.(scale).\"\nnormal_lpdf(y::AbstractVector, location::AbstractVector, scale::Real) = \"Do something which only requires computing log(scale) once.\"\n\n\n\nLike Stan, @slic models will use matrix expressions to keep the memory footprint low. This will (should) be more efficient than using something like a bump allocator and just-allocate-and-reuse away.\n\n\n\n\nThe aim is to make it easy to write correct code.\n\n\nLike Turing.jl’s unified way of defining parameters, unlike Stan’s split-across-blocks way of defining parameters.\n\n\n\nLike Turing.jl: a parameter’s constraints are automatically inferred from the support of its prior - but can of course be adjusted:\n@slic begin \n    \"Unconstrained\"\n    obs_intercept ~ std_normal()\n    \"Constrained to be positive\"\n    obs_scale ~ std_lognormal()\n    \"Constrained to be positive\"\n    obs_slope ~ std_normal(;lower=0.)\n    obs ~ normal(obs_intercept + obs_slope * x, obs_scale)\nend\n\n\n\nLike Turing.jl: the type of any model expression can be specified, but it need not be specified.\n\n\n\nWe will support tracing through a lot of common Julia syntax, like broadcasting, generators, maps and do blocks.\nThis is a big benefit compared to e.g. Stan. Julia\n\nhas a unified way of handling higher-dimensional arrays - no funny array [] vector or matrix business,\nhas a unified way of handling broadcasting - no “.* is the only broadcasted operation allowed, and only for specific pairs of arguments”,\nsupports higher-order functions - map, filter, reduce are our friends,\nsupports generators - [f(i) for i in something if condition(i)] is nice,\nhas a richer type, template and dispatch system,\nallows easy interoperability and extendability.\n\n\n\n\nWhile tracing will be limited to a subset of Julia syntax, “unknown” user defined functions will simply not be traced recursively. We will not allow these opaque functions to introduce model parameters, and we will “assume the worst” for the activity analysis.\n\n\n\n\nThe aim is to make it easy to extend @slic’s functionality.\n\n\nThe data/parameter/generated-quantities activity analysis comes out of two passes through the model (one forward, one reverse).\nThe cross-validation activity analysis comes out of another pass through the model.\nThere is nothing stopping us from allowing additional or alternative passes, and there should not be anything stopping you from implementing these custom passes.\nOne possible custom pass would e.g. translate the @slic model to a Stan program."
  },
  {
    "objectID": "slic/index.html#modularity",
    "href": "slic/index.html#modularity",
    "title": "The @slic macro",
    "section": "",
    "text": "The aim is to make it easy to iterate on the statistical model.\n\n\nLike Turing.jl’s submodels (where’s the documentation for this?) or SlicStan’s functions that declare parameters (first example in the paper).\nGetting tired of always coding up the same hierarchical priors? @slic will support reusing model components:\nhierarchical_prior = @slic begin\n    location ~ std_normal()\n    scale ~ std_lognormal()\n    x ~ normal(location, scale; n)\n    return x\nend\nhierarchical_model = @slic begin \n    obs_location ~ hierarchical_prior(;n)\n    obs_scale ~ std_lognormal()\n    obs ~ normal(obs_location, obs_scale)\nend\nAlternative syntax proposals are appreciated!\n\n\n\nEver wanted to switch out one prior for another, but didn’t want to implement this functionality when coding up your first exploratory model? @slic will support switching out arbitrary model components:\n\"My first, exploratory model to check that things work\"\npooled_model = @slic begin \n    obs_location ~ std_normal()\n    obs_scale ~ std_lognormal()\n    obs ~ normal(obs_location, obs_scale)\nend\n\"Equivalent to the previous implementation of `hierarchical_model` above\"\nhierarchical_model = pooled_model(quote \n    obs_location ~ hierarchical_prior(;n)\nend)\nAlternative syntax proposals are appreciated!\n\n\n\nLike Turing.jl’s Conditioning or this Stan PR.\nEver wanted to pin hierarchical scale parameters? @slic will support pinning arbitrary model components:\n\"The `hierarchical_model` with the hierarchical scale parameter fixed to 1.\"\nsemihierarchical_model = hierarchical_model(;obs_location_scale=1.)\nTuring’s Deconditioning could also easily be supported, but as always syntax proposals are appreciated!\n\n\n\nGetting tired of reimplementing parts of your model to perform e.g. leave-one-subject-out cross-validation? @slic will support automatic model rewrites to perform e.g. automatic leave-one-subject-out cross-validation from the same model implementation that you have used to sample from the posterior:\nhierarchical_prior = @slic begin\n    location ~ std_normal()\n    scale ~ std_lognormal()\n    x ~ normal(location, scale; n)\n    return x\nend\nhierarchical_model = @slic begin \n    n = maximum(subject_idx)\n    obs_intercept ~ hierarchical_prior(;n)\n    obs_slope ~ hierarchical_prior(;n)\n    obs_location = obs_intercept + dot(covariates, obs_slope)\n    obs_scale ~ std_lognormal()\n    obs[subject_idx] ~ normal(obs_location, obs_scale)\nend\nsamples = nuts_draws(hierarchical_model(;subject_idx, covariates, obs))\n\"Alternative 1: Constructing just the CV model\"\ncv_model = cv(\n    hierarchical_model; \n    subject_idx=held_out_subject_idx, \n    covariates=held_out_covariates, \n    obs=held_out_obs\n)\n\"Alternative 2: Constructing the CV model and computing the necessary quantities\"\ncv_info = cv(\n    samples; \n    subject_idx=held_out_subject_idx, \n    covariates=held_out_covariates, \n    obs=held_out_obs\n)\nIn the above example, the hierarchical parameters will be fixed and reused from samples, while the subject specific parameters will be automatically resampled inedependently for each draw to compute the likelihood. Determining which parameters get fixed/resued and which get resampled is done by tracing through the model.\nAlternative syntax proposals are appreciated!"
  },
  {
    "objectID": "slic/index.html#computational-efficiency",
    "href": "slic/index.html#computational-efficiency",
    "title": "The @slic macro",
    "section": "",
    "text": "The aim is to make it easy to write computationally efficient code.\n\n\nLike SlicStan, @slic will support automatically determining whether model components are\n\ndata - passed to the model,\ntransformed data - need to be computed only once per model instantiation/conditioning/deconditioning,\nparameters - contribute to the posterior (MCMC-)dimension and potentially need to be transformed appropriately,\ntransformed parameters - have to be computed every gradient evaluation, because they do affect the likelihood,\ngenerated quantities - have to be computed only once per sample and can be sampled independently, because they do not affect the likelihood.\n\nThis allows for the natural model specification in a single place, while not sacrificing any performance.\nIn the below model,\n\nif we do not specify anything, every model component will be sampled independently,\nif we specify only obs_location or obs_scale, every other model component will still be sampled independently, and\nif we specify only obs, both obs_location and obs_scale become parameters, and obs_likelihood and obs_prediction get automatically added as generated quantities.\n\n@slic begin \n    obs_location ~ std_normal()\n    obs_scale ~ std_lognormal()\n    obs ~ normal(obs_location, obs_scale)\nend\n\n\n\nThe most efficient way to compute intermediate functional quantities will depend on the type, shape and potentially activity of the arguments passed to the function. We provide efficient primitives which explot this, e.g.:\nnormal_lpdf(y::AbstractVector, location::AbstractVector, scale::AbstractVector) = \"Do something which requires computing log.(scale).\"\nnormal_lpdf(y::AbstractVector, location::AbstractVector, scale::Real) = \"Do something which only requires computing log(scale) once.\"\n\n\n\nLike Stan, @slic models will use matrix expressions to keep the memory footprint low. This will (should) be more efficient than using something like a bump allocator and just-allocate-and-reuse away."
  },
  {
    "objectID": "slic/index.html#expressiveness",
    "href": "slic/index.html#expressiveness",
    "title": "The @slic macro",
    "section": "",
    "text": "The aim is to make it easy to write correct code.\n\n\nLike Turing.jl’s unified way of defining parameters, unlike Stan’s split-across-blocks way of defining parameters.\n\n\n\nLike Turing.jl: a parameter’s constraints are automatically inferred from the support of its prior - but can of course be adjusted:\n@slic begin \n    \"Unconstrained\"\n    obs_intercept ~ std_normal()\n    \"Constrained to be positive\"\n    obs_scale ~ std_lognormal()\n    \"Constrained to be positive\"\n    obs_slope ~ std_normal(;lower=0.)\n    obs ~ normal(obs_intercept + obs_slope * x, obs_scale)\nend\n\n\n\nLike Turing.jl: the type of any model expression can be specified, but it need not be specified.\n\n\n\nWe will support tracing through a lot of common Julia syntax, like broadcasting, generators, maps and do blocks.\nThis is a big benefit compared to e.g. Stan. Julia\n\nhas a unified way of handling higher-dimensional arrays - no funny array [] vector or matrix business,\nhas a unified way of handling broadcasting - no “.* is the only broadcasted operation allowed, and only for specific pairs of arguments”,\nsupports higher-order functions - map, filter, reduce are our friends,\nsupports generators - [f(i) for i in something if condition(i)] is nice,\nhas a richer type, template and dispatch system,\nallows easy interoperability and extendability.\n\n\n\n\nWhile tracing will be limited to a subset of Julia syntax, “unknown” user defined functions will simply not be traced recursively. We will not allow these opaque functions to introduce model parameters, and we will “assume the worst” for the activity analysis."
  },
  {
    "objectID": "slic/index.html#extendability",
    "href": "slic/index.html#extendability",
    "title": "The @slic macro",
    "section": "",
    "text": "The aim is to make it easy to extend @slic’s functionality.\n\n\nThe data/parameter/generated-quantities activity analysis comes out of two passes through the model (one forward, one reverse).\nThe cross-validation activity analysis comes out of another pass through the model.\nThere is nothing stopping us from allowing additional or alternative passes, and there should not be anything stopping you from implementing these custom passes.\nOne possible custom pass would e.g. translate the @slic model to a Stan program."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "index.html#differences-in-the-returned-log-density",
    "href": "index.html#differences-in-the-returned-log-density",
    "title": "",
    "section": "Differences in the returned log-density",
    "text": "Differences in the returned log-density\nStan’s default “sampling statement” (e.g. y ~ normal(mu, sigma);) automatically drops constant terms (unless configured differently), see https://mc-stan.org/docs/reference-manual/statements.html#log-probability-increment-vs.-distribution-statement. Constant terms are terms which do not depend on model parameters, and this package’s macros and functions currently do not try to figure out which terms do not depend on model parameters, and as such we never drop them. This may lead to (constant) differences in the computed log-densities from the Stan and Julia implementations."
  },
  {
    "objectID": "index.html#some-models-are-not-implemented-yet-or-may-have-smaller-or-bigger-errors",
    "href": "index.html#some-models-are-not-implemented-yet-or-may-have-smaller-or-bigger-errors",
    "title": "",
    "section": "Some models are not implemented yet, or may have smaller or bigger errors",
    "text": "Some models are not implemented yet, or may have smaller or bigger errors\nI’ve implemented many of the models, but I haven’t implemented all of them, and I probably have made some mistakes in implementing some of them."
  },
  {
    "objectID": "index.html#some-models-may-have-been-implemented-suboptimally",
    "href": "index.html#some-models-may-have-been-implemented-suboptimally",
    "title": "",
    "section": "Some models may have been implemented suboptimally",
    "text": "Some models may have been implemented suboptimally\nJust that."
  }
]