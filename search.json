[
  {
    "objectID": "implementations.html",
    "href": "implementations.html",
    "title": "Julia posteriordb implementations",
    "section": "",
    "text": "See the PosteriorDB.jl extension at https://github.com/nsiccha/StanBlocks.jl/blob/main/ext/PosteriorDBExt.jl, also included below:\nmodule PosteriorDBExt\n\nimport PosteriorDB\nimport StanBlocks\nimport StanBlocks: julia_implementation, @stan, @parameters, @transformed_parameters, @model, @broadcasted\nimport StanBlocks: bernoulli_lpmf, binomial_lpmf, log_sum_exp, logit, binomial_logit_lpmf, bernoulli_logit_lpmf, inv_logit, log_inv_logit, rep_vector, square, normal_lpdf, sd, multi_normal_lpdf, student_t_lpdf, gp_exp_quad_cov, log_mix, append_row, append_col, pow, diag_matrix, normal_id_glm_lpdf, rep_matrix, rep_row_vector, cholesky_decompose, dot_self, cumulative_sum, softmax, log1m_inv_logit, matrix_constrain, integrate_ode_rk45, integrate_ode_bdf, poisson_lpdf, nothrow_log, categorical_lpmf, dirichlet_lpdf, exponential_lpdf, sub_col, stan_tail, dot_product, segment, inv_gamma_lpdf, diag_pre_multiply, multi_normal_cholesky_lpdf, logistic_lpdf\nusing Statistics, LinearAlgebra\n\n@inline PosteriorDB.implementation(model::PosteriorDB.Model, ::Val{:stan_blocks}) = julia_implementation(Val(Symbol(PosteriorDB.name(model))))\n# @inline StanBlocks.stan_implementation(posterior::PosteriorDB.Posterior) = StanProblem(\n#     PosteriorDB.path(PosteriorDB.implementation(PosteriorDB.model(posterior), \"stan\")), \n#     PosteriorDB.load(PosteriorDB.dataset(posterior), String);\n#     nan_on_error=true\n# )\n\njulia_implementation(posterior::PosteriorDB.Posterior) = julia_implementation(\n    Val(Symbol(PosteriorDB.name(PosteriorDB.model(posterior))));\n    Dict([Symbol(k)=&gt;v for (k, v) in pairs(PosteriorDB.load(PosteriorDB.dataset(posterior)))])...\n)\n\njulia_implementation(::Val{:earn_height}; N, earn, height, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0.)\n        end\n        @model begin\n            earn ~ normal(@broadcasted(beta[1] + beta[2] * height), sigma);\n        end\n    end\nend\n\njulia_implementation(::Val{:wells_dist}; N, switched, dist, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n        end\n        @model begin\n            switched ~ bernoulli_logit(@broadcasted(beta[1] + beta[2] * dist));\n        end\n    end\nend\njulia_implementation(::Val{:sesame_one_pred_a}; N, encouraged, watched, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0.)\n        end\n        @model begin\n            watched ~ normal(@broadcasted(beta[1] + beta[2] * encouraged), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:Rate_1_model}; n, k, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0.,upper=1.)\n        end\n        @model begin\n            theta ~ beta(1, 1)\n            k ~ binomial(n, theta)\n        end\n    end\nend\njulia_implementation(::Val{:nes_logit_model}; N, income, vote, kwargs...) = begin \n    X = reshape(income, (N, 1))\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[1]\n        end\n        @model begin\n            vote ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_momiq}; N, kid_score, mom_iq, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0)\n        end\n        @model begin\n            sigma ~ cauchy(0, 2.5);\n            kid_score ~ normal(@broadcasted(beta[1] + beta[2] * mom_iq), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_momhs}; N, kid_score, mom_hs, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0)\n        end\n        @model begin\n            sigma ~ cauchy(0, 2.5);\n            kid_score ~ normal(@broadcasted(beta[1] + beta[2] * mom_hs), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:logearn_height}; N, earn, height, kwargs...) = begin \n    log_earn = @. log(earn)\n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0)\n        end\n        @model begin\n            log_earn ~ normal(@broadcasted(beta[1] + beta[2] * height), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:blr}; N, D, X, y, kwargs...) = begin \n    @assert size(X) == (N,D)\n    @stan begin \n        @parameters begin\n            beta::vector[D]\n            sigma::real(lower=0)\n        end\n        @model begin\n            target += normal_lpdf(beta, 0, 10);\n            target += normal_lpdf(sigma, 0, 10);\n            target += normal_lpdf(y, X * beta, sigma);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dist100_model}; N, switched, dist, kwargs...) = begin \n    dist100 = @. dist / 100.\n    X = reshape(dist100, (N, 1))\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[1]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:Rate_3_model}; n1, n2, k1, k2, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0,upper=1)\n        end\n        @model begin\n            theta ~ beta(1, 1)\n            k1 ~ binomial(n1, theta)\n            k2 ~ binomial(n2, theta)\n        end\n    end\nend\njulia_implementation(::Val{:logearn_height_male}; N, earn, height, male, kwargs...) = begin \n    log_earn = @. log(earn)\n    @stan begin \n        @parameters begin\n            beta::vector[3]\n            sigma::real(lower=0)\n        end\n        @model begin\n            log_earn ~ normal(@broadcasted(beta[1] + beta[2] * height + beta[3] * male), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_momhsiq}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[3]\n            sigma::real(lower=0)\n        end\n        @model begin\n            sigma ~ cauchy(0, 2.5);\n            kid_score ~ normal(@broadcasted(beta[1] + beta[2] * mom_hs + beta[3] * mom_iq), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:log10earn_height}; N, earn, height, kwargs...) = begin \n    log10_earn = @. log10(earn)\n    @stan begin \n        @parameters begin\n            beta::vector[2]\n            sigma::real(lower=0)\n        end\n        @model begin\n            log10_earn ~ normal(@broadcasted(beta[1] + beta[2] * height), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dist100ars_model}; N, switched, dist, arsenic, kwargs...) = begin \n    dist100 = @. dist / 100.\n    X = hcat(dist100, arsenic)\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[2]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:low_dim_gauss_mix_collapse}; N, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::vector[2]\n            sigma::vector(lower=0)[2]\n            theta::real(lower=0,upper=1)\n        end\n        @model begin\n            sigma ~ normal(0, 2);\n            mu ~ normal(0, 2);\n            theta ~ beta(5, 5);\n            for n in 1:N\n              target += log_mix(theta, normal_lpdf(y[n], mu[1], sigma[1]),\n                                normal_lpdf(y[n], mu[2], sigma[2]));\n            end\n        end\n    end\nend\njulia_implementation(::Val{:normal_mixture_k}; K, N, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::simplex[K]\n            mu::vector[K]\n            sigma::vector(lower=0.,upper=10.)[K]\n        end\n        @model begin\n            mu ~ normal(0., 10.);\n            for n in 1:N\n                ps = @broadcasted(log(theta) + normal_lpdf(y[n], mu, sigma))\n                target += log_sum_exp(ps);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:low_dim_gauss_mix}; N, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::ordered[2]\n            sigma::vector(lower=0)[2]\n            theta::real(lower=0,upper=1)\n        end\n        @model begin\n            sigma ~ normal(0, 2);\n            mu ~ normal(0, 2);\n            theta ~ beta(5, 5);\n            for n in 1:N\n                target += log_mix(theta, normal_lpdf(y[n], mu[1], sigma[1]),\n                                normal_lpdf(y[n], mu[2], sigma[2]));\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_county}; N, J, county, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            a::vector[J]\n            mu_a::real\n            sigma_a::real(lower=0, upper=100)\n            sigma_y::real(lower=0, upper=100)\n        end\n        @model @views begin\n            y_hat = a[county]\n            \n            mu_a ~ normal(0, 1);\n            a ~ normal(mu_a, sigma_a);\n            y ~ normal(y_hat, sigma_y);\n        end\n    end\nend\njulia_implementation(::Val{:logearn_logheight_male}; N, earn, height, male, kwargs...) = begin \n    log_earn = @. log(earn)\n    log_height = @. log(height)\n    @stan begin \n        @parameters begin\n            beta::vector[3]\n            sigma::real(lower=0)\n        end\n        @model begin\n            log_earn ~ normal(@broadcasted(beta[1] + beta[2] * log_height + beta[3] * male), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dae_model}; N, switched, dist, arsenic, educ, kwargs...) = begin \n    dist100 = @. dist / 100.\n    educ4 = @. educ / 4.\n    X = hcat(dist100, arsenic, educ4)\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[3]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:arK}; K, T, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[K]\n            sigma::real(lower=0)\n        end\n        @model begin\n            alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            sigma ~ cauchy(0, 2.5);\n            for t in K+1:T\n                mu = alpha\n                for k in 1:K\n                    mu += beta[k] * y[t-k]\n                end\n                y[t] ~ normal(mu, sigma)\n            end\n        end\n    end\nend\njulia_implementation(::Val{:wells_interaction_model}; N, switched, dist, arsenic, kwargs...) = begin \n    dist100 = @. dist / 100.\n    inter = @. dist100 * arsenic\n    X = hcat(dist100, arsenic, inter)\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[3]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:radon_pooled}; N, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::real\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_y ~ normal(0, 1);\n            alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n\n            log_radon ~ normal(@broadcasted(alpha + beta * floor_measure), sigma_y)\n        end\n    end\nend\njulia_implementation(::Val{:logearn_interaction}; N, earn, height, male, kwargs...) = begin \n        log_earn = @. log(earn)\n        inter = @. height * male\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_earn ~ normal(@broadcasted(beta[1] + beta[2] * height + beta[3] * male + beta[4] * inter), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:logmesquite_logvolume}; N, weight, diam1, diam2, canopy_height, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_canopy_volume = @. log(diam1 * diam2 * canopy_height);\n@stan begin \n            @parameters begin\n                beta::vector[2]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_canopy_volume), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:garch11}; T, y, sigma1, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::real\n            alpha0::real(lower=0.)\n            alpha1::real(lower=0., upper=1.)\n            beta1::real(lower=0., upper=1. - alpha1)\n        end\n        @model begin\n            sigma = sigma1\n            y[1] ~ normal(mu, sigma)\n            for t in 2:T\n                sigma = sqrt(alpha0 + alpha1 * square(y[t - 1] - mu) + beta1 * square(sigma))\n                y[t] ~ normal(mu, sigma)\n            end\n        end\n    end\nend\njulia_implementation(::Val{:eight_schools_centered}; J, y, sigma, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::vector[J]\n            mu::real\n            tau::real(lower=0)\n        end\n        @model begin\n            tau ~ cauchy(0, 5);\n            theta ~ normal(mu, tau);\n            y ~ normal(theta, sigma);\n            mu ~ normal(0, 5);\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_interaction}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n        inter = @. mom_hs * mom_iq;\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                sigma ~ cauchy(0, 2.5);\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * mom_hs + beta[3] * mom_iq + beta[4] * inter), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:mesquite}; N, weight, diam1, diam2, canopy_height, total_height, density, group, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[7]\n            sigma::real(lower=0)\n        end\n        @model begin\n            weight ~ normal(@broadcasted(beta[1] + beta[2] * diam1 + beta[3] * diam2\n            + beta[4] * canopy_height + beta[5] * total_height\n            + beta[6] * density + beta[7] * group), sigma);\n        end\n    end\nend\njulia_implementation(::Val{:gp_regr}; N, x, y, kwargs...) = begin \n@stan begin \n            @parameters begin\n                rho::real(lower=0)\n                alpha::real(lower=0)\n                sigma::real(lower=0)\n            end\n            @model begin\n                cov = gp_exp_quad_cov(x, alpha, rho) + diag_matrix(rep_vector(sigma, N));\n                # L_cov = cholesky_decompose(cov);\n  \n                rho ~ gamma(25, 4);\n                alpha ~ normal(0, 2);\n                sigma ~ normal(0, 1);\n                \n                y ~ multi_normal(rep_vector(0., N), cov);\n                # Think about how to do this\n                # y ~ multi_normal_cholesky(rep_vector(0, N), L_cov);\n            end\n    end\nend\njulia_implementation(::Val{:kidscore_mom_work}; N, kid_score, mom_work, kwargs...) = begin \n        work2 = @. Float64(mom_work == 2)\n        work3 = @. Float64(mom_work == 3)\n        work4 = @. Float64(mom_work == 4)\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * work2 + beta[3] * work3\n                + beta[4] * work4), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:Rate_2_model}; n1, n2, k1, k2, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta1::real(lower=0,upper=1)\n            theta2::real(lower=0,upper=1)\n        end\n        delta = theta1 - theta2\n        @model begin\n            theta1 ~ beta(1, 1)\n            theta2 ~ beta(1, 1)\n            k1 ~ binomial(n1, theta1)\n            k2 ~ binomial(n2, theta2)\n        end\n    end\nend\njulia_implementation(::Val{:wells_interaction_c_model}; N, switched, dist, arsenic, kwargs...) = begin \n        c_dist100 = @. (dist - $mean(dist)) / 100.0;\n        c_arsenic = @. arsenic - $mean(arsenic);\n        inter = @. c_dist100 * c_arsenic\n        X = hcat(c_dist100, c_arsenic, inter)\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::vector[3]\n            end\n            @model begin\n                switched ~ bernoulli_logit_glm(X, alpha, beta);\n            end\n        end\nend\njulia_implementation(::Val{:radon_county_intercept}; N, J, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[J]\n            beta::real\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_y ~ normal(0, 1);\n            alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            for n in 1:N\n                mu = alpha[county_idx[n]] + beta * floor_measure[n];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_interaction_c}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n        c_mom_hs = @. mom_hs - $mean(mom_hs);\n        c_mom_iq = @. mom_iq - $mean(mom_iq);\n        inter = @. c_mom_hs * c_mom_iq;\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * c_mom_hs + beta[3] * c_mom_iq + beta[4] * inter), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:kidscore_interaction_c2}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n        c_mom_hs = @. mom_hs - .5;\n        c_mom_iq = @. mom_iq - 100;\n        inter = @. c_mom_hs * c_mom_iq;\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * c_mom_hs + beta[3] * c_mom_iq + beta[4] * inter), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:gp_pois_regr}; N, x, k, kwargs...) = begin \n        nugget = diag_matrix(rep_vector(1e-10, N))\n@stan begin \n            @parameters begin\n                rho::real(lower=0)\n                alpha::real(lower=0)\n                f_tilde::vector[N]\n            end\n            @transformed_parameters begin \n                cov = gp_exp_quad_cov(x, alpha, rho)\n                cov .+= nugget;\n                L_cov = cholesky_decompose(cov);\n                f = L_cov * f_tilde;\n            end\n            @model begin\n                rho ~ gamma(25, 4);\n                alpha ~ normal(0, 2);\n                f_tilde ~ normal(0, 1);\n                \n                k ~ poisson_log(f);\n            end\n    end\nend\njulia_implementation(::Val{:surgical_model}; N, r, n, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::real\n            sigmasq::real(lower=0)\n            b::vector[N]\n        end\n        @transformed_parameters begin \n            sigma = sqrt(sigmasq)\n            p = @broadcasted inv_logit(b)\n        end\n        @model begin\n            mu ~ normal(0.0, 1000.0);\n            sigmasq ~ inv_gamma(0.001, 0.001);\n            b ~ normal(mu, sigma);\n            r ~ binomial_logit(n, b);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dae_c_model}; N, switched, dist, arsenic, educ, kwargs...) = begin \n    c_dist100 = @. (dist - $mean(dist)) / 100.0;\n    c_arsenic = @. arsenic - $mean(arsenic);\n    da_inter = @. c_dist100 * c_arsenic;\n    educ4 = @. educ / 4.\n    X = hcat(c_dist100, c_arsenic, da_inter, educ4)\n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[4]\n        end\n        @model begin\n            switched ~ bernoulli_logit_glm(X, alpha, beta);\n        end\n    end\nend\njulia_implementation(::Val{:Rate_4_model}; n, k, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0,upper=1)\n            thetaprior::real(lower=0,upper=1)\n        end\n        @model begin\n            theta ~ beta(1, 1);\n            thetaprior ~ beta(1, 1);\n            k ~ binomial(n, theta);\n        end\n    end\nend\njulia_implementation(::Val{:logearn_interaction_z}; N, earn, height, male, kwargs...) = begin \n        log_earn = @. log(earn)\n        z_height = @. (height - $mean(height)) / $sd(height);\n        inter = @. z_height * male\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_earn ~ normal(@broadcasted(beta[1] + beta[2] * z_height + beta[3] * male + beta[4] * inter), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:normal_mixture}; N, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0,upper=1)\n            mu::vector[2]\n        end\n        @model begin\n            theta ~ uniform(0, 1); \n            for k in 1:2\n                mu[k] ~ normal(0, 10);\n            end\n            for n in 1:N\n                target += log_mix(theta, normal_lpdf(y[n], mu[1], 1.0),\n                                normal_lpdf(y[n], mu[2], 1.0));\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_partially_pooled_centered}; N, J, county_idx, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[J]\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n\n            alpha ~ normal(mu_alpha, sigma_alpha);\n            for n in 1:N\n                mu = alpha[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:kidscore_interaction_z}; N, kid_score, mom_iq, mom_hs, kwargs...) = begin \n        c_mom_hs = @. (mom_hs - $mean(mom_hs)) / (2 * $sd(mom_hs));\n        c_mom_iq = @. (mom_iq - $mean(mom_iq)) / (2 * $sd(mom_iq));\n        inter = @. c_mom_hs * c_mom_iq;\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                kid_score ~ normal(@broadcasted(beta[1] + beta[2] * c_mom_hs + beta[3] * c_mom_iq + beta[4] * inter), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:kilpisjarvi}; N, x, y, xpred, pmualpha, psalpha, pmubeta, psbeta, kwargs...) = begin \n        x_ = x\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::real\n                sigma::real(lower=0)\n            end\n            @model begin\n                alpha ~ normal(pmualpha, psalpha);\n                beta ~ normal(pmubeta, psbeta);\n                y ~ normal(@broadcasted(alpha + beta * x_), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:wells_daae_c_model}; N, switched, dist, arsenic, assoc, educ, kwargs...) = begin \n        c_dist100 = @. (dist - $mean(dist)) / 100.0;\n        c_arsenic = @. arsenic - $mean(arsenic);\n        da_inter = @. c_dist100 * c_arsenic;\n        educ4 = @. educ / 4.\n        X = hcat(c_dist100, c_arsenic, da_inter, assoc, educ4)\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::vector[5]\n            end\n            @model begin\n                switched ~ bernoulli_logit_glm(X, alpha, beta);\n            end\n    end\nend\njulia_implementation(::Val{:eight_schools_noncentered}; J, y, sigma, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta_trans::vector[J]\n            mu::real\n            tau::real(lower=0)\n        end\n        theta = @broadcasted(theta_trans * tau + mu);\n        @model begin\n            theta_trans ~ normal(0, 1);\n            y ~ normal(theta, sigma);\n            mu ~ normal(0, 5);\n            tau ~ cauchy(0, 5);\n        end\n    end\nend\njulia_implementation(::Val{:Rate_5_model}; n1, n2, k1, k2, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::real(lower=0,upper=1)\n        end\n        @model begin\n            theta ~ beta(1, 1);\n            k1 ~ binomial(n1, theta);\n            k2 ~ binomial(n2, theta);\n        end\n    end\nend\njulia_implementation(::Val{:dugongs_model}; N, x, Y, kwargs...) = begin \n        x_ = x\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::real\n                lambda::real(lower=.5,upper=1.)\n                tau::real(lower=0.)\n            end\n            @transformed_parameters begin\n                sigma = 1. / sqrt(tau);\n                U3 = logit(lambda);\n            end\n            @model begin\n                for i in 1:N\n                    m = alpha - beta * pow(lambda, x_[i]);\n                    Y[i] ~ normal(m, sigma);\n                end\n                \n                alpha ~ normal(0.0, 1000.);\n                beta ~ normal(0.0, 1000.);\n                lambda ~ uniform(.5, 1.);\n                tau ~ gamma(.0001, .0001);\n            end\n    end\nend\njulia_implementation(::Val{:irt_2pl}; I, J, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            sigma_theta::real(lower=0);\n            theta::vector[J];\n\n            sigma_a::real(lower=0);\n            a::vector(lower=0)[I];\n\n            mu_b::real;\n            sigma_b::real(lower=0);\n            b::vector[I];\n        end\n        @model begin\n            sigma_theta ~ cauchy(0, 2);\n            theta ~ normal(0, sigma_theta);\n            \n            sigma_a ~ cauchy(0, 2);\n            a ~ lognormal(0, sigma_a);\n            \n            mu_b ~ normal(0, 5);\n            sigma_b ~ cauchy(0, 2);\n            b ~ normal(mu_b, sigma_b);\n            \n            for i in 1:I\n                y[i,:] ~ bernoulli_logit(@broadcasted(a[i] * (theta - b[i])));\n            end\n        end\n    end\nend\njulia_implementation(::Val{:logmesquite_logva}; N, weight, diam1, diam2, canopy_height, group, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_canopy_volume = @. log(diam1 * diam2 * canopy_height);\n        log_canopy_area = @. log(diam1 * diam2)\n@stan begin \n            @parameters begin\n                beta::vector[4]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_canopy_volume\n                + beta[3] * log_canopy_area + beta[4] * group), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:radon_variable_slope_centered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::real\n            beta::vector[J]\n            mu_beta::real\n            sigma_beta::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            alpha ~ normal(0, 10);\n            sigma_y ~ normal(0, 1);\n            sigma_beta ~ normal(0, 1);\n            mu_beta ~ normal(0, 10);\n            \n            beta ~ normal(mu_beta, sigma_beta);\n            for n in 1:N\n                mu = alpha + floor_measure[n] * beta[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_variable_intercept_centered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[J]\n            beta::real\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            \n            alpha ~ normal(mu_alpha, sigma_alpha);\n            for n in 1:N\n                mu = alpha[county_idx[n]] + floor_measure[n] * beta;\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:seeds_stanified_model}; I, n, N, x1, x2, kwargs...) = begin \n        x1x2 = @. x1 * x2;\n@stan begin \n            @parameters begin\n                alpha0::real;\n                alpha1::real;\n                alpha12::real;\n                alpha2::real;\n                b::vector[I];\n                sigma::real(lower=0);\n            end\n            @model begin\n                alpha0 ~ normal(0.0, 1.0);\n                alpha1 ~ normal(0.0, 1.0);\n                alpha2 ~ normal(0.0, 1.0);\n                alpha12 ~ normal(0.0, 1.0);\n                sigma ~ cauchy(0, 1);\n                \n                b ~ normal(0.0, sigma);\n                n ~ binomial_logit(N,\n                                   @broadcasted(alpha0 + alpha1 * x1 + alpha2 * x2 + alpha12 * x1x2 + b));\n            end\n    end\nend\njulia_implementation(::Val{:state_space_stochastic_level_stochastic_seasonal}; n, y, x, w, kwargs...) = begin \n        x_ = x\n        mu_lower = mean(y) - 3 * sd(y)\n        mu_upper = mean(y) + 3 * sd(y)\n@stan begin \n            @parameters begin\n                mu::vector(lower=mu_lower, upper=mu_upper)[n]\n                seasonal::vector[n]\n                beta::real\n                lambda::real\n                sigma::positive_ordered[3]\n            end\n            @model @views begin\n                for t in 12:n\n                    seasonal[t] ~ normal(-sum(seasonal[t-11:t-1]), sigma[1]);\n                end\n                \n                for t in 2:n\n                    mu[t] ~ normal(mu[t - 1], sigma[2]);\n                end\n                \n                y ~ normal(@broadcasted(mu + beta * x_ + lambda * w + seasonal), sigma[3]);\n                \n                sigma ~ student_t(4, 0, 1);\n            end\n        end\nend\njulia_implementation(::Val{:radon_partially_pooled_noncentered}; N, J, county_idx, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha_raw::vector[J]\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        alpha = @.(mu_alpha + sigma_alpha * alpha_raw);\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            alpha_raw ~ normal(0, 1);\n\n            for n in 1:N\n                mu = alpha[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:seeds_model}; I, n, N, x1, x2, kwargs...) = begin \n        x1x2 = @. x1 * x2;\n@stan begin \n            @parameters begin\n                alpha0::real;\n                alpha1::real;\n                alpha12::real;\n                alpha2::real;\n                tau::real(lower=0);\n                b::vector[I];\n            end\n            sigma = 1.0 / sqrt(tau);\n            @model begin\n                alpha0 ~ normal(0.0, 1.0E3);\n                alpha1 ~ normal(0.0, 1.0E3);\n                alpha2 ~ normal(0.0, 1.0E3);\n                alpha12 ~ normal(0.0, 1.0E3);\n                tau ~ gamma(1.0E-3, 1.0E-3);\n                \n                b ~ normal(0.0, sigma);\n                n ~ binomial_logit(N,\n                                   @broadcasted(alpha0 + alpha1 * x1 + alpha2 * x2 + alpha12 * x1x2 + b));\n            end\n        end\nend\njulia_implementation(::Val{:arma11}; T, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            mu::real\n            phi::real\n            theta::real\n            sigma::real(lower=0)\n        end\n        @model begin\n            mu ~ normal(0, 10);\n            phi ~ normal(0, 2);\n            theta ~ normal(0, 2);\n            sigma ~ cauchy(0, 2.5);\n            nu = mu + phi * mu\n            err = y[1] - nu\n            err ~ normal(0, sigma);\n            for t in 2:T\n                nu = mu + phi * y[t-1] + theta * err\n                err = y[t] - nu\n                err ~ normal(0, sigma);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_hierarchical_intercept_centered}; J, N, county_idx, log_uppm, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[J]\n            beta::vector[2]\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        @model begin\n            sigma_alpha ~ normal(0, 1);\n            sigma_y ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            \n            alpha ~ normal(mu_alpha, sigma_alpha);\n            for n in 1:N\n                muj = alpha[county_idx[n]] + log_uppm[n] * beta[1]\n                mu = muj + floor_measure[n] * beta[2];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:seeds_centered_model}; I, n, N, x1, x2, kwargs...) = begin \n        x1x2 = @. x1 * x2;\n@stan begin \n            @parameters begin\n                alpha0::real;\n                alpha1::real;\n                alpha12::real;\n                alpha2::real;\n                c::vector[I];\n                sigma::real(lower=0);\n            end\n            b = @. c - $mean(c);\n            @model begin\n                alpha0 ~ normal(0.0, 1.0);\n                alpha1 ~ normal(0.0, 1.0);\n                alpha2 ~ normal(0.0, 1.0);\n                alpha12 ~ normal(0.0, 1.0);\n                sigma ~ cauchy(0, 1);\n                \n                c ~ normal(0.0, sigma);\n                n ~ binomial_logit(N,\n                                   @broadcasted(alpha0 + alpha1 * x1 + alpha2 * x2 + alpha12 * x1x2 + b));\n            end\n    end\nend\njulia_implementation(::Val{:pilots}; N, n_groups, n_scenarios, group_id, scenario_id, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            a::vector[n_groups];\n            b::vector[n_scenarios];\n            mu_a::real;\n            mu_b::real;\n            sigma_a::real(lower=0, upper=100);\n            sigma_b::real(lower=0, upper=100);\n            sigma_y::real(lower=0, upper=100);\n        end\n        y_hat = @broadcasted(a[group_id] + b[scenario_id]);\n        @model begin\n            mu_a ~ normal(0, 1);\n            a ~ normal(10 * mu_a, sigma_a);\n            \n            mu_b ~ normal(0, 1);\n            b ~ normal(10 * mu_b, sigma_b);\n            \n            y ~ normal(y_hat, sigma_y);\n        end\n    end\nend\njulia_implementation(::Val{:wells_dae_inter_model}; N, switched, dist, arsenic, educ, kwargs...) = begin \n        c_dist100 = @. (dist - $mean(dist)) / 100.0;\n        c_arsenic = @. arsenic - $mean(arsenic);\n        c_educ4 = @. (educ - $mean(educ)) / 4.\n        da_inter = @. c_dist100 * c_arsenic;\n        de_inter = @. c_dist100 * c_educ4;\n        ae_inter = @. c_arsenic * c_educ4;\n        X = hcat(c_dist100, c_arsenic, c_educ4, da_inter, de_inter, ae_inter, )\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::vector[6]\n            end\n            @model begin\n                switched ~ bernoulli_logit_glm(X, alpha, beta);\n            end\n    end\nend\njulia_implementation(::Val{:radon_variable_slope_noncentered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::real\n            beta_raw::vector[J]\n            mu_beta::real\n            sigma_beta::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        beta = @. mu_beta + sigma_beta * beta_raw;\n        @model begin\n            alpha ~ normal(0, 10);\n            sigma_y ~ normal(0, 1);\n            sigma_beta ~ normal(0, 1);\n            mu_beta ~ normal(0, 10);\n            beta_raw ~ normal(0, 1);\n\n            for n in 1:N\n                mu = alpha + floor_measure[n] * beta[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_variable_intercept_slope_centered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n@stan begin \n            @parameters begin\n                sigma_y::real(lower=0)\n                sigma_alpha::real(lower=0)\n                sigma_beta::real(lower=0)\n                alpha::vector[J]\n                beta::vector[J]\n                mu_alpha::real\n                mu_beta::real\n            end\n            @model begin\n                sigma_y ~ normal(0, 1);\n                sigma_beta ~ normal(0, 1);\n                sigma_alpha ~ normal(0, 1);\n                mu_alpha ~ normal(0, 10);\n                mu_beta ~ normal(0, 10);\n                \n                alpha ~ normal(mu_alpha, sigma_alpha);\n                beta ~ normal(mu_beta, sigma_beta);\n                for n in 1:N\n                    mu = alpha[county_idx[n]] + floor_measure[n] * beta[county_idx[n]];\n                    target += normal_lpdf(log_radon[n], mu, sigma_y);\n                end\n            end\n        end\nend\njulia_implementation(::Val{:radon_variable_intercept_noncentered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha_raw::vector[J]\n            beta::real\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        alpha = @. mu_alpha + sigma_alpha * alpha_raw;\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            alpha_raw ~ normal(0, 1);\n\n            for n in 1:N\n                mu = alpha[county_idx[n]] + floor_measure[n] * beta;\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:GLM_Poisson_model}; n, C, year, kwargs...) = begin \n        year_squared = year .^ 2\n        year_cubed = year .^ 3\n@stan begin \n            @parameters begin\n                alpha::real(lower=-20, upper=+20)\n                beta1::real(lower=-10, upper=+10)\n                beta2::real(lower=-10, upper=+10)\n                beta3::real(lower=-10, upper=+10)\n            end\n            log_lambda = @broadcasted(alpha + beta1 * year + beta2 * year_squared + beta3 * year_cubed)\n            @model begin\n                C ~ poisson_log(log_lambda);\n            end\n    end\nend\njulia_implementation(::Val{:ldaK5}; V, M, N, w, doc, alpha, beta, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::simplex[M,5];\n            phi::simplex[5,V];\n        end\n        @model @views begin\n            for m in 1:M\n                theta[m,:] ~ dirichlet(alpha);\n            end\n            for k in 1:5\n                phi[k,:] ~ dirichlet(beta);\n            end\n            for n in 1:N\n                gamma = @broadcasted(log(theta[doc[n], :]) + log(phi[:, w[n]]))\n                target += log_sum_exp(gamma);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:dogs}; n_dogs, n_trials, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            beta::vector[3];\n        end\n        @model begin\n            beta ~ normal(0, 100);\n            for i in 1:n_dogs\n                n_avoid = 0\n                n_shock = 0\n                for j in 1:n_trials\n                    p = beta[1] + beta[2] * n_avoid + beta[3] * n_shock\n                    y[i, j] ~ bernoulli_logit(p);\n                    n_avoid += 1 - y[i,j]\n                    n_shock += y[i,j]\n                end\n            end\n        end\n    end\nend\njulia_implementation(::Val{:nes}; N, partyid7, real_ideo, race_adj, educ1, gender, income, age_discrete, kwargs...) = begin \n        age30_44 = @. Float64(age_discrete == 2);\n        age45_64 = @. Float64(age_discrete == 3);\n        age65up = @. Float64(age_discrete == 4);\n@stan begin \n            @parameters begin\n                beta::vector[9]\n                sigma::real(lower=0)\n            end\n            @model begin\n                partyid7 ~ normal(@broadcasted(beta[1] + beta[2] * real_ideo + beta[3] * race_adj\n                + beta[4] * age30_44 + beta[5] * age45_64\n                + beta[6] * age65up + beta[7] * educ1 + beta[8] * gender\n                + beta[9] * income), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:dogs_log}; n_dogs, n_trials, y, kwargs...) = begin \n    @assert size(y) == (n_dogs, n_trials)\n@stan begin \n        @parameters begin\n            beta::vector[2];\n        end\n        @model begin\n            beta[1] ~ uniform(-100, 0);\n            beta[2] ~ uniform(0, 100);\n            for i in 1:n_dogs\n                n_avoid = 0\n                n_shock = 0\n                for j in 1:n_trials\n                    p = inv_logit(beta[1] * n_avoid + beta[2] * n_shock)\n                    y[i, j] ~ bernoulli(p);\n                    n_avoid += 1 - y[i,j]\n                    n_shock += y[i,j]\n                end\n            end\n        end\n    end\nend\njulia_implementation(::Val{:GLM_Binomial_model};nyears, C, N, year, kwargs...) = begin \n        year_squared = year .^ 2\n@stan begin \n            @parameters begin\n                alpha::real\n                beta1::real\n                beta2::real\n            end\n            logit_p = @broadcasted alpha + beta1 * year + beta2 * year_squared;\n            @model begin\n                alpha ~ normal(0, 100)\n                beta1 ~ normal(0, 100)\n                beta2 ~ normal(0, 100)\n                C ~ binomial_logit(N, logit_p)\n            end\n    end\nend\njulia_implementation(::Val{:radon_hierarchical_intercept_noncentered}; J, N, county_idx, log_uppm, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha_raw::vector[J]\n            beta::vector[2]\n            mu_alpha::real\n            sigma_alpha::real(lower=0)\n            sigma_y::real(lower=0)\n        end\n        alpha = @. mu_alpha + sigma_alpha * alpha_raw;\n        @model begin\n            sigma_alpha ~ normal(0, 1);\n            sigma_y ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            beta ~ normal(0, 10);\n            alpha_raw ~ normal(0, 1);\n\n            for n in 1:N\n                muj = alpha[county_idx[n]] + log_uppm[n] * beta[1]\n                mu = muj + floor_measure[n] * beta[2];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:logmesquite_logvash}; N, weight, diam1, diam2, canopy_height, total_height, group, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_canopy_volume = @. log(diam1 * diam2 * canopy_height);\n        log_canopy_area = @. log(diam1 * diam2)\n        log_canopy_shape = @. log(diam1 / diam2);\n        log_total_height = @. log(total_height);\n@stan begin \n            @parameters begin\n                beta::vector[6]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_canopy_volume\n                + beta[3] * log_canopy_area\n                + beta[4] * log_canopy_shape\n                + beta[5] * log_total_height + beta[6] * group), sigma);\n            end\n        end\nend\njulia_implementation(::Val{:ldaK2}; V, M, N, w, doc, kwargs...) = begin \n        K = 2\n        alpha = fill(1, K)\n        beta = fill(1, V)\n@stan begin \n            @parameters begin\n                theta::simplex[M,K];\n                phi::simplex[K,V];\n            end\n            @model @views begin\n                for m in 1:M\n                  theta[m,:] ~ dirichlet(alpha);\n                end\n                for k in 1:K\n                  phi[k,:] ~ dirichlet(beta);\n                end\n                for n in 1:N\n                    gamma = @broadcasted log(theta[doc[n], :]) + log(phi[:, w[n]])\n                  target += log_sum_exp(gamma);\n                end\n            end\n    end\nend\njulia_implementation(::Val{:logmesquite}; N, weight, diam1, diam2, canopy_height, total_height, density, group, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_diam1 = @. log(diam1);\n        log_diam2 = @. log(diam2);\n        log_canopy_height = @. log(canopy_height);\n        log_total_height = @. log(total_height);\n        log_density = @. log(density);\n@stan begin \n            @parameters begin\n                beta::vector[7]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_diam1 + beta[3] * log_diam2\n                + beta[4] * log_canopy_height\n                + beta[5] * log_total_height + beta[6] * log_density\n                + beta[7] * group), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:rats_model}; N, Npts, rat, x, y, xbar, kwargs...) = begin \n        x_ = x\n@stan begin \n            @parameters begin\n                alpha::vector[N];\n                beta::vector[N];\n                \n                mu_alpha::real;\n                mu_beta::real;\n                sigma_y::real(lower=0);\n                sigma_alpha::real(lower=0);\n                sigma_beta::real(lower=0);\n            end\n            @model begin\n                mu_alpha ~ normal(0, 100);\n                mu_beta ~ normal(0, 100);\n                alpha ~ normal(mu_alpha, sigma_alpha);\n                beta ~ normal(mu_beta, sigma_beta);\n                for n in 1:Npts\n                  irat = rat[n];\n                  y[n] ~ normal(alpha[irat] + beta[irat] * (x_[n] - xbar), sigma_y);\n                end\n            end\n    end\nend\njulia_implementation(::Val{:logmesquite_logvas}; N, weight, diam1, diam2, canopy_height, total_height, density, group, kwargs...) = begin \n        log_weight = @. log(weight);\n        log_canopy_volume = @. log(diam1 * diam2 * canopy_height);\n        log_canopy_area = @. log(diam1 * diam2)\n        log_canopy_shape = @. log(diam1 / diam2);\n        log_total_height = @. log(total_height);\n        log_density = @. log(density);\n@stan begin \n            @parameters begin\n                beta::vector[7]\n                sigma::real(lower=0)\n            end\n            @model begin\n                log_weight ~ normal(@broadcasted(beta[1] + beta[2] * log_canopy_volume\n                + beta[3] * log_canopy_area\n                + beta[4] * log_canopy_shape\n                + beta[5] * log_total_height + beta[6] * log_density\n                + beta[7] * group), sigma);\n            end\n    end\nend\njulia_implementation(::Val{:lsat_model}; N, R, T, culm, response, kwargs...) = begin \n    r = zeros(Int64, (T, N))\n    for j in 1:culm[1], k in 1:T\n        r[k, j] = response[1, k];\n    end\n    for i in 2:R\n        for j in (culm[i-1]+1):culm[i], k in 1:T\n            r[k, j] = response[i, k];\n        end\n    end\n    ones = fill(1., N)\n    @stan begin \n        @parameters begin\n            alpha::vector[T];\n            theta::vector[N];\n            beta::real(lower=0);\n        end\n        @model @views begin\n            alpha ~ normal(0, 100.);\n            theta ~ normal(0, 1);\n            beta ~ normal(0.0, 100.);\n            for k in 1:T\n                r[k,:] ~ bernoulli_logit(beta * theta - alpha[k] * ones);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:radon_variable_intercept_slope_noncentered}; J, N, county_idx, floor_measure, log_radon, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            sigma_y::real(lower=0)\n            sigma_alpha::real(lower=0)\n            sigma_beta::real(lower=0)\n            alpha_raw::vector[J]\n            beta_raw::vector[J]\n            mu_alpha::real\n            mu_beta::real\n        end\n        alpha = @. mu_alpha + sigma_alpha * alpha_raw;\n        beta = @. mu_beta + sigma_beta * beta_raw;\n        @model begin\n            sigma_y ~ normal(0, 1);\n            sigma_beta ~ normal(0, 1);\n            sigma_alpha ~ normal(0, 1);\n            mu_alpha ~ normal(0, 10);\n            mu_beta ~ normal(0, 10);\n            alpha_raw ~ normal(0, 1);\n            beta_raw ~ normal(0, 1);\n\n            for n in 1:N\n                mu = alpha[county_idx[n]] + floor_measure[n] * beta[county_idx[n]];\n                target += normal_lpdf(log_radon[n], mu, sigma_y);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:GLMM_Poisson_model};n, C, year) = begin \n    year_squared = year .^ 2\n    year_cubed = year .^ 3\n    @stan begin\n        @parameters begin\n            alpha::real(lower=-20, upper=+20)\n            beta1::real(lower=-10, upper=+10)\n            beta2::real(lower=-10, upper=+20)\n            beta3::real(lower=-10, upper=+10)\n            eps::vector[n]\n            sigma::real(lower=0, upper=5)\n        end\n        log_lambda = @broadcasted alpha + beta1 * year + beta2 * year_squared + beta3 * year_cubed + eps\n        @model begin\n            C ~ poisson_log(log_lambda)\n            eps ~ normal(0, sigma)\n        end\n    end\nend\njulia_implementation(::Val{:GLMM1_model};nsite, nobs, obs, obsyear, obssite, misyear, missite, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            alpha::vector[nsite]\n            mu_alpha::real\n            sd_alpha::real(lower=0,upper=5)\n        end\n    #   log_lambda = rep_matrix(alpha', nyear);\n        @model begin\n            alpha ~ normal(mu_alpha, sd_alpha)\n            mu_alpha ~ normal(0, 10)\n            for i in 1:nobs\n                # obs[i] ~ poisson_log(log_lambda[obsyear[i], obssite[i]])\n                obs[i] ~ poisson_log(alpha[obssite[i]])\n            end\n        end\n    end\nend\njulia_implementation(::Val{:hier_2pl}; I, J, N, ii, jj, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta::vector[J];\n            xi1::vector[I];\n            xi2::vector[I];\n            mu::vector[2];\n            tau::vector(lower=0)[2];\n            L_Omega::cholesky_factor_corr[2]\n        end\n        xi = hcat(xi1, xi2)\n        alpha = @. exp(xi1);\n        beta = xi2;\n        @model begin\n            L_Sigma = diag_pre_multiply(tau, L_Omega);\n            for i in 1:I\n                target += multi_normal_cholesky_lpdf(xi[i, :], mu, L_Sigma);\n            end\n            theta ~ normal(0, 1);\n            L_Omega ~ lkj_corr_cholesky(4);\n            mu[1] ~ normal(0, 1);\n            tau[1] ~ exponential(.1);\n            mu[2] ~ normal(0, 5);\n            tau[2] ~ exponential(.1);\n            y ~ bernoulli_logit(alpha[ii] .* (theta[jj] - beta[ii]));\n        end\n    end\nend\njulia_implementation(::Val{:dogs_hierarchical}; n_dogs, n_trials, y, kwargs...) = begin \n        J = n_dogs;\n        T = n_trials;\n        prev_shock = zeros((J,T));\n        prev_avoid = zeros((J,T));\n        \n        for j in 1:J\n            prev_shock[j, 1] = 0;\n            prev_avoid[j, 1] = 0;\n            for t in 2:T\n                prev_shock[j, t] = prev_shock[j, t - 1] + y[j, t - 1];\n                prev_avoid[j, t] = prev_avoid[j, t - 1] + 1 - y[j, t - 1];\n            end\n        end\n@stan begin \n            @parameters begin\n                a::real(lower=0, upper=1);\n                b::real(lower=0, upper=1);\n            end\n            @model begin\n                y ~ bernoulli(@broadcasted(a ^ prev_shock * b ^ prev_avoid));\n            end\n    end\nend\n\n\njulia_implementation(::Val{:dogs_nonhierarchical}; n_dogs, n_trials, y, kwargs...) = begin \n    J = n_dogs;\n    T = n_trials;\n    prev_shock = zeros((J,T));\n    prev_avoid = zeros((J,T));\n    \n    for j in 1:J\n        prev_shock[j, 1] = 0;\n        prev_avoid[j, 1] = 0;\n        for t in 2:T\n            prev_shock[j, t] = prev_shock[j, t - 1] + y[j, t - 1];\n            prev_avoid[j, t] = prev_avoid[j, t - 1] + 1 - y[j, t - 1];\n        end\n    end\n    @stan begin \n        @parameters begin\n            mu_logit_ab::vector[2]\n            sigma_logit_ab::vector(lower=0)[2]\n            L_logit_ab::cholesky_factor_corr[2]\n            z::matrix[J, 2]\n        end\n        @model @views begin\n            logit_ab = rep_vector(1, J) * mu_logit_ab'\n                                    + z * diag_pre_multiply(sigma_logit_ab, L_logit_ab);\n            a = inv_logit.(logit_ab[ : , 1]);\n            b = inv_logit.(logit_ab[ : , 2]);\n            y ~ bernoulli(@broadcasted(a ^ prev_shock * b ^ prev_avoid));\n            mu_logit_ab ~ logistic(0, 1);\n            sigma_logit_ab ~ normal(0, 1);\n            L_logit_ab ~ lkj_corr_cholesky(2);\n            (z) ~ normal(0, 1);\n        end\n    end\nend\njulia_implementation(::Val{:M0_model}; M, T, y, kwargs...) = begin\n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int64, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0,upper=1)\n                p::real(lower=0,upper=1)\n            end\n            @model begin\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + binomial_lpmf(s[i], T, p)\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + binomial_lpmf(0, T, p),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n    end\nend\njulia_implementation(::Val{:diamonds}; N, Y, K, X, prior_only, kwargs...) = begin\n        Kc = K - 1;\n        Xc = zeros((N, Kc))\n        means_X = zeros(Kc)\n        for i in 2:K\n            means_X[i - 1] = mean(X[ : , i]);\n            @. Xc[ : , i - 1] = X[ : , i] - means_X[i - 1];\n        end\n@stan begin \n            @parameters begin\n                b::vector[Kc];\n                Intercept::real;\n                sigma::real(lower=0.)\n            end\n            @model begin\n                target += normal_lpdf(b, 0., 1.);\n                target += student_t_lpdf(Intercept, 3., 8., 10.);\n                target += student_t_lpdf(sigma, 3., 0., 10.)# - 1 * student_t_lccdf(0, 3, 0, 10);\n                if !(prior_only == 1)\n                    target += normal_id_glm_lpdf(Y, Xc, Intercept, b, sigma);\n                end\n            end\n    end\nend\njulia_implementation(::Val{:Mt_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int64, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0,upper=1)\n                p::vector(lower=0,upper=1)[T]\n            end\n            @model @views begin\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + bernoulli_lpmf(y[i,:], p)\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + bernoulli_lpmf(y[i,:], p),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n    end\nend\njulia_implementation(::Val{:election88_full}; \n    N,\n    n_age,\n    n_age_edu,\n    n_edu,\n    n_region_full,\n    n_state,\n    age,\n    age_edu,\n    black,\n    edu,\n    female,\n    region_full,\n    state,\n    v_prev_full,\n    y,\n    kwargs...) = begin \n@stan begin \n            @parameters begin\n                a::vector[n_age];\n                b::vector[n_edu];\n                c::vector[n_age_edu];\n                d::vector[n_state];\n                e::vector[n_region_full];\n                beta::vector[5];\n                sigma_a::real(lower=0, upper=100);\n                sigma_b::real(lower=0, upper=100);\n                sigma_c::real(lower=0, upper=100);\n                sigma_d::real(lower=0, upper=100);\n                sigma_e::real(lower=0, upper=100);\n            end\n            @model @views begin\n                y_hat = @broadcasted (beta[1] + beta[2] * black + beta[3] * female\n                    + beta[5] * female * black + beta[4] * v_prev_full\n                    + a[age] + b[edu] + c[age_edu] + d[state]\n                    + e[region_full])\n                a ~ normal(0, sigma_a);\n                b ~ normal(0, sigma_b);\n                c ~ normal(0, sigma_c);\n                d ~ normal(0, sigma_d);\n                e ~ normal(0, sigma_e);\n                beta ~ normal(0, 100);\n                y ~ bernoulli_logit(y_hat);\n            end\n        end\nend\njulia_implementation(::Val{:nn_rbm1bJ10}; N, M, x, K, y, kwargs...) = begin \n            J = 10\n            nu_alpha = 0.5;\n            s2_0_alpha = (0.05 / M ^ (1 / nu_alpha)) ^ 2;\n            nu_beta = 0.5;\n            s2_0_beta = (0.05 / J ^ (1 / nu_beta)) ^ 2;\n            \n            ones = rep_vector(1., N);\n            x1 = append_col(ones, x);\n\n    return @stan begin end\n    @stan begin \n        @parameters begin\n            sigma2_alpha::real(lower=0);\n            sigma2_beta::real(lower=0);\n            alpha::matrix[M, J];\n            beta::matrix[J, K - 1];\n            alpha1::row_vector[J];\n            beta1::row_vector[K - 1];\n        end\n        @model @views begin\n            v = append_col(\n                ones,\n                append_col(\n                    ones,\n                    tanh.(x1 * append_row(alpha1, alpha))\n                ) * append_row(beta1, beta)\n            );\n            alpha1 ~ normal(0, 1);\n            beta1 ~ normal(0, 1);\n            sigma2_alpha ~ inv_gamma(nu_alpha / 2, nu_alpha * s2_0_alpha / 2);\n            sigma2_beta ~ inv_gamma(nu_beta / 2, nu_beta * s2_0_beta / 2);\n            \n            (alpha) ~ normal(0, sqrt(sigma2_alpha));\n            (beta) ~ normal(0, sqrt(sigma2_beta));\n            for n in 1:N\n                y[n] ~ categorical_logit(v[n, :]);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:nn_rbm1bJ100}; N, M, x, K, y, kwargs...) = begin \n    J = 100\n    nu_alpha = 0.5;\n    s2_0_alpha = (0.05 / M ^ (1 / nu_alpha)) ^ 2;\n    nu_beta = 0.5;\n    s2_0_beta = (0.05 / J ^ (1 / nu_beta)) ^ 2;\n    \n    ones = rep_vector(1., N);\n    x1 = append_col(ones, x);\n    return @stan begin end\n    @stan begin \n        @parameters begin\n            sigma2_alpha::real(lower=0);\n            sigma2_beta::real(lower=0);\n            alpha::matrix[M, J];\n            beta::matrix[J, K - 1];\n            alpha1::row_vector[J];\n            beta1::row_vector[K - 1];\n        end\n        @model @views begin\n            v = append_col(\n                ones,\n                append_col(\n                    ones,\n                    tanh.(x1 * append_row(alpha1, alpha))\n                ) * append_row(beta1, beta)\n            );\n            alpha1 ~ normal(0, 1);\n            beta1 ~ normal(0, 1);\n            sigma2_alpha ~ inv_gamma(nu_alpha / 2, nu_alpha * s2_0_alpha / 2);\n            sigma2_beta ~ inv_gamma(nu_beta / 2, nu_beta * s2_0_beta / 2);\n            \n            (alpha) ~ normal(0, sqrt(sigma2_alpha));\n            (beta) ~ normal(0, sqrt(sigma2_beta));\n            for n in 1:N\n                y[n] ~ categorical_logit(v[n, :]);\n            end\n        end\n    end\nend\njulia_implementation(::Val{:Survey_model}; nmax, m, k) = begin \n    nmin = maximum(k)\n    @stan begin \n        @parameters begin \n            theta::real(lower=0, upper=1)\n        end\n        @model begin \n            target += log_sum_exp([\n                n &lt; nmin ? log(1.0 / nmax) - Inf : log(1.0 / nmax) + binomial_lpmf(k, n, theta)\n                for n in 1:nmax\n            ])\n        end\n    end\nend\n\njulia_implementation(::Val{:sir}; N_t, t, y0, stoi_hat, B_hat) = begin \n    y0 = collect(Float64, y0)\n    simple_SIR(t, y, theta, x_r, x_i)  = begin\n        dydt = zero(y)\n        \n        dydt[1] = -theta[1] * y[4] / (y[4] + theta[2]) * y[1];\n        dydt[2] = theta[1] * y[4] / (y[4] + theta[2]) * y[1] - theta[3] * y[2];\n        dydt[3] = theta[3] * y[2];\n        dydt[4] = theta[4] * y[2] - theta[5] * y[4];\n        \n        return dydt;\n    end\n    t0 = 0.\n    kappa = 1000000.\n    @stan begin \n        @parameters begin \n            beta::real(lower=0)\n            gamma::real(lower=0)\n            xi::real(lower=0)\n            delta::real(lower=0)\n        end\n        @model @views begin \n            y = integrate_ode_rk45(simple_SIR, y0, t0, t, [beta, kappa, gamma, xi, delta]);\n\n            beta ~ cauchy(0, 2.5);\n            gamma ~ cauchy(0, 1);\n            xi ~ cauchy(0, 25);\n            delta ~ cauchy(0, 1);\n\n            stoi_hat[1] ~ poisson(y0[1] - y[1, 1]);\n            for n in 2:N_t\n                stoi_hat[n] ~ poisson(max(1e-16, y[n - 1, 1] - y[n, 1]));\n            end\n            \n            B_hat ~ lognormal(@broadcasted(nothrow_log(y[:, 4])), 0.15);\n        \n        end\n    end\nend\n\njulia_implementation(::Val{:lotka_volterra}; N, ts, y_init, y) = begin \n    dz_dt(t, z, theta, x_r, x_i)  = begin\n        u, v = z\n        \n        alpha, beta, gamma, delta = theta\n\n        du_dt = (alpha - beta * v) * u\n        dv_dt = (-gamma +delta * u) * v\n        [du_dt, dv_dt]\n    end\n    @stan begin \n        @parameters begin \n            theta::real(lower=0)[4]\n            z_init::real(lower=0)[2]\n            sigma::real(lower=0)[2]\n        end\n        @model @views begin \n            z = integrate_ode_rk45(dz_dt, z_init, 0, ts, theta,\n                missing, missing, 1e-5, 1e-3, 5e2);\n\n            theta[[1,3]] ~ normal(1, 0.5);\n            theta[[2,4]] ~ normal(0.05, 0.05);\n            sigma ~ lognormal(-1, 1);\n            z_init ~ lognormal(log(10), 1);\n            y_init ~ lognormal(@broadcasted(log(z_init)), sigma);\n            y ~ lognormal(@broadcasted(nothrow_log(z)), sigma');\n        end\n    end\nend\n\njulia_implementation(::Val{:soil_incubation}; totalC_t0, t0, N_t, ts, eCO2mean, kwargs...) = begin \n    two_pool_feedback(t, C, theta, x_r, x_i)  = begin\n        k1, k2, alpha21, alpha12 = theta\n        [\n            -k1 * C[1] + alpha12 * k2 * C[2]\n            -k2 * C[2] + alpha21 * k1 * C[1]\n        ]\n    end\n    evolved_CO2(N_t, t0, ts, gamma, totalC_t0, k1, k2, alpha21, alpha12) = begin \n        C_t0 = [gamma * totalC_t0, (1 - gamma) * totalC_t0]\n        theta = k1, k2, alpha21, alpha12\n        C_hat = integrate_ode_rk45(two_pool_feedback, C_t0, t0, ts, theta)\n        totalC_t0 .- sum.(eachrow(C_hat))\n    end\n    @stan begin \n        @parameters begin \n            k1::real(lower=0)\n            k2::real(lower=0)\n            alpha21::real(lower=0)\n            alpha12::real(lower=0)\n            gamma::real(lower=0, upper=1)\n            sigma::real(lower=0)\n        end\n        @model @views begin \n            eCO2_hat = evolved_CO2(N_t, t0, ts, gamma, totalC_t0, k1, k2, alpha21, alpha12)\n            gamma ~ beta(10, 1); \n            k1 ~ normal(0, 1);\n            k2 ~ normal(0, 1);\n            alpha21 ~ normal(0, 1);\n            alpha12 ~ normal(0, 1);\n            sigma ~ cauchy(0, 1);\n            eCO2mean ~ normal(eCO2_hat, sigma);\n        end\n    end\nend\n\njulia_implementation(::Val{:one_comp_mm_elim_abs}; t0, D, V, N_t, times, C_hat) = begin \n    one_comp_mm_elim_abs(t, y, theta, x_r, x_i)  = begin\n        k_a, K_m, V_m = theta\n        D, V = x_r\n        dose = 0.\n        elim = (V_m / V) * y[1] / (K_m + y[1]);\n        if t &gt; 0\n            dose = exp(-k_a * t) * D * k_a / V;\n        end\n        [dose - elim]\n    end\n    C0 = [0.]\n    x_r = [D,V]\n    x_i = missing\n    @stan begin \n        @parameters begin \n            k_a::real(lower=0)\n            K_m::real(lower=0)\n            V_m::real(lower=0)\n            sigma::real(lower=0)\n        end\n        @model @views begin \n            theta = [k_a, K_m, V_m]\n            C = integrate_ode_bdf(one_comp_mm_elim_abs, C0, t0, times, theta, x_r, x_i)\n            k_a ~ cauchy(0, 1);\n            K_m ~ cauchy(0, 1);\n            V_m ~ cauchy(0, 1);\n            sigma ~ cauchy(0, 1);\n            \n            C_hat ~ lognormal(@broadcasted(nothrow_log(C[:, 1])), sigma);\n        end\n    end\nend\n\n\njulia_implementation(::Val{:bym2_offset_only}; N, N_edges, node1, node2, y, E, scaling_factor, kwargs...) = begin \n        log_E = @. log(E)\n@stan begin \n            @parameters begin\n                beta0::real;\n                sigma::real(lower=0);\n                rho::real(lower=0, upper=1);\n                theta::vector[N];\n                phi::vector[N];\n            end\n            convolved_re = @broadcasted(sqrt(1 - rho) * theta + sqrt(rho / scaling_factor) * phi)\n            @model @views begin\n                y ~ poisson_log(@broadcasted(log_E + beta0 + convolved_re * sigma));\n                \n                target += -0.5 * dot_self(phi[node1] - phi[node2]);\n                \n                beta0 ~ normal(0, 1);\n                theta ~ normal(0, 1);\n                sigma ~ normal(0, 1);\n                rho ~ beta(0.5, 0.5);\n                sum(phi) ~ normal(0, 0.001 * N);\n            end\n        end\nend\njulia_implementation(::Val{:bones_model}; nChild, nInd, gamma, delta, ncat, grade, kwargs...) = begin \n        # error(ncat)\n    @stan begin \n        @parameters begin\n            theta::real[nChild]\n        end\n        @model @views begin\n            theta ~ normal(0.0, 36.);\n            p = zeros((nChild, nInd, 5))\n            Q = zeros((nChild, nInd, 4))\n            for i in 1:nChild\n                for j in 1:nInd\n                    for k in 1:(ncat[j]-1)\n                        Q[i,j,k] = inv_logit(delta[j] * (theta[i] - gamma[j, k]))\n                    end\n                    p[i,j,1] = 1 - Q[i,j,1]\n                    for k in 2:(ncat[j]-1)\n                        p[i, j, k] = Q[i, j, k - 1] - Q[i, j, k];\n                    end\n                    p[i, j, ncat[j]] = Q[i, j, ncat[j] - 1];\n                    if grade[i, j] != -1\n                        target += log(p[i, j, grade[i, j]]);\n                    end\n                end\n            end\n        end\n    end\nend\njulia_implementation(::Val{:logistic_regression_rhs}; n, d, y, x, scale_icept,\n    scale_global,\n    nu_global,\n    nu_local, \n    slab_scale,\n    slab_df, kwargs...) = begin \n        x = Matrix{Float64}(x)\n@stan begin \n            @parameters begin\n                beta0::real;\n                z::vector[d];\n                tau::real(lower=0.);\n                lambda::vector(lower=0.)[d];\n                caux::real(lower=0.);\n            end\n            c = slab_scale * sqrt(caux);\n            lambda_tilde = @broadcasted sqrt(c ^ 2 * square(lambda) / (c ^ 2 + tau ^ 2 * square(lambda)));\n            beta = @. z * lambda_tilde * tau;\n            @model begin\n                z ~ std_normal();\n                lambda ~ student_t(nu_local, 0., 1.);\n                tau ~ student_t(nu_global, 0, 2. * scale_global);\n                caux ~ inv_gamma(0.5 * slab_df, 0.5 * slab_df);\n                beta0 ~ normal(0., scale_icept);\n                \n                y ~ bernoulli_logit_glm(x, beta0, beta);\n            end\n        end\nend\n\njulia_implementation(::Val{:hmm_example}; N, K, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta1::simplex[K]\n            theta2::simplex[K]\n            mu::positive_ordered[K]\n        end\n        theta = hcat(theta1, theta2)'\n        @model @views begin\n            target += normal_lpdf(mu[1], 3, 1);\n            target += normal_lpdf(mu[2], 10, 1);\n            acc = zeros(K)\n            gamma = zeros((K,N))'\n            gamma[1, :] .= @. normal_lpdf(y[1], mu, 1) \n            for t in 2:N\n                for k in 1:K\n                    for j in 1:K\n                        acc[j] =  gamma[t - 1, j] + log(theta[j, k]) + normal_lpdf(y[t], mu[k], 1);\n                    end\n                    gamma[t, k] = log_sum_exp(acc);\n                end\n            end\n            target += log_sum_exp(gamma[N, :])\n        end\n    end\nend\njulia_implementation(::Val{:Mb_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int64, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0.,upper=1.)\n                p::real(lower=0.,upper=1.)\n                c::real(lower=0.,upper=1.)\n            end\n            @model @views begin\n                p_eff = hcat(\n                    fill(p, M), \n                    @. (1 - y[:, 1:end-1]) * p + y[:, 1:end-1] * c\n                )\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + bernoulli_lpmf(y[i, :], p_eff[i, :])\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + bernoulli_lpmf(y[i, :], p_eff[i, :]),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n        end\nend\njulia_implementation(::Val{:Mh_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, ) \n        C = 0\n        for i in 1:M\n            if y[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0,upper=1)\n                mean_p::real(lower=0,upper=1)\n                sigma::real(lower=0,upper=5)\n                eps_raw::vector[M]\n            end\n            eps = @. logit(mean_p) + sigma * eps_raw\n            @model begin\n                eps_raw ~ normal(0, 1)\n                for i in 1:M\n                    if y[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + binomial_logit_lpmf(y[i], T, eps[i])\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + binomial_logit_lpmf(0, T, eps[i]),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n        end\nend\njulia_implementation(::Val{:Mth_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int64, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n        @stan begin \n            @parameters begin\n                omega::real(lower=0.,upper=1.)\n                mean_p::vector(lower=0.,upper=1.)[T]\n                sigma::real(lower=0., upper=5.)\n                eps_raw::vector[M]\n            end\n            @model @views begin\n                logit_p = @. logit(mean_p)' .+ sigma * eps_raw\n                eps_raw ~ normal(0., 1.)\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + bernoulli_logit_lpmf(y[i, :], logit_p[i, :])\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + bernoulli_logit_lpmf(y[i, :], logit_p[i, :]),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n    end\nend\njulia_implementation(::Val{Symbol(\"2pl_latent_reg_irt\")}; I, J, N, ii, jj, y, K, W, kwargs...) = begin \n    obtain_adjustments(W) = begin \n        local M, K = size(W)\n        adj = zeros((2, K))\n        adj[1,1] = 0\n        adj[2, 1] = 1\n        for k in 2:K\n            min_w = minimum(W[:,k])\n            max_w = maximum(W[:,k])\n            minmax_count = sum(w-&gt;w in (min_w, max_w), W[:, k])\n            if minmax_count == M\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = max_w - min_w;\n            else\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = sd(W[:, k]) * 2;\n            end\n        end\n        return adj\n    end\n    adj = obtain_adjustments(W);\n    W_adj = @. ((W - adj[1:1,:])/adj[2:2,:])\n    @stan begin \n        @parameters begin\n            alpha::vector(lower=0)[I];\n            beta_free::vector[I - 1] ;\n            theta::vector[J];\n            lambda_adj::vector[K];\n        end\n        beta = vcat(beta_free, -sum(beta_free))\n        @model @views begin\n            alpha ~ lognormal(1, 1);\n            target += normal_lpdf(beta, 0, 3);\n            lambda_adj ~ student_t(3, 0, 1);\n            theta ~ normal(W_adj * lambda_adj, 1);\n            y ~ bernoulli_logit(@broadcasted(alpha[ii] * theta[jj] - beta[ii]));\n        end\n    end\nend\njulia_implementation(::Val{:Mtbh_model}; M, T, y, kwargs...) = begin \n        @assert size(y) == (M, T) \n        C = 0\n        s = zeros(Int64, M)\n        for i in 1:M\n            s[i] = sum(y[i, :])\n            if s[i] &gt; 0\n                C += 1\n            end\n        end\n@stan begin \n            @parameters begin\n                omega::real(lower=0,upper=1)\n                mean_p::vector(lower=0,upper=1)[T]\n                gamma::real\n                sigma::real(lower=0, upper=3)\n                eps_raw::vector[M]\n            end\n            @model @views begin\n                eps = @. sigma * eps_raw\n                alpha = @. logit(mean_p)\n                logit_p = hcat(\n                    @.(alpha[1] + eps),\n                    @.(alpha[2:end]' + eps + gamma * y[:, 1:end-1])\n                )\n                gamma ~ normal(0, 10)\n                eps_raw ~ normal(0, 1)\n                for i in 1:M\n                    if s[i] &gt; 0\n                        target += bernoulli_lpmf(1, omega) + bernoulli_logit_lpmf(y[i, :], logit_p[i, :])\n                    else\n                        target += log_sum_exp(bernoulli_lpmf(1, omega)\n                            + bernoulli_logit_lpmf(y[i, :], logit_p[i, :]),\n                            bernoulli_lpmf(0, omega))\n                    end\n                end\n            end\n    end\nend\njulia_implementation(::Val{:multi_occupancy}; J, K, n, X, S, kwargs...) = begin \n    cov_matrix_2d(sigma, rho) = begin\n        rv12 = sigma[1] * sigma[2] * rho\n        [\n            square(sigma[1]) rv12\n            rv12 square(sigma[2])\n        ]\n    end\n    lp_observed(X, K, logit_psi, logit_theta) = log_inv_logit(logit_psi) + binomial_logit_lpmf(X, K, logit_theta);\n    lp_unobserved(K, logit_psi, logit_theta) = log_sum_exp(\n        lp_observed(0, K, logit_psi, logit_theta),\n        log1m_inv_logit(logit_psi)\n    );\n    lp_never_observed(J, K, logit_psi, logit_theta, Omega) = begin\n        lp_unavailable = bernoulli_lpmf(0, Omega);\n        lp_available = bernoulli_lpmf(1, Omega) + J * lp_unobserved(K, logit_psi, logit_theta);\n        return log_sum_exp(lp_unavailable, lp_available);\n    end\n@stan begin \n            @parameters begin\n                alpha::real\n                beta::real\n                Omega::real(lower=0, upper=+1)\n                rho_uv::real(lower=-1, upper=+1)\n                sigma_uv::vector(lower=0,upper=+Inf)[2]\n                uv1::vector[S]\n                uv2::vector[S]\n            end\n            @transformed_parameters begin \n                uv = hcat(uv1, uv2)\n                logit_psi = @. uv1 + alpha\n                logit_theta = @. uv2 + beta\n            end\n            @model begin\n                alpha ~ cauchy(0, 2.5);\n                beta ~ cauchy(0, 2.5);\n                sigma_uv ~ cauchy(0, 2.5);\n                ((rho_uv + 1) / 2) ~ beta(2, 2);\n                target += multi_normal_lpdf(uv, rep_vector(0., 2), cov_matrix_2d(sigma_uv, rho_uv));\n                Omega ~ beta(2, 2);\n  \n                for i in 1:n \n                    1 ~ bernoulli(Omega); \n                    for j in 1:J\n                        if X[i, j] &gt; 0\n                            target += lp_observed(X[i, j], K, logit_psi[i], logit_theta[i]);\n                        else\n                            target += lp_unobserved(K, logit_psi[i], logit_theta[i]);\n                        end\n                    end\n                end\n                for i in (n + 1):S\n                  target += lp_never_observed(J, K, logit_psi[i], logit_theta[i], Omega);\n                end\n            end\n        end\nend\njulia_implementation(::Val{:losscurve_sislob};\ngrowthmodel_id, \nn_data,\nn_time,\nn_cohort,\ncohort_id,\nt_idx,\ncohort_maxtime,\nt_value,\npremium,\nloss,\nkwargs...) = begin \n    growth_factor_weibull(t, omega, theta) = begin\n        return 1 - exp(-(t / theta) ^ omega);\n    end\n\n    growth_factor_loglogistic(t, omega, theta) = begin\n        pow_t_omega = t ^ omega;\n        return pow_t_omega / (pow_t_omega + theta ^ omega);\n    end\n    @stan begin \n            @parameters begin\n                omega::real(lower=0);\n                theta::real(lower=0);\n                \n                LR::vector(lower=0)[n_cohort];\n                \n                mu_LR::real;\n                sd_LR::real(lower=0);\n                \n                loss_sd::real(lower=0);\n            end\n            gf = if growthmodel_id == 1\n                @. growth_factor_weibull(t_value, omega, theta)\n            else\n                @. growth_factor_loglogistic(t_value, omega, theta)\n            end\n            @model @views begin\n                mu_LR ~ normal(0, 0.5);\n                sd_LR ~ lognormal(0, 0.5);\n                \n                LR ~ lognormal(mu_LR, sd_LR);\n                \n                loss_sd ~ lognormal(0, 0.7);\n                \n                omega ~ lognormal(0, 0.5);\n                theta ~ lognormal(0, 0.5);\n                \n                loss ~ normal(@broadcasted(LR[cohort_id] * premium[cohort_id] * gf[t_idx]), (loss_sd * premium)[cohort_id]);\n            end\n    end\nend\n\njulia_implementation(::Val{:accel_splines}; N,Y,Ks,Xs,knots_1,Zs_1_1, Ks_sigma, Xs_sigma,knots_sigma_1,Zs_sigma_1_1,prior_only, kwargs...) = begin \n@stan begin \n            @parameters begin\n                Intercept::real;\n                bs::vector[Ks];\n                zs_1_1::vector[knots_1];\n                sds_1_1::real(lower=0);\n                Intercept_sigma::real;\n                bs_sigma::vector[Ks_sigma];\n                zs_sigma_1_1::vector[knots_sigma_1];\n                sds_sigma_1_1::real(lower=0);\n            end\n            s_1_1 = @. sds_1_1 * zs_1_1\n            s_sigma_1_1 = @. sds_sigma_1_1 * zs_sigma_1_1;\n            @model begin\n                mu = Intercept .+ Xs * bs + Zs_1_1 * s_1_1;\n                sigma = exp.(Intercept_sigma .+ Xs_sigma * bs_sigma\n                                  + Zs_sigma_1_1 * s_sigma_1_1);\n                target += student_t_lpdf(Intercept, 3, -13, 36);\n                target += normal_lpdf(zs_1_1, 0, 1);\n                target += student_t_lpdf(sds_1_1, 3, 0, 36)\n                        #   - 1 * student_t_lccdf(0, 3, 0, 36);\n                target += student_t_lpdf(Intercept_sigma, 3, 0, 10);\n                target += normal_lpdf(zs_sigma_1_1, 0, 1);\n                target += student_t_lpdf(sds_sigma_1_1, 3, 0, 36)\n                        #   - 1 * student_t_lccdf(0, 3, 0, 36);\n                if !(prior_only == 1)\n                    target += normal_lpdf(Y, mu, sigma);\n                end\n            end\n        end\nend\njulia_implementation(::Val{Symbol(\"grsm_latent_reg_irt\")}; I, J, N, ii, jj, y, K, W, kwargs...) = begin \n    rsm(y, theta, beta, kappa) = begin\n      unsummed = vcat(0, theta .- beta .- kappa);\n      probs = softmax(cumulative_sum(unsummed));\n      return categorical_lpmf(y + 1, probs);\n    end\n    obtain_adjustments(W) = begin \n        local M, K = size(W)\n        adj = zeros((2, K))\n        adj[1,1] = 0\n        adj[2, 1] = 1\n        for k in 2:K\n            min_w = minimum(W[:,k])\n            max_w = maximum(W[:,k])\n            minmax_count = sum(w-&gt;w in (min_w, max_w), W[:, k])\n            if minmax_count == M\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = max_w - min_w;\n            else\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = sd(W[:, k]) * 2;\n            end\n        end\n        return adj\n    end\n        m = maximum(y)\n        adj = obtain_adjustments(W);\n        W_adj = @. ((W - adj[1:1,:])/adj[2:2,:])\n@stan begin \n            @parameters begin\n                alpha::vector(lower=0)[I];\n                beta_free::vector[I - 1] ;\n                kappa_free::vector[m - 1] ;\n                theta::vector[J];\n                lambda_adj::vector[K];\n            end\n            beta = vcat(beta_free, -sum(beta_free))\n            kappa = vcat(kappa_free, -sum(kappa_free))\n            @model @views begin\n                alpha ~ lognormal(1, 1);\n                target += normal_lpdf(beta, 0, 3);\n                target += normal_lpdf(kappa, 0, 3);\n                theta ~ normal(W_adj * lambda_adj, 1);\n                lambda_adj ~ student_t(3, 0, 1);\n                for n in 1:N\n                    target += rsm(y[n], theta[jj[n]] .* alpha[ii[n]], beta[ii[n]], kappa)\n                end\n            end\n    end\nend\njulia_implementation(::Val{:prophet};\n    T,\n    K,\n    t,\n    cap,\n    y,\n    S,\n    t_change,\n    X,\n    sigmas,\n    tau,\n    trend_indicator,\n    s_a,\n    s_m, \n    kwargs...) = begin \n    get_changepoint_matrix(t, t_change, T, S) = begin\n        local A = rep_matrix(0, T, S);\n        a_row = rep_row_vector(0, S);\n        cp_idx = 1;\n        \n        for i in 1:T\n          while ((cp_idx &lt;= S) && (t[i] &gt;= t_change[cp_idx])) \n            a_row[cp_idx] = 1;\n            cp_idx = cp_idx + 1;\n          end\n          A[i,:] = a_row;\n        end\n        return A;\n      end\n      \n      \n      logistic_gamma(k, m, delta, t_change, S) = begin\n        k_s = append_row(k, k + cumulative_sum(delta));\n        \n        m_pr = m; \n        for i in 1:S\n          gamma[i] = (t_change[i] - m_pr) * (1 - k_s[i] / k_s[i + 1]);\n          m_pr = m_pr + gamma[i]; \n        end\n        return gamma;\n      end\n      \n      logistic_trend(k, m, delta, t, cap, A, t_change, S) = begin\n        gamma = logistic_gamma(k, m, delta, t_change, S);\n        return cap .* inv_logit.((k .+ A * delta) .* (t .- m .- A * gamma));\n      end\n      \n      linear_trend(k, m, delta, t, A, t_change) = begin\n        return (k .+ A * delta) .* t .+ (m .+ A * (-t_change .* delta));\n      end\n        A = get_changepoint_matrix(t, t_change, T, S)\n@stan begin \n            @parameters begin\n                k::real\n                m::real\n                delta::vector[S]\n                sigma_obs::real(lower=0)\n                beta::vector[K]\n            end\n            @model begin\n                k ~ normal(0, 5);\n                m ~ normal(0, 5);\n                delta ~ double_exponential(0, tau);\n                sigma_obs ~ normal(0, 0.5);\n                beta ~ normal(0, sigmas);\n                \n                if trend_indicator == 0\n                    y ~ normal(linear_trend(k, m, delta, t, A, t_change)\n                             .* (1 .+ X * (beta .* s_m)) + X * (beta .* s_a), sigma_obs);\n                elseif trend_indicator == 1\n                    y ~ normal(logistic_trend(k, m, delta, t, cap, A, t_change, S)\n                             .* (1 .+ X * (beta .* s_m)) + X * (beta .* s_a), sigma_obs);\n                end\n            end\n        end\nend\n\njulia_implementation(::Val{:hmm_gaussian}; T, K, y, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            pi1::simplex[K]\n            A::simplex[K,K]\n            mu::ordered[K]\n            sigma::vector(lower=0)[K]\n        end\n        @model @views begin\n            logalpha = zeros((K,T))'\n            accumulator = zeros(K)\n            logalpha[1,:] .= log.(pi1) .+ normal_lpdf(y[1], mu, sigma);\n            for t in 2 : T\n                for j in 1:K\n                    for i in 1:K\n                        accumulator[i] = logalpha[t - 1, i] + log(A[i, j]) + normal_lpdf(y[t], mu[j], sigma[j]);\n                    end\n                logalpha[t, j] = log_sum_exp(accumulator);\n                end\n            end\n            target += log_sum_exp(logalpha[T,:]);\n        end\n    end\nend\n\njulia_implementation(::Val{:hmm_drive_0}; K, N, u, v, alpha, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta1::simplex[K]\n            theta2::simplex[K]\n            phi::positive_ordered[K]\n            lambda::positive_ordered[K]\n        end\n        theta = hcat(theta1, theta2)'\n        @model @views begin\n            for k in 1:K\n              target += dirichlet_lpdf(theta[k, :], alpha[k, :]);\n            end\n            target += normal_lpdf(phi[1], 0, 1);\n            target += normal_lpdf(phi[2], 3, 1);\n            target += normal_lpdf(lambda[1], 0, 1);\n            target += normal_lpdf(lambda[2], 3, 1);\n\n            acc = zeros(K)\n            gamma = zeros((K,N))'\n            gamma[1,:] .= @.(exponential_lpdf(u[1], phi) + exponential_lpdf(v[1], lambda))\n            for t in 2:N\n                for k in 1:K\n                    for j in 1:K\n                        acc[j] = gamma[t - 1, j] + log(theta[j, k]) + exponential_lpdf(u[t], phi[k]) + exponential_lpdf(v[t], lambda[k]);\n                    end\n                    gamma[t, k] = log_sum_exp(acc);\n                end\n            end\n            target += log_sum_exp(gamma[N, :])\n        end\n    end\nend\n\njulia_implementation(::Val{:hmm_drive_1}; K, N, u, v, alpha, tau, rho, kwargs...) = begin \n    @stan begin \n        @parameters begin\n            theta1::simplex[K]\n            theta2::simplex[K]\n            phi::ordered[K]\n            lambda::ordered[K]\n        end\n        theta = hcat(theta1, theta2)'\n        @model @views begin\n            for k in 1:K\n              target += dirichlet_lpdf(theta[k, :], alpha[k, :]);\n            end\n            target += normal_lpdf(phi[1], 0, 1);\n            target += normal_lpdf(phi[2], 3, 1);\n            target += normal_lpdf(lambda[1], 0, 1);\n            target += normal_lpdf(lambda[2], 3, 1);\n\n            acc = zeros(K)\n            gamma = zeros((K,N))'\n            gamma[1,:] .= @.(normal_lpdf(u[1], phi, tau) + normal_lpdf(v[1], lambda, rho))\n            for t in 2:N\n                for k in 1:K\n                    for j in 1:K\n                        acc[j] = gamma[t - 1, j] + log(theta[j, k]) + normal_lpdf(u[t], phi[k], tau) + normal_lpdf(v[t], lambda[k], rho);\n                    end\n                    gamma[t, k] = log_sum_exp(acc);\n                end\n            end\n            target += log_sum_exp(gamma[N, :])\n        end\n    end\nend\n\njulia_implementation(::Val{:iohmm_reg}; T, K, M, y, u, kwargs...) = begin \n    @inline normalize(x) = x ./ sum(x)\n    @stan begin \n        @parameters begin\n            pi1::simplex[K]\n            w::vector[K,M]\n            b::vector[K,M]\n            sigma::vector(lower=0)[K]\n        end\n        @model @views begin\n            unA = hcat(pi1, w * u'[:, 2:end])'\n            A = copy(unA)\n            for t in 2:T\n                A[t, :] .= softmax(unA[t, :])\n            end\n            logA = log.(A)\n            logoblik = normal_lpdf.(y, u * b', sigma')\n            accumulator = zeros(K)\n            logalpha = zeros((K,T))'\n            logalpha[1,:] .= @.(log(pi1) + logoblik[1,:])\n            for t in 2:T\n                for j in 1:K\n                    for i in 1:K\n                        accumulator[i] = logalpha[t - 1, i] + logA[t,i] + logoblik[t,j];\n                    end\n                    logalpha[t, j] = log_sum_exp(accumulator);\n                end\n            end\n            w ~ normal(0, 5);\n            b ~ normal(0, 5);\n            sigma ~ normal(0, 3);\n            target += log_sum_exp(logalpha[T,:]);\n        end\n    end\nend\n\njulia_implementation(::Val{:accel_gp}; N, Y, Kgp_1, Dgp_1, NBgp_1, Xgp_1, slambda_1, Kgp_sigma_1, Dgp_sigma_1, NBgp_sigma_1, Xgp_sigma_1, slambda_sigma_1, prior_only, kwargs...) = begin \n    @inline sqrt_spd_cov_exp_quad(slambda, sdgp, lscale) = begin \n        NB, D = size(slambda)\n        Dls = length(lscale)\n        if Dls == 1\n            constant = (sdgp) * (sqrt(2 * pi) * lscale[1]) ^ (D/2)\n            neg_half_lscale2 = -0.25 * square(lscale[1])\n            @.(constant * exp(neg_half_lscale2 * dot_self($eachrow(slambda))))\n        else\n            error()\n        end\n    end\n    @inline gpa(X, sdgp, lscale, zgp, slambda) = X * (sqrt_spd_cov_exp_quad(slambda, sdgp, lscale) .* zgp)\n    @stan begin \n        @parameters begin\n            Intercept::real\n            sdgp_1::real(lower=0)\n            lscale_1::real(lower=0)\n            zgp_1::vector[NBgp_1]\n            Intercept_sigma::real\n            sdgp_sigma_1::real(lower=0)\n            lscale_sigma_1::real(lower=0)\n            zgp_sigma_1::vector[NBgp_sigma_1]\n        end\n        vsdgp_1 = fill(sdgp_1, 1)\n        vlscale_1 = fill(lscale_1, (1, 1))\n        vsdgp_sigma_1 = fill(sdgp_sigma_1, 1)\n        vlscale_sigma_1 = fill(lscale_sigma_1, (1, 1))\n        @model @views begin\n            mu = Intercept .+ gpa(Xgp_1, vsdgp_1[1], vlscale_1[1,:], zgp_1, slambda_1);\n            sigma = exp.(Intercept_sigma .+ gpa(Xgp_sigma_1, vsdgp_sigma_1[1], vlscale_sigma_1[1,:], zgp_sigma_1, slambda_sigma_1));\n            target += student_t_lpdf(Intercept, 3, -13, 36);\n            target += student_t_lpdf(vsdgp_1, 3, 0, 36)\n                    #   - 1 * student_t_lccdf(0, 3, 0, 36);\n            target += normal_lpdf(zgp_1, 0, 1);\n            target += inv_gamma_lpdf(vlscale_1[1,:], 1.124909, 0.0177);\n            target += student_t_lpdf(Intercept_sigma, 3, 0, 10);\n            target += student_t_lpdf(vsdgp_sigma_1, 3, 0, 36)\n                    #   - 1 * student_t_lccdf(0, 3, 0, 36);\n            target += normal_lpdf(zgp_sigma_1, 0, 1);\n            target += inv_gamma_lpdf(vlscale_sigma_1[1,:], 1.124909, 0.0177);\n            if prior_only == 0\n              target += normal_lpdf(Y, mu, sigma);\n            end\n        end\n    end\nend\n\n\n\njulia_implementation(::Val{:hierarchical_gp};\n        N,\n        N_states,\n        N_regions,\n        N_years_obs,\n        N_years,\n        state_region_ind,\n        state_ind,\n        region_ind,\n        year_ind,\n        y,\n        kwargs...) = begin \n    @stan begin \n        years = 1:N_years\n        counts = fill(2, 17)\n        @parameters begin\n            GP_region_std::matrix[N_years, N_regions]\n            GP_state_std::matrix[N_years, N_states]\n            year_std::vector[N_years_obs]\n            state_std::vector[N_states]\n            region_std::vector[N_regions]\n            tot_var::real(lower=0)\n            prop_var::simplex[17]\n            mu::real\n            length_GP_region_long::real(lower=0)\n            length_GP_state_long::real(lower=0)\n            length_GP_region_short::real(lower=0)\n            length_GP_state_short::real(lower=0)\n        end\n\n\n        vars = 17 * prop_var * tot_var;\n        sigma_year = sqrt(vars[1]);\n        sigma_region = sqrt(vars[2]);\n        sigma_state = @.(sqrt(vars[3:end]))\n        \n        sigma_GP_region_long = sqrt(vars[13]);\n        sigma_GP_state_long = sqrt(vars[14]);\n        sigma_GP_region_short = sqrt(vars[15]);\n        sigma_GP_state_short = sqrt(vars[16]);\n        sigma_error_state_2 = sqrt(vars[17]);\n        \n        region_re = sigma_region * region_std;\n        year_re = sigma_year * year_std;\n        state_re = sigma_state[state_region_ind] .* state_std;\n        \n        begin\n            cov_region = gp_exp_quad_cov(years, sigma_GP_region_long,\n                                        length_GP_region_long)\n                        + gp_exp_quad_cov(years, sigma_GP_region_short,\n                                        length_GP_region_short);\n            cov_state = gp_exp_quad_cov(years, sigma_GP_state_long,\n                                        length_GP_state_long)\n                        + gp_exp_quad_cov(years, sigma_GP_state_short,\n                                        length_GP_state_short);\n            for year in 1 : N_years\n                cov_region[year, year] = cov_region[year, year] + 1e-6;\n                cov_state[year, year] = cov_state[year, year] + 1e-6;\n            end\n            \n            L_cov_region = cholesky_decompose(cov_region);\n            L_cov_state = cholesky_decompose(cov_state);\n            GP_region = L_cov_region * GP_region_std;\n            GP_state = L_cov_state * GP_state_std;\n        end\n        @model begin\n            obs_mu = zeros(N)\n            for n in 1 : N\n                obs_mu[n] = mu + year_re[year_ind[n]] + state_re[state_ind[n]]\n                            + region_re[region_ind[n]]\n                            + GP_region[year_ind[n], region_ind[n]]\n                            + GP_state[year_ind[n], state_ind[n]];\n                end\n                y ~ normal(obs_mu, sigma_error_state_2); \n                \n                (GP_region_std) ~ normal(0, 1);\n                (GP_state_std) ~ normal(0, 1);\n                year_std ~ normal(0, 1);\n                state_std ~ normal(0, 1);\n                region_std ~ normal(0, 1);\n                mu ~ normal(.5, .5);\n                tot_var ~ gamma(3, 3);\n                prop_var ~ dirichlet(counts);\n                length_GP_region_long ~ weibull(30, 8);\n                length_GP_state_long ~ weibull(30, 8);\n                length_GP_region_short ~ weibull(30, 3);\n                length_GP_state_short ~ weibull(30, 3);\n        end\n    end\nend\njulia_implementation(::Val{:kronecker_gp}; n1, n2, x1, y, kwargs...) = begin \n    kron_mvprod(A, B, V) = adjoint(A * adjoint(B * V))\n    calculate_eigenvalues(A, B, sigma2) = A .* B' .+ sigma2\n    xd  = -(x1 .- x1') .^ 2\n    return @stan begin end\n    @stan begin \n        @parameters begin\n            var1::real(lower=0)\n            bw1::real(lower=0)\n            L::cholesky_factor_corr[n2]\n            sigma1::real(lower=.00001)\n        end\n        Lambda = multiply_lower_tri_self_transpose(L)\n        Sigma1 = Symmetric(var1 .* exp.(xd .* bw1) + .00001 * I);\n        R1, Q1 = eigen(Sigma1);\n        R2, Q2 = eigen(Lambda);\n        eigenvalues = calculate_eigenvalues(R2, R1, sigma1);\n        @model @views begin\n            var1 ~ lognormal(0, 1);\n            bw1 ~ cauchy(0, 2.5);\n            sigma1 ~ lognormal(0, 1);\n            L ~ lkj_corr_cholesky(2);\n            target += -0.5 * sum(y .* kron_mvprod(Q1, Q2, kron_mvprod(transpose(Q1), transpose(Q2), y) ./ eigenvalues)) - 0.5 * sum(log(eigenvalues))\n        end\n    end\nend\n\n\njulia_implementation(::Val{:covid19imperial_v2}; M, P, N0, N, N2, cases, deaths, f, X, EpidemicStart, pop, SI, kwargs...) = begin \n    SI_rev = reverse(SI)\n    f_rev = mapreduce(reverse, hcat, eachcol(f))'\n    @stan begin \n        @parameters begin\n            mu::real(lower=0)[M]\n            alpha_hier::real(lower=0)[P]\n            kappa::real(lower=0)\n            y::real(lower=0)[M]\n            phi::real(lower=0)\n            tau::real(lower=0)\n            ifr_noise::real(lower=0)[M]\n        end\n        @model @views begin\n            prediction = rep_matrix(0., N2, M);\n            E_deaths = rep_matrix(0., N2, M);\n            Rt = rep_matrix(0., N2, M);\n            Rt_adj = copy(Rt);\n            cumm_sum = rep_matrix(0., N2, M);\n            alpha = alpha_hier .- (log(1.05) / 6.)\n            for m in 1:M\n                prediction[1:N0, m] .= y[m]\n                cumm_sum[2:N0, m] .= cumulative_sum(prediction[2:N0, m]);\n                Rt[:, m] .= mu[m] * exp.(-X[m,:,:] * alpha);\n                Rt_adj[1:N0, m] .= Rt[1:N0, m];\n                for i in (N0+1):N2\n                    convolution = dot_product(sub_col(prediction, 1, m, i - 1), stan_tail(SI_rev, i - 1))\n                    cumm_sum[i, m] = cumm_sum[i - 1, m] + prediction[i - 1, m];\n                    Rt_adj[i, m] = ((pop[m] - cumm_sum[i, m]) / pop[m]) * Rt[i, m];\n                    prediction[i, m] = Rt_adj[i, m] * convolution;\n                end\n                E_deaths[1, m] = 1e-15 * prediction[1, m];\n                for i in 2:N2\n                    E_deaths[i, m] = ifr_noise[m] * dot_product(sub_col(prediction, 1, m, i - 1), stan_tail(f_rev[m, :], i - 1));\n                end\n            end\n            tau ~ exponential(0.03);\n            y ~ exponential(1 / tau)\n            phi ~ normal(0, 5);\n            kappa ~ normal(0, 0.5);\n            mu ~ normal(3.28, kappa);\n            alpha_hier ~ gamma(.1667, 1);\n            ifr_noise ~ normal(1, 0.1);\n            for m in 1:M\n                deaths[EpidemicStart[m] : N[m], m] ~ neg_binomial_2(E_deaths[EpidemicStart[m] : N[m], m], phi);\n            end\n        end\n    end\nend\n\n\njulia_implementation(::Val{:covid19imperial_v3}; kwargs...) = julia_implementation(Val{:covid19imperial_v2}(); kwargs...)\n\njulia_implementation(::Val{:gpcm_latent_reg_irt}; I, J, N, ii, jj, y, K, W, kwargs...) = begin \n    pcm(y, theta, beta) = begin \n        unsummed = append_row(rep_vector(0.0, 1), theta .- beta)\n        probs = softmax(cumulative_sum(unsummed))\n        categorical_lpmf(y + 1, probs)\n    end\n    obtain_adjustments(W) = begin \n        local M, K = size(W)\n        adj = zeros((2, K))\n        adj[1,1] = 0\n        adj[2, 1] = 1\n        for k in 2:K\n            min_w = minimum(W[:,k])\n            max_w = maximum(W[:,k])\n            minmax_count = sum(w-&gt;w in (min_w, max_w), W[:, k])\n            if minmax_count == M\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = max_w - min_w;\n            else\n                adj[1, k] = mean(W[:, k]);\n                adj[2, k] = sd(W[:, k]) * 2;\n            end\n        end\n        return adj\n    end\n    m = fill(0, I)\n    pos = fill(1, I)\n    for n in 1:N\n        if y[n] &gt; m[ii[n]]\n            m[ii[n]] = y[n]\n        end\n    end\n    for i in 2:I\n        pos[i] = m[i - 1] + pos[i - 1]\n    end\n    adj = obtain_adjustments(W);\n    W_adj = @. ((W - adj[1:1,:])/adj[2:2,:])\n    @stan begin \n        @parameters begin\n            alpha::real(lower=0)[I]\n            beta_free::vector[sum(m) - 1]\n            theta::vector[J]\n            lambda_adj::vector[K]\n        end\n        beta = vcat(beta_free, -sum(beta_free))\n        @model @views begin\n            alpha ~ lognormal(1, 1);\n            target += normal_lpdf(beta, 0, 3);\n            theta ~ normal(W_adj * lambda_adj, 1);\n            lambda_adj ~ student_t(3, 0, 1);\n            for n in 1:N\n                target += pcm(y[n], theta[jj[n]] .* alpha[ii[n]], segment(beta, pos[ii[n]], m[ii[n]]));\n            end\n        end\n    end\nend\nend"
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "Julia vs Stan performance comparison",
    "section": "",
    "text": "Warning\n\n\n\nALL COMPARISONS ARE PRELIMINARY, INTERPRET THEM WITH A BIG HELPING OF SALT. WILL ELABORATE.\n\n\n\nRuntime overview\nThe below plot shows the relative primitive runtime (x-axis, Julia vs Stan, left: Julia is faster) and the relative gradient runtime (y-axis, Julia+Enzyme vs Stan, bottom: Julia is faster) for the posteriordb models for which the overview table has a yes in the usable column. The color of the dots represents the posterior dimension. Hovering over the data points will show the posterior name, its dimension, the allocations required by Julia during the primitive run and a short explanation, e.g. for the topmost point: mesquite-logmesquite_logvash (D=7, #allocs=0-&gt;70) - Julia's primitive is ~4.5 times faster, but Julia's gradient is ~16.0 times slower.\n\n\n\n    \n    \n\n\n\n\n\nPrimitive runtime comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nposterior name\ndimension\nStan mean relative runtime\nJulia mean relative runtime\nJulia allocations\nimplementations\n\n\n\n\nGLMM_Poisson_data-GLMM_Poisson_model\n45\n3.5 ± 0.0095\n1.0 ± 0.0022\n0.0\n\nStan, Julia\n\n\n\nGLMM_data-GLMM1_model\n237\n3.5 ± 0.016\n1.0 ± 0.0023\n0.0\n\nStan, Julia\n\n\n\nGLM_Binomial_data-GLM_Binomial_model\n3\n1.6 ± 0.0053\n1.0 ± 0.0023\n0.0\n\nStan, Julia\n\n\n\nGLM_Poisson_Data-GLM_Poisson_model\n4\n2.9 ± 0.0089\n1.0 ± 0.0024\n0.0\n\nStan, Julia\n\n\n\nM0_data-M0_model\n2\n1.2 ± 0.0064\n1.0 ± 0.0022\n0.0\n\nStan, Julia\n\n\n\nRate_1_data-Rate_1_model\n1\n2.6 ± 0.0058\n1.0 ± 0.0013\n0.0\n\nStan, Julia\n\n\n\nRate_2_data-Rate_2_model\n2\n2.0 ± 0.0037\n1.0 ± 0.0027\n0.0\n\nStan, Julia\n\n\n\nRate_3_data-Rate_3_model\n1\n1.8 ± 0.004\n1.0 ± 0.0019\n0.0\n\nStan, Julia\n\n\n\nRate_4_data-Rate_4_model\n2\n2.6 ± 0.0061\n1.0 ± 0.0027\n0.0\n\nStan, Julia\n\n\n\nRate_5_data-Rate_5_model\n1\n2.1 ± 0.0046\n1.0 ± 0.0025\n0.0\n\nStan, Julia\n\n\n\ndogs-dogs\n3\n4.9 ± 0.035\n1.0 ± 0.0031\n0.0\n\nStan, Julia\n\n\n\ndogs-dogs_hierarchical\n2\n2.5 ± 0.024\n1.0 ± 0.0038\n0.0\n\nStan, Julia\n\n\n\ndogs-dogs_log\n2\n2.7 ± 0.016\n1.0 ± 0.002\n0.0\n\nStan, Julia\n\n\n\ndugongs_data-dugongs_model\n4\n1.8 ± 0.006\n1.0 ± 0.0024\n0.0\n\nStan, Julia\n\n\n\nearnings-earn_height\n3\n8.2 ± 0.031\n1.0 ± 0.0034\n0.0\n\nStan, Julia\n\n\n\nearnings-log10earn_height\n3\n9.2 ± 0.079\n1.0 ± 0.0029\n0.0\n\nStan, Julia\n\n\n\nearnings-logearn_height\n3\n8.4 ± 0.039\n1.0 ± 0.0051\n0.0\n\nStan, Julia\n\n\n\nearnings-logearn_height_male\n4\n2.9 ± 0.0099\n1.0 ± 0.0023\n0.0\n\nStan, Julia\n\n\n\nearnings-logearn_interaction\n5\n3.1 ± 0.0093\n1.0 ± 0.002\n0.0\n\nStan, Julia\n\n\n\nearnings-logearn_interaction_z\n5\n3.1 ± 0.011\n1.0 ± 0.0021\n0.0\n\nStan, Julia\n\n\n\nearnings-logearn_logheight_male\n4\n2.5 ± 0.011\n1.0 ± 0.003\n0.0\n\nStan, Julia\n\n\n\ngarch-garch11\n4\n3.5 ± 0.014\n1.0 ± 0.0021\n0.0\n\nStan, Julia\n\n\n\nkidiq_with_mom_work-kidscore_interaction_c\n5\n2.9 ± 0.012\n1.0 ± 0.0062\n0.0\n\nStan, Julia\n\n\n\nkidiq_with_mom_work-kidscore_interaction_c2\n5\n3.5 ± 0.012\n1.0 ± 0.0027\n0.0\n\nStan, Julia\n\n\n\nkidiq_with_mom_work-kidscore_interaction_z\n5\n3.2 ± 0.012\n1.0 ± 0.0032\n0.0\n\nStan, Julia\n\n\n\nkidiq_with_mom_work-kidscore_mom_work\n5\n3.4 ± 0.011\n1.0 ± 0.0028\n0.0\n\nStan, Julia\n\n\n\nkilpisjarvi_mod-kilpisjarvi\n3\n7.9 ± 0.029\n1.0 ± 0.0027\n0.0\n\nStan, Julia\n\n\n\nmesquite-logmesquite\n8\n4.9 ± 0.014\n1.0 ± 0.0023\n0.0\n\nStan, Julia\n\n\n\nmesquite-logmesquite_logva\n5\n3.6 ± 0.013\n1.0 ± 0.002\n0.0\n\nStan, Julia\n\n\n\nmesquite-logmesquite_logvas\n8\n4.6 ± 0.012\n1.0 ± 0.0028\n0.0\n\nStan, Julia\n\n\n\nmesquite-logmesquite_logvash\n7\n4.6 ± 0.023\n1.0 ± 0.0027\n0.0\n\nStan, Julia\n\n\n\nmesquite-logmesquite_logvolume\n3\n9.0 ± 0.026\n1.0 ± 0.0015\n0.0\n\nStan, Julia\n\n\n\nmesquite-mesquite\n8\n4.5 ± 0.014\n1.0 ± 0.0023\n0.0\n\nStan, Julia\n\n\n\nnes1972-nes\n10\n3.9 ± 0.068\n1.0 ± 0.0026\n0.0\n\nStan, Julia\n\n\n\nnes1976-nes\n10\n4.5 ± 0.076\n1.0 ± 0.0031\n0.0\n\nStan, Julia\n\n\n\nnes1980-nes\n10\n4.1 ± 0.019\n1.0 ± 0.0019\n0.0\n\nStan, Julia\n\n\n\nnes1984-nes\n10\n4.1 ± 0.069\n1.0 ± 0.0037\n0.0\n\nStan, Julia\n\n\n\nnes1988-nes\n10\n4.4 ± 0.065\n1.0 ± 0.0028\n0.0\n\nStan, Julia\n\n\n\nnes1992-nes\n10\n4.0 ± 0.071\n1.0 ± 0.0023\n0.0\n\nStan, Julia\n\n\n\nnes1996-nes\n10\n4.2 ± 0.055\n1.0 ± 0.0032\n0.0\n\nStan, Julia\n\n\n\nnes2000-nes\n10\n3.6 ± 0.014\n1.0 ± 0.0036\n0.0\n\nStan, Julia\n\n\n\nnormal_2-normal_mixture\n3\n2.7 ± 0.021\n1.0 ± 0.0038\n0.0\n\nStan, Julia\n\n\n\nradon_all-radon_county_intercept\n388\n5.6 ± 0.3\n1.0 ± 0.0043\n0.0\n\nStan, Julia\n\n\n\nradon_all-radon_hierarchical_intercept_centered\n391\n6.8 ± 0.45\n1.0 ± 0.0063\n0.0\n\nStan, Julia\n\n\n\nradon_all-radon_partially_pooled_centered\n389\n5.3 ± 0.3\n1.0 ± 0.0061\n0.0\n\nStan, Julia\n\n\n\nradon_all-radon_pooled\n3\n36.0 ± 2.0\n1.0 ± 0.0037\n0.0\n\nStan, Julia\n\n\n\nradon_all-radon_variable_intercept_centered\n390\n6.3 ± 0.37\n1.0 ± 0.005\n0.0\n\nStan, Julia\n\n\n\nradon_all-radon_variable_intercept_slope_centered\n777\n6.0 ± 0.38\n1.0 ± 0.0061\n0.0\n\nStan, Julia\n\n\n\nradon_all-radon_variable_slope_centered\n390\n6.4 ± 0.38\n1.0 ± 0.0046\n0.0\n\nStan, Julia\n\n\n\nradon_mn-radon_county_intercept\n87\n4.3 ± 0.018\n1.0 ± 0.0026\n0.0\n\nStan, Julia\n\n\n\nradon_mn-radon_hierarchical_intercept_centered\n90\n4.6 ± 0.018\n1.0 ± 0.0019\n0.0\n\nStan, Julia\n\n\n\nradon_mn-radon_partially_pooled_centered\n88\n3.7 ± 0.015\n1.0 ± 0.0027\n0.0\n\nStan, Julia\n\n\n\nradon_mn-radon_pooled\n3\n25.0 ± 0.092\n1.0 ± 0.0033\n0.0\n\nStan, Julia\n\n\n\nradon_mn-radon_variable_intercept_centered\n89\n4.4 ± 0.012\n1.0 ± 0.002\n0.0\n\nStan, Julia\n\n\n\nradon_mn-radon_variable_intercept_slope_centered\n175\n3.8 ± 0.012\n1.0 ± 0.0021\n0.0\n\nStan, Julia\n\n\n\nradon_mn-radon_variable_slope_centered\n89\n4.0 ± 0.011\n1.0 ± 0.0024\n0.0\n\nStan, Julia\n\n\n\nrats_data-rats_model\n65\n3.9 ± 0.014\n1.0 ± 0.002\n0.0\n\nStan, Julia\n\n\n\nseeds_data-seeds_model\n26\n1.6 ± 0.0048\n1.0 ± 0.0023\n0.0\n\nStan, Julia\n\n\n\nsesame_data-sesame_one_pred_a\n3\n8.6 ± 0.025\n1.0 ± 0.002\n0.0\n\nStan, Julia\n\n\n\nsurgical_data-surgical_model\n14\n2.2 ± 0.012\n1.0 ± 0.0045\n0.0\n\nStan, Julia\n\n\n\nwells_data-wells_dist\n2\n1.9 ± 0.0076\n1.0 ± 0.0068\n0.0\n\nStan, Julia\n\n\n\nMh_data-Mh_model\n388\n1.6 ± 0.0047\n1.0 ± 0.0026\n1.0\n\nStan, Julia\n\n\n\nMt_data-Mt_model\n4\n1.8 ± 0.0095\n1.0 ± 0.0029\n1.0\n\nStan, Julia\n\n\n\nSurvey_data-Survey_model\n1\n1.1 ± 0.0069\n1.0 ± 0.0057\n1.0\n\nStan, Julia\n\n\n\nlow_dim_gauss_mix_collapse-low_dim_gauss_mix_collapse\n5\n2.1 ± 0.016\n1.0 ± 0.0024\n1.0\n\nStan, Julia\n\n\n\nradon_all-radon_hierarchical_intercept_noncentered\n391\n7.1 ± 0.49\n1.0 ± 0.0047\n1.0\n\nStan, Julia\n\n\n\nradon_all-radon_partially_pooled_noncentered\n389\n5.2 ± 0.24\n1.0 ± 0.0045\n1.0\n\nStan, Julia\n\n\n\nradon_all-radon_variable_intercept_noncentered\n390\n6.6 ± 0.4\n1.0 ± 0.0046\n1.0\n\nStan, Julia\n\n\n\nradon_all-radon_variable_slope_noncentered\n390\n6.6 ± 0.38\n1.0 ± 0.0034\n1.0\n\nStan, Julia\n\n\n\nradon_mn-radon_hierarchical_intercept_noncentered\n90\n5.1 ± 0.044\n1.0 ± 0.0024\n1.0\n\nStan, Julia\n\n\n\nradon_mn-radon_partially_pooled_noncentered\n88\n3.7 ± 0.011\n1.0 ± 0.0029\n1.0\n\nStan, Julia\n\n\n\nradon_mn-radon_variable_intercept_noncentered\n89\n4.3 ± 0.019\n1.0 ± 0.0036\n1.0\n\nStan, Julia\n\n\n\nradon_mn-radon_variable_slope_noncentered\n89\n4.5 ± 0.02\n1.0 ± 0.0025\n1.0\n\nStan, Julia\n\n\n\nsblrc-blr\n6\n2.4 ± 0.028\n1.0 ± 0.35\n1.0\n\nStan, Julia\n\n\n\nsblri-blr\n6\n1.8 ± 0.0052\n1.0 ± 0.25\n1.0\n\nStan, Julia\n\n\n\nnes_logit_data-nes_logit_model\n2\n1.0 ± 0.003\n1.2 ± 0.0049\n2.0\n\nStan, Julia\n\n\n\nnormal_5-normal_mixture_k\n14\n2.3 ± 0.13\n1.0 ± 0.0057\n2.0\n\nStan, Julia\n\n\n\npilots-pilots\n18\n1.8 ± 0.0052\n1.0 ± 0.005\n2.0\n\nStan, Julia\n\n\n\nradon_all-radon_variable_intercept_slope_noncentered\n777\n5.8 ± 0.34\n1.0 ± 0.0051\n2.0\n\nStan, Julia\n\n\n\nradon_mn-radon_variable_intercept_slope_noncentered\n175\n4.2 ± 0.017\n1.0 ± 0.0034\n2.0\n\nStan, Julia\n\n\n\nradon_mod-radon_county\n389\n2.0 ± 0.018\n1.0 ± 0.18\n2.0\n\nStan, Julia\n\n\n\nthree_docs1200-ldaK2\n7\n1.0 ± 0.013\n1.1 ± 0.42\n2.0\n\nStan, Julia\n\n\n\nthree_men1-ldaK2\n502\n1.8 ± 0.082\n1.0 ± 0.0066\n2.0\n\nStan, Julia\n\n\n\nthree_men2-ldaK2\n510\n1.8 ± 0.077\n1.0 ± 0.0086\n2.0\n\nStan, Julia\n\n\n\nthree_men3-ldaK2\n505\n1.6 ± 0.095\n1.0 ± 0.025\n2.0\n\nStan, Julia\n\n\n\nMb_data-Mb_model\n3\n3.0 ± 0.053\n1.0 ± 0.0061\n3.0\n\nStan, Julia\n\n\n\nMth_data-Mth_model\n394\n1.8 ± 0.016\n1.0 ± 0.0058\n3.0\n\nStan, Julia\n\n\n\nbones_data-bones_model\n13\n3.7 ± 0.043\n1.0 ± 0.006\n3.0\n\nStan, Julia\n\n\n\nprideprejudice_chapter-ldaK5\n7714\n1.8 ± 0.18\n1.0 ± 0.03\n3.0\n\nStan, Julia\n\n\n\nuk_drivers-state_space_stochastic_level_stochastic_seasonal\n389\n3.1 ± 0.024\n1.0 ± 0.0037\n3.0\n\nStan, Julia\n\n\n\ndiamonds-diamonds\n26\n1.0 ± 0.005\n1.2 ± 0.37\n4.0\n\nStan, Julia\n\n\n\nlow_dim_gauss_mix-low_dim_gauss_mix\n5\n2.2 ± 0.025\n1.0 ± 0.0041\n4.0\n\nStan, Julia\n\n\n\novarian-logistic_regression_rhs\n3075\n1.7 ± 0.068\n1.0 ± 0.0084\n4.0\n\nStan, Julia\n\n\n\nprideprejudice_paragraph-ldaK5\n15570\n2.1 ± 0.16\n1.0 ± 0.033\n4.0\n\nStan, Julia\n\n\n\nwells_data-wells_daae_c_model\n6\n1.0 ± 0.0063\n1.5 ± 0.32\n4.0\n\nStan, Julia\n\n\n\nwells_data-wells_dae_c_model\n5\n1.0 ± 0.0055\n1.1 ± 0.0058\n4.0\n\nStan, Julia\n\n\n\nwells_data-wells_dae_inter_model\n7\n1.0 ± 0.0059\n1.1 ± 0.0055\n4.0\n\nStan, Julia\n\n\n\nwells_data-wells_dae_model\n4\n1.0 ± 0.0045\n1.4 ± 0.2\n4.0\n\nStan, Julia\n\n\n\nwells_data-wells_dist100_model\n2\n1.0 ± 0.0078\n1.3 ± 0.19\n4.0\n\nStan, Julia\n\n\n\nwells_data-wells_dist100ars_model\n3\n1.0 ± 0.0036\n1.1 ± 0.0067\n4.0\n\nStan, Julia\n\n\n\nwells_data-wells_interaction_c_model\n4\n1.0 ± 0.005\n1.1 ± 0.0078\n4.0\n\nStan, Julia\n\n\n\nwells_data-wells_interaction_model\n4\n1.0 ± 0.0052\n1.4 ± 0.3\n4.0\n\nStan, Julia\n\n\n\nMtbh_data-Mtbh_model\n154\n1.4 ± 0.0071\n1.0 ± 0.34\n6.0\n\nStan, Julia\n\n\n\ngp_pois_regr-gp_pois_regr\n13\n1.0 ± 0.0039\n1.1 ± 0.21\n6.0\n\nStan, Julia\n\n\n\nprostate-logistic_regression_rhs\n11935\n2.0 ± 0.2\n1.0 ± 0.029\n6.0\n\nStan, Julia\n\n\n\ntraffic_accident_nyc-bym2_offset_only\n3845\n2.6 ± 0.06\n1.0 ± 0.13\n6.0\n\nStan, Julia\n\n\n\nhmm_example-hmm_example\n4\n3.3 ± 0.028\n1.0 ± 0.0027\n7.0\n\nStan, Julia\n\n\n\nhmm_gaussian_simulated-hmm_gaussian\n14\n4.0 ± 0.21\n1.0 ± 0.0056\n8.0\n\nStan, Julia\n\n\n\nbball_drive_event_0-hmm_drive_0\n6\n3.3 ± 0.11\n1.0 ± 0.0055\n9.0\n\nStan, Julia\n\n\n\ngp_pois_regr-gp_regr\n3\n1.1 ± 0.0044\n1.0 ± 0.27\n9.0\n\nStan, Julia\n\n\n\nelection88-election88_full\n90\n2.9 ± 0.24\n1.0 ± 0.016\n10.0\n\nStan, Julia\n\n\n\nbball_drive_event_1-hmm_drive_1\n6\n4.0 ± 0.15\n1.0 ± 0.0044\n11.0\n\nStan, Julia\n\n\n\nmcycle_splines-accel_splines\n82\n1.5 ± 0.0038\n1.0 ± 0.31\n11.0\n\nStan, Julia\n\n\n\nmcycle_gp-accel_gp\n66\n2.4 ± 0.0092\n1.0 ± 0.0047\n12.0\n\nStan, Julia\n\n\n\nlsat_data-lsat_model\n1006\n1.3 ± 0.018\n1.0 ± 0.18\n15.0\n\nStan, Julia\n\n\n\necdc0401-covid19imperial_v2\n51\n3.3 ± 0.2\n1.0 ± 0.008\n80.0\n\nStan, Julia\n\n\n\necdc0501-covid19imperial_v2\n51\n1.0 ± 0.065\n18.0 ± 18.0\n80.0\n\nStan, Julia\n\n\n\niohmm_reg_simulated-iohmm_reg\n29\n4.4 ± 0.38\n1.0 ± 0.36\n512.0\n\nStan, Julia\n\n\n\n\n\n\n\n\n\nGradient runtime comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nposterior name\ndimension\nStan mean relative runtime\nJulia+Enzyme mean relative runtime\nJulia+Enzyme allocations\nimplementations\nJulia version\n\n\n\n\nGLMM_Poisson_data-GLMM_Poisson_model\n45\n1.6 ± 0.0053\n1.0 ± 0.0024\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nGLMM_data-GLMM1_model\n237\n1.8 ± 0.012\n1.0 ± 0.0038\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nGLM_Binomial_data-GLM_Binomial_model\n3\n1.0 ± 0.0037\n1.1 ± 0.0021\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nGLM_Poisson_Data-GLM_Poisson_model\n4\n1.7 ± 0.0052\n1.0 ± 0.0018\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nM0_data-M0_model\n2\n1.0 ± 0.0055\n1.0 ± 0.0018\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nRate_1_data-Rate_1_model\n1\n2.8 ± 0.0094\n1.0 ± 0.0017\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nRate_2_data-Rate_2_model\n2\n2.2 ± 0.0059\n1.0 ± 0.0023\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nRate_3_data-Rate_3_model\n1\n2.1 ± 0.0066\n1.0 ± 0.0022\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nRate_4_data-Rate_4_model\n2\n2.9 ± 0.0098\n1.0 ± 0.003\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nRate_5_data-Rate_5_model\n1\n2.6 ± 0.0059\n1.0 ± 0.0025\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\ndogs-dogs\n3\n3.6 ± 0.04\n1.0 ± 0.0054\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\ndogs-dogs_hierarchical\n2\n1.0 ± 0.011\n1.5 ± 0.0083\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\ndogs-dogs_log\n2\n1.9 ± 0.014\n1.0 ± 0.0032\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\ndugongs_data-dugongs_model\n4\n1.3 ± 0.0062\n1.0 ± 0.0031\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nearnings-earn_height\n3\n5.5 ± 0.026\n1.0 ± 0.0023\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nearnings-log10earn_height\n3\n3.5 ± 0.013\n1.0 ± 0.0026\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nearnings-logearn_height\n3\n2.5 ± 0.012\n1.0 ± 0.0073\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nearnings-logearn_height_male\n4\n4.8 ± 0.024\n1.0 ± 0.0033\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nearnings-logearn_interaction\n5\n5.7 ± 0.034\n1.0 ± 0.0028\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nearnings-logearn_interaction_z\n5\n5.1 ± 0.025\n1.0 ± 0.0029\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nearnings-logearn_logheight_male\n4\n4.7 ± 0.023\n1.0 ± 0.0038\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\ngarch-garch11\n4\n2.9 ± 0.012\n1.0 ± 0.0022\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nkidiq_with_mom_work-kidscore_interaction_c\n5\n4.9 ± 0.031\n1.0 ± 0.0041\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nkidiq_with_mom_work-kidscore_interaction_c2\n5\n4.1 ± 0.016\n1.0 ± 0.0052\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nkidiq_with_mom_work-kidscore_interaction_z\n5\n4.0 ± 0.017\n1.0 ± 0.0052\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nkidiq_with_mom_work-kidscore_mom_work\n5\n4.3 ± 0.021\n1.0 ± 0.0051\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nkilpisjarvi_mod-kilpisjarvi\n3\n3.6 ± 0.026\n1.0 ± 0.0021\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nmesquite-logmesquite_logva\n5\n4.2 ± 0.02\n1.0 ± 0.002\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nmesquite-logmesquite_logvolume\n3\n2.9 ± 0.0089\n1.0 ± 0.0021\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nnormal_2-normal_mixture\n3\n1.7 ± 0.019\n1.0 ± 0.0036\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_all-radon_county_intercept\n388\n7.8 ± 0.6\n1.0 ± 0.0054\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_all-radon_hierarchical_intercept_centered\n391\n11.0 ± 1.0\n1.0 ± 0.0048\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_all-radon_partially_pooled_centered\n389\n5.1 ± 0.3\n1.0 ± 0.0066\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_all-radon_pooled\n3\n20.0 ± 1.8\n1.0 ± 0.0043\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_all-radon_variable_intercept_centered\n390\n10.0 ± 1.2\n1.0 ± 0.007\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_all-radon_variable_intercept_slope_centered\n777\n6.8 ± 0.61\n1.0 ± 0.0056\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_all-radon_variable_slope_centered\n390\n7.5 ± 0.6\n1.0 ± 0.0083\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_mn-radon_county_intercept\n87\n5.0 ± 0.036\n1.0 ± 0.0023\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_mn-radon_hierarchical_intercept_centered\n90\n5.5 ± 0.052\n1.0 ± 0.0027\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_mn-radon_partially_pooled_centered\n88\n4.0 ± 0.021\n1.0 ± 0.0022\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_mn-radon_pooled\n3\n9.1 ± 0.044\n1.0 ± 0.0031\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_mn-radon_variable_intercept_centered\n89\n4.7 ± 0.032\n1.0 ± 0.0036\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_mn-radon_variable_intercept_slope_centered\n175\n4.1 ± 0.033\n1.0 ± 0.0024\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_mn-radon_variable_slope_centered\n89\n5.0 ± 0.029\n1.0 ± 0.0023\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nrats_data-rats_model\n65\n3.3 ± 0.014\n1.0 ± 0.0021\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nseeds_data-seeds_model\n26\n1.1 ± 0.0051\n1.0 ± 0.0024\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nsesame_data-sesame_one_pred_a\n3\n4.4 ± 0.013\n1.0 ± 0.002\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nsurgical_data-surgical_model\n14\n1.1 ± 0.0024\n1.0 ± 0.0021\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nwells_data-wells_dist\n2\n1.0 ± 0.0066\n1.1 ± 0.0071\n0.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nMh_data-Mh_model\n388\n1.1 ± 0.0047\n1.0 ± 0.0038\n2.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nlow_dim_gauss_mix_collapse-low_dim_gauss_mix_collapse\n5\n1.4 ± 0.016\n1.0 ± 0.0039\n2.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_all-radon_hierarchical_intercept_noncentered\n391\n12.0 ± 1.2\n1.0 ± 0.0058\n2.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_all-radon_partially_pooled_noncentered\n389\n5.9 ± 0.42\n1.0 ± 0.0051\n2.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_all-radon_variable_intercept_noncentered\n390\n8.0 ± 0.69\n1.0 ± 0.0051\n2.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_all-radon_variable_slope_noncentered\n390\n7.7 ± 0.65\n1.0 ± 0.0046\n2.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_mn-radon_hierarchical_intercept_noncentered\n90\n5.7 ± 0.038\n1.0 ± 0.0024\n2.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_mn-radon_partially_pooled_noncentered\n88\n3.5 ± 0.015\n1.0 ± 0.0021\n2.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_mn-radon_variable_intercept_noncentered\n89\n4.8 ± 0.027\n1.0 ± 0.0021\n2.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_mn-radon_variable_slope_noncentered\n89\n4.2 ± 0.027\n1.0 ± 0.0026\n2.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nsblrc-blr\n6\n1.0 ± 0.0051\n1.0 ± 0.0055\n2.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nsblri-blr\n6\n1.0 ± 0.0034\n2.0 ± 0.67\n2.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_mod-radon_county\n389\n1.0 ± 0.014\n1.4 ± 0.012\n2.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nnes_logit_data-nes_logit_model\n2\n1.0 ± 0.004\n2.8 ± 0.011\n4.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\npilots-pilots\n18\n1.0 ± 0.0027\n1.5 ± 0.0048\n4.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_all-radon_variable_intercept_slope_noncentered\n777\n5.9 ± 0.52\n1.0 ± 0.014\n4.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nradon_mn-radon_variable_intercept_slope_noncentered\n175\n3.5 ± 0.021\n1.0 ± 0.0024\n4.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nbones_data-bones_model\n13\n1.0 ± 0.013\n2.3 ± 0.9\n6.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\ndiamonds-diamonds\n26\n1.0 ± 0.0098\n4.5 ± 0.19\n8.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nwells_data-wells_daae_c_model\n6\n1.0 ± 0.0048\n2.5 ± 0.02\n8.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nwells_data-wells_dae_c_model\n5\n1.0 ± 0.0039\n2.6 ± 0.017\n8.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nwells_data-wells_dae_inter_model\n7\n1.0 ± 0.0046\n2.9 ± 0.033\n8.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nwells_data-wells_dae_model\n4\n1.0 ± 0.0049\n2.5 ± 0.017\n8.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nwells_data-wells_dist100_model\n2\n1.0 ± 0.0053\n2.9 ± 0.021\n8.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nwells_data-wells_dist100ars_model\n3\n1.0 ± 0.0049\n2.6 ± 0.021\n8.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nwells_data-wells_interaction_c_model\n4\n1.0 ± 0.0054\n4.0 ± 1.1\n8.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nwells_data-wells_interaction_model\n4\n1.0 ± 0.0049\n2.9 ± 0.026\n8.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\ntraffic_accident_nyc-bym2_offset_only\n3845\n1.0 ± 0.052\n1.0 ± 0.011\n8.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nlow_dim_gauss_mix-low_dim_gauss_mix\n5\n1.3 ± 0.016\n1.0 ± 0.0053\n9.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nnormal_5-normal_mixture_k\n14\n1.0 ± 0.082\n3.4 ± 0.34\n10.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\ngp_pois_regr-gp_pois_regr\n13\n1.0 ± 0.0041\n2.2 ± 0.0097\n10.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nelection88-election88_full\n90\n1.8 ± 0.23\n1.0 ± 0.073\n10.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\ngp_pois_regr-gp_regr\n3\n1.0 ± 0.0059\n1.8 ± 0.0086\n15.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nhmm_gaussian_simulated-hmm_gaussian\n14\n1.0 ± 0.089\n2.2 ± 0.25\n17.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nmcycle_splines-accel_splines\n82\n1.0 ± 0.0066\n3.7 ± 2.5\n22.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nhmm_example-hmm_example\n4\n1.0 ± 0.007\n2.8 ± 0.059\n23.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nprideprejudice_chapter-ldaK5\n7714\n1.0 ± 0.057\n2.5 ± 0.14\n24.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nmcycle_gp-accel_gp\n66\n1.1 ± 0.0063\n1.0 ± 0.0059\n24.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\novarian-logistic_regression_rhs\n3075\n1.0 ± 0.043\n2.0 ± 0.058\n25.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nMt_data-Mt_model\n4\n1.0 ± 0.0062\n4.9 ± 0.082\n26.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nprostate-logistic_regression_rhs\n11935\n1.0 ± 0.13\n2.8 ± 1.1\n29.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nMth_data-Mth_model\n394\n1.0 ± 0.0098\n3.0 ± 0.1\n30.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nMb_data-Mb_model\n3\n1.0 ± 0.0092\n3.4 ± 0.13\n32.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nprideprejudice_paragraph-ldaK5\n15570\n1.0 ± 0.028\n2.7 ± 0.025\n32.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nthree_docs1200-ldaK2\n7\n1.0 ± 0.018\n6.2 ± 0.48\n34.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nthree_men1-ldaK2\n502\n1.0 ± 0.082\n4.6 ± 1.5\n34.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nthree_men2-ldaK2\n510\n1.0 ± 0.078\n3.9 ± 0.32\n34.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nthree_men3-ldaK2\n505\n1.0 ± 0.078\n3.8 ± 0.37\n34.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nMtbh_data-Mtbh_model\n154\n1.0 ± 0.011\n3.9 ± 1.7\n38.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nSurvey_data-Survey_model\n1\n1.0 ± 0.0046\n4.3 ± 0.22\n40.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nbball_drive_event_0-hmm_drive_0\n6\n1.0 ± 0.085\n2.3 ± 0.23\n40.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nbball_drive_event_1-hmm_drive_1\n6\n1.0 ± 0.055\n3.4 ± 0.26\n44.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nnes2000-nes\n10\n1.0 ± 0.0039\n6.3 ± 0.053\n56.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nmesquite-logmesquite_logvash\n7\n1.0 ± 0.0039\n19.0 ± 7.6\n70.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nmesquite-mesquite\n8\n1.0 ± 0.0032\n9.5 ± 0.041\n70.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nlsat_data-lsat_model\n1006\n1.0 ± 0.031\n1.4 ± 0.018\n74.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nmesquite-logmesquite\n8\n1.0 ± 0.0033\n9.6 ± 0.035\n84.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nmesquite-logmesquite_logvas\n8\n1.0 ± 0.0032\n11.0 ± 0.048\n84.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nnes1972-nes\n10\n1.0 ± 0.023\n6.2 ± 0.15\n88.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nnes1976-nes\n10\n1.0 ± 0.039\n6.3 ± 0.18\n88.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nnes1980-nes\n10\n1.0 ± 0.01\n6.8 ± 0.11\n88.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nnes1984-nes\n10\n1.0 ± 0.025\n5.4 ± 0.088\n88.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nnes1988-nes\n10\n1.0 ± 0.026\n7.3 ± 0.22\n88.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nnes1992-nes\n10\n1.0 ± 0.022\n6.8 ± 1.2\n88.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\nnes1996-nes\n10\n1.0 ± 0.02\n5.2 ± 0.068\n88.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21\n\n\niohmm_reg_simulated-iohmm_reg\n29\n1.0 ± 0.11\n1.4 ± 0.13\n1126.0\n\nStan, Julia\n\nJulia 1.10.7 + Enzyme 0.13.21"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "StanBlocks.jl",
    "section": "",
    "text": "Implements many - but currently not all - of the Bayesian models in posteriordb by implementing Julia macros and functions which mimick Stan blocks and functions respectively, with relatively light dependencies. Using the macros and functions defined in this package, the “shortest” posteriordb model (earn_height.stan)\nbecomes\nInstantiating the posterior (i.e. model + data) requires loading PosteriorDB.jl, which provides access to the datasets, e.g. to load the earnings-earn_height posterior (earn_height model + earning data):"
  },
  {
    "objectID": "index.html#differences-in-the-returned-log-density",
    "href": "index.html#differences-in-the-returned-log-density",
    "title": "StanBlocks.jl",
    "section": "Differences in the returned log-density",
    "text": "Differences in the returned log-density\nStan’s default “sampling statement” (e.g. y ~ normal(mu, sigma);) automatically drops constant terms (unless configured differently), see https://mc-stan.org/docs/reference-manual/statements.html#log-probability-increment-vs.-distribution-statement. Constant terms are terms which do not depend on model parameters, and this package’s macros and functions currently do not try to figure out which terms do not depend on model parameters, and as such we never drop them. This may lead to (constant) differences in the computed log-densities from the Stan and Julia implementations."
  },
  {
    "objectID": "index.html#some-models-are-not-implemented-yet-or-may-have-smaller-or-bigger-errors",
    "href": "index.html#some-models-are-not-implemented-yet-or-may-have-smaller-or-bigger-errors",
    "title": "StanBlocks.jl",
    "section": "Some models are not implemented yet, or may have smaller or bigger errors",
    "text": "Some models are not implemented yet, or may have smaller or bigger errors\nI’ve implemented many of the models, but I haven’t implemented all of them, and I probably have made some mistakes in implementing some of them."
  },
  {
    "objectID": "index.html#some-models-may-have-been-implemented-suboptimally",
    "href": "index.html#some-models-may-have-been-implemented-suboptimally",
    "title": "StanBlocks.jl",
    "section": "Some models may have been implemented suboptimally",
    "text": "Some models may have been implemented suboptimally\nJust that."
  }
]