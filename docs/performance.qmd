---
title: "Julia vs Stan performance comparison"
---

This page compares the performance of Julia's and Stan's log density and log density gradient computations for the implemented posteriors. Several caveats apply:

* The `posteriordb` Stan implementations were never meant to represent "perfect and best-performant" practices.
* The StanBlocks.jl implementations are not line-by-line translations of the Stan implementations. Sometimes small optimizations were applied, to make the implementation fall more in line with common Julia practices, or to make the code more friendly for Julia's AD packages, e.g. by avoiding mutation.
* Stan often automatically drops constant terms (unless configured differently), see [https://mc-stan.org/docs/reference-manual/statements.html#log-probability-increment-vs.-distribution-statement](https://mc-stan.org/docs/reference-manual/statements.html#log-probability-increment-vs.-distribution-statement), thus avoiding superfluous (for its purposes) computation, while the StanBlocks.jl implementations do not.
* Stan implements a lot of custom AD rules, while StanBlocks.jl does not at all, and Enzyme.jl does rarely (if ever?). I suspect that adding AD rules for `_glm_` type functions would further improve Julia's performance.
* The StanBlocks.jl "sampling" statements try to be clever about avoiding repeated computations. While I am not sure whether Stan applies the same optimizations, in principle it could do that without extra work by the user. 
* While preliminary benchmark runs included "all" Julia AD packages, all of them are almost always much slower than Enzyme.jl for the implemented posteriors, which on top of that performance advantage also supports more Julia language features than some of the other AD packages. As such, I am only comparing Enzyme and Stan. Enzyme outperforming every other AD package for *these* posteriors/loss functions does of course not mean that it will necessarily do as well for other applications.
* Enzyme's development is progressing quite quickly. While it currently sometimes crashes Julia, or it sometimes errors while trying to compute a gradient, in general Enzyme's performance and reliability are continuously and quickly improving.
* Stan's benchmark is done from Julia via `BridgeStan.jl`. While I think that any performance penalty should be extremely small, I am not 100% sure. BridgeStan uses the `-O3` compiler flag by default, but no additional ones.
* All benchmarks are happening with a single thread on my local machine.
* **There are probably more caveats!**

::: {.callout-warning}
**In general, doing performance comparisons is quite tricky, for more reasons than just the ones mentioned above. The below plot and tables should most definitely NOT be interpreted as "A is X-times faster than B".**
:::

```{julia}
using DataFrames, StanBlocks, Markdown, PosteriorDB, StanLogDensityProblems, LogDensityProblems, Statistics, OrderedCollections, PrettyTables, Serialization, Chairmarks, Enzyme, BridgeStan, Markdown, Pkg
using Logging, Test
include("julia/common.jl")
const ENZYME_VERSION = filter(x->x.second.name=="Enzyme", Pkg.dependencies()) |> x->first(x)[2].version
const VERSION_STRING = "Julia $VERSION + Enzyme $ENZYME_VERSION" 
getsampletimes(x::Chairmarks.Benchmark) = getfield.(x.samples, :time); 
```
```{julia}
pbenchmarks = map(posterior_names) do posterior_name
    validity_path = joinpath("cache", "$posterior_name.sjl")
    performance_path = joinpath("cache", "$(posterior_name)_primitive.sjl")
    isfile(validity_path) || return 
    mtime(performance_path) > mtime(validity_path) && return Serialization.deserialize(performance_path)
    validity = Serialization.deserialize(validity_path)
    validity.usable == "yes" || return
    println("Benchmarking primitive $posterior_name")
    try
        post = PosteriorDB.posterior(pdb, posterior_name)
        jlpdf = StanBlocks.julia_implementation(post)
        stan_path = PosteriorDB.path(PosteriorDB.implementation(PosteriorDB.model(post), "stan"))
        stan_problem = with_logger(ConsoleLogger(stderr, Logging.Error)) do 
            StanProblem(
                stan_path, 
                PosteriorDB.load(PosteriorDB.dataset(post), String);
                nan_on_error=true
            )
        end
        slpdf(x) = LogDensityProblems.logdensity(stan_problem, x)
        n = LogDensityProblems.dimension(jlpdf)
        jbenchmark = (@be randn(n) jlpdf)
        sbenchmark = (@be randn(n) slpdf)
        min_time = min(mean(jbenchmark).time, mean(sbenchmark).time)
        jtimes = getsampletimes(jbenchmark) ./ min_time
        stimes = getsampletimes(sbenchmark) ./ min_time
        rv = (;
            jtime=UncertainStatistic(mean, jtimes), 
            jallocs=mean(jbenchmark).allocs, 
            stime=UncertainStatistic(mean, stimes)
        )
        Serialization.serialize(performance_path, rv) 
        display(rv)
        return rv
    catch
    end
end 
df = DataFrame([
    merge((;posterior_name, dimension=posterior_dimension(posterior_name)), row) for (posterior_name, row) in zip(posterior_names, pbenchmarks) if !isnothing(row)
])
sort!(df, :jallocs)
pdf = DataFrame(OrderedDict(
    "posterior name"=>df.posterior_name,
    "dimension"=>df.dimension,
    "Stan mean relative runtime"=>df.stime,
    "Julia mean relative runtime"=>df.jtime,
    "Julia allocations"=>df.jallocs,
    "implementations"=>implementations_string.(df.posterior_name) 
))

gbenchmarks = map(df.posterior_name) do posterior_name
    # posterior_name == "earnings-earn_height" || return
    performance_path = joinpath("cache", "$(posterior_name)_gradient.sjl")
    validity_path = joinpath("cache", "$posterior_name.sjl")
    isfile(validity_path) || return  
    if mtime(performance_path) > mtime(validity_path)
        rv = Serialization.deserialize(performance_path)
        !isnothing(rv) && return rv
    end
    validity = Serialization.deserialize(validity_path)
    validity.usable == "yes" || return
    println("Benchmarking gradient $posterior_name")
    try
        Serialization.serialize(performance_path, nothing)
        post = PosteriorDB.posterior(pdb, posterior_name)
        jlpdf = StanBlocks.julia_implementation(post)
        stan_path = PosteriorDB.path(PosteriorDB.implementation(PosteriorDB.model(post), "stan"))
        stan_problem = with_logger(ConsoleLogger(stderr, Logging.Error)) do 
            StanProblem(
                stan_path, 
                PosteriorDB.load(PosteriorDB.dataset(post), String);
                nan_on_error=true
            )
        end
        elpdf((x,g)) = begin 
            xg = Enzyme.Duplicated(x, g)
            Enzyme.autodiff(
                Enzyme.set_runtime_activity(Enzyme.ReverseWithPrimal), Enzyme.Const(jlpdf), 
                Enzyme.Active, 
                xg
            )[2], g
        end
        slpdf((x,g)) = BridgeStan.log_density_gradient!(stan_problem.model, x, g)
        n = LogDensityProblems.dimension(jlpdf)
        sbenchmark = (@be (randn(n),zeros(n)) slpdf)
        stimes = getsampletimes(sbenchmark)
        rv = (;
            jtime=UncertainStatistic(mean, [Inf]), 
            jallocs=Inf,
            stime=UncertainStatistic(mean, stimes ./ mean(stimes)), 
            sallocs=mean(sbenchmark).allocs,
            jversion=VERSION_STRING
        )
        Serialization.serialize(performance_path, rv)
        jbenchmark = (@be (randn(n),zeros(n)) elpdf)
        min_time = min(mean(jbenchmark).time, mean(sbenchmark).time)
        jtimes = getsampletimes(jbenchmark) ./ min_time
        stimes = getsampletimes(sbenchmark) ./ min_time
        rv = (;
            jtime=UncertainStatistic(mean, jtimes), 
            jallocs=mean(jbenchmark).allocs, 
            stime=UncertainStatistic(mean, stimes), 
            sallocs=mean(sbenchmark).allocs,
            jversion=VERSION_STRING
        )
        Serialization.serialize(performance_path, rv)
        display(rv)
        return rv
    catch 
        # rethrow(rv) 
    end
end
jdf = DataFrame([
    (;
        prow.posterior_name, 
        prow.dimension,
        pallocs=prow.jallocs, 
        gallocs=grow.jallocs, 
        ptime=val(prow.jtime) / val(prow.stime), 
        gtime=val(grow.jtime) / val(grow.stime),
    ) for (prow, grow) in zip(eachrow(df), gbenchmarks) if !isnothing(grow) && isfinite(grow.jallocs)
])

df = DataFrame([
    merge((;posterior_name), row) for (posterior_name, row) in zip(df.posterior_name, gbenchmarks) if !isnothing(row) && isfinite(row.jallocs)
])
sort!(df, :jallocs)
gdf = DataFrame(OrderedDict(
    "posterior name"=>df.posterior_name,
    "dimension"=>posterior_dimension.(df.posterior_name),
    "Stan mean relative runtime"=>df.stime,
    "Julia+Enzyme mean relative runtime"=>df.jtime,
    "Julia+Enzyme allocations"=>df.jallocs,
    "implementations"=>implementations_string.(df.posterior_name),
    "Julia version"=>df.jversion
)); 
```

# Runtime overview

The below plot shows the relative primitive runtime (x-axis, Julia vs Stan, left: Julia is faster) and the relative gradient runtime (y-axis, Julia+Enzyme vs Stan, bottom: Julia is faster) for the `posteriordb` models for which the [overview table](index.qmd#overview-of-posteriors) has a `yes` in the `usable` column. The color of the dots represents the posterior dimension.
Hovering over the data points will show the posterior name, its dimension, the allocations required by Julia during the primitive and gradient run and a short explanation, e.g. for the topmost point: `mesquite-logmesquite_logvash (D=7, #allocs=0->70) - Julia's primitive is ~4.5 times faster, but Julia's gradient is ~16.0 times slower.`

:::{.column-page}

::: {.callout-warning}
**In general, doing performance comparisons is quite tricky, for more reasons than just the ones mentioned above. The below plot and tables should most definitely NOT be interpreted as "A is X-times faster than B".**
:::
```{julia}
 
# pretty_table(jdf; backend=Val(:html), show_subheader=false, table_class="interactive")
color = log.(jdf.dimension)
color .-= minimum(color)
color ./= maximum(color)
color = cgrad(:thermal)[color]'
# hover = map((n,d)->"$n (D=$d)\ntest", jdf.posterior_name, jdf.dimension) |> permutedims
hover = map(eachrow(jdf)) do row
    pdescr = if row.ptime > 1
        "$(round(row.ptime; sigdigits=2)) times slower"
    else
        "$(round(inv(row.ptime); sigdigits=2)) times faster"
    end
    gdescr = if row.gtime > 1
        "$(round(row.gtime; sigdigits=2)) times slower"
    else
        "$(round(inv(row.gtime); sigdigits=2)) times faster"
    end
    jdescr = if (row.ptime > 1) == (row.gtime > 1) 
        "and"
    else
        "but"
    end
    descr = "Julia's primitive is ~$pdescr, $jdescr Julia's gradient is ~$gdescr."
    "$(row.posterior_name) (D=$(row.dimension), #allocs=$(Int(row.pallocs))->$(Int(row.gallocs))) <br> $descr"
end |> permutedims

Plots.vline!(
Plots.hline!(
    Plots.scatter(
        (jdf.ptime)', 
        (jdf.gtime)'; 
        color, hover, 
        xlabel="Relative primitive runtime\n(Julia vs Stan, left: Julia is faster)", 
        ylabel="Relative gradient runtime\n(Julia+Enzyme vs Stan, bottom: Julia is faster)",
        xscale=:log10, yscale=:log10, 
        legend=false,
        size=(1000, 600)
    ),
    [1], color=:black
),
    [1], color=:black
)
```
:::

# Primitive runtime comparison

:::{.column-page}

```{julia}
pretty_table(pdf; backend=Val(:html), show_subheader=false, table_class="interactive")
```
:::

# Gradient runtime comparison

:::{.column-page}

```{julia}
pretty_table(gdf; backend=Val(:html), show_subheader=false, table_class="interactive")
```
:::